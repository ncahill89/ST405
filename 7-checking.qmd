---
title: "Bayesian Data Analysis"
subtitle: "Model Checking"
author: "Dr Niamh Cahill (she/her)"
format: revealjs
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(bayesplot)
library(rjags)
library(R2jags)
library(tidyverse)
library(rstanarm)
library(tidybayes)
```

## Posterior Predictive Checking {style="font-size: 60%;"}

The idea behind posterior predictive checking is simple: if a model is a good fit then we should be able to use it to generate data that looks a lot like the data we observed.

-   To generate the data used for posterior predictive checks (PPCs) we simulate from the posterior predictive distribution.

-   For each draw (simulation) $s= 1, \ldots , S$ of the parameters from the posterior distribution, we draw an entire vector of $N$ outcomes $\tilde{y}^{(s)}$ from the posterior predictive distribution by simulation from the data model, conditional on parameters.

-   The result is an $S \times N$ matrix of draws $\tilde{y}$

-   When simulating from the posterior predictive distribution using the same values of the predictors X that we used when fitting the model we denote the simulations $y^{rep}$.

-   When predicting new or future observations we denote the simulations $\tilde{y}$.

-   We will use the `bayesplot` package to create various graphical displays for posterior predictive checks (PPCs).

## Predictive Distribution for Observations, $y^{rep}$ {style="font-size: 60%;"}

Let's revisit the Bayesian regression Model applied to Kid IQ:

-   **Model for Observations**:

    $$y_i \sim N(\mu_i, \sigma)$$

-   **Regression Structure**:\
    $$\mu_i = \alpha_{j[i]} + \beta_{j[i]} \left( x_i - \text{mean}(x) \right), \quad \text{where } j[i] = \text{education level for observation } i$$

    -   $\alpha_j \sim N(100, 30^2), \quad j = 1, 2$
    -   $\beta_j \sim N(0, 2^2), \quad j = 1, 2$

To simulate new observations ($y^{\text{rep}}$) based on this model, we follow these steps:

1.  **Sample** $\mu_i^{(s)}$ and $\sigma^{(s)}$ from the posterior distribution $p(\mu, \sigma \mid y)$.

2.  **Generate Replicates** $y_i^{\text{rep}(s)}$ by drawing from $N(\mu_i^{(s)}, \sigma^{(s)})$.

This process provides a predictive distribution of Kid IQ observations, enabling us to compare the model's predictions to the observed data.

## JAGS model to include $y^{rep}$ {style="font-size: 60%;"}

```{r}
lrmodel2 ="
model{
	for (i in 1:n){
 		y.i[i] ~ dnorm(mu.i[i], sigma^-2)
 		mu.i[i] <- alpha.j[hs_index[i]] + beta.j[hs_index[i]]*(x.i[i] - mean(x.i))
	}

#Priors
for(j in 1:m)
{
alpha.j[j] ~ dnorm(100, 30^-2) 
beta.j[j] ~ dnorm(0, 2^-2) 
}
sigma ~ dunif(0,50)

## predictive distribution
for (i in 1:n) {yrep[i] ~ dnorm(mu.i[i], sigma^-2)}
}
"
```

```{r, include = FALSE, echo=FALSE}
jags.data <- list(y.i = kidiq$kid_score, 
                  x.i = kidiq$mom_iq, 
                  hs_index = as.numeric(kidiq$mom_hs + 1),
                  n = nrow(kidiq),
                  m = 2)

parnames <- c("alpha.j","beta.j","mu.i","yrep")

mod <- jags(data = jags.data, 
            parameters.to.save=parnames, 
            model.file = textConnection(lrmodel2))
```

## Posterior predictive checking: histograms {style="font-size: 60%;"}

To use the PPC function from the `bayesplot` package we need the outcome values `y` and a matrix of replicates `yrep`

```{r}
y <- kidiq$kid_score
yrep <- mod$BUGSoutput$sims.list$yrep
```

Now let's look at histograms of some of the `yrep` datasets and see how they compare to `y`

```{r, echo = TRUE, message=FALSE, fig.height=1.5,fig.width=4}
library(bayesplot)
color_scheme_set("brightblue")
ppc_hist(y, yrep[1:3, ])
```

## Posterior predictive checking: density overlay {style="font-size: 60%;"}

We can also look at is a comparison of the distribution of y and the distributions of some of the simulated datasets in the `yrep` matrix via density plots.

```{r, echo = TRUE, fig.height=2,fig.width=4}
ppc_dens_overlay(y, yrep[1:50, ])
```

## Posterior predictive checking: test statistic {style="font-size: 60%;"}

Decide on a test quantity (min, max) $T(y,\theta)$. A good statistic would ideally be independent of the parameters of the model.

-   obtain the summary quantity for the observed data $T(y)$

-   obtain the summary quantity for the replicated data $T(y,\theta)$

Let's consider the maximum Kid IQ score as the test quantity

```{r, echo = TRUE, message=FALSE, fig.height=2,fig.width=4}
max_y <- max(kidiq$kid_score)
ppc_stat(y,yrep, stat = "max")
```

## Posterior predictive checking: test statistic {style="font-size: 60%;"}

Many of the available PPCs can also be carried out within levels of a grouping variable.

For example we can compute the test statistic within levels of the highschool grouping variable and a separate plot is made for each level.

```{r, echo = TRUE, message=FALSE, fig.height=2,fig.width=4}
max_y <- max(kidiq$kid_score)
ppc_stat_grouped(y,yrep,group = kidiq$mom_hs,  stat = "max")
```

Hereâ€™s an enhanced version of your sensitivity analysis slide for better clarity and emphasis:

------------------------------------------------------------------------

## Sensitivity Analysis {style="font-size: 60%;"}

How do model structure and prior choices impact the results?

-   **Test Alternative Models and Priors**

    Evaluate the robustness of inferences by experimenting with different model configurations and prior distributions.

-   **Compare Sensitivity of Key Inference Metrics**

    Assess how changes in model assumptions affect critical inference measures.

-   **Sensitivity of Extremes vs. Central Tendency**

    Extremes (e.g., percentiles) tend to be more affected by model changes than measures of central tendency (means, medians).

-   **Extrapolation vs. Interpolation**

    Predictions made outside the observed predictor range (extrapolation) show higher sensitivity to model assumptions than predictions within this range (interpolation).

## Prior predictive checks {style="font-size: 60%;"}

Prior predictive checks allow us to look at the situation prior to any data being observed in order to see what data distribution is being implied by the choice of priors and likelihood.

-   Ideally this distribution would have at least some mass around all plausible data sets.

JAGS has a simulation functionality that allows us to easily generate prior predictive distributions based on our specified model.

-   The likelihood and priors can be specified in a "data" block so that we can simulate what data generated from the likelihood would look like, based solely on our prior assumptions.

```{r}
lrmodel_prior ="
data{
	for (i in 1:n){
	  yrep_prior[i] ~ dnorm(mu.i[i], sigma^-2)
 		mu.i[i] <- alpha.j[hs_index[i]] + beta.j[hs_index[i]]*(x.i[i] - mean(x.i))
	}
#Priors
for(j in 1:m)
{
alpha.j[j] ~ dnorm(80, 30^-2) 
beta.j[j] ~ dnorm(0, 2^-2) 
}
sigma ~ dt(30,10^-2,1)T(0,) #truncated t-distribution
} # end data block
model{
fake <- 0
}
"
```

## Prior predictive checks {style="font-size: 60%;"}

We can use the `run.jags()` function from the `runjags` package to generate the simulations.

```{r, message = FALSE, eval = FALSE}
jags.data <- list(x.i = kidiq$mom_iq, 
                  hs_index = as.numeric(kidiq$mom_hs + 1),
                  n = nrow(kidiq),
                  m = 2)

yrep_prior <- matrix(NA, nrow = 50, ncol = nrow(kidiq))

for(i in 1:50)
  {
out <- runjags::run.jags(lrmodel_prior, 
                         data = jags.data,monitor=c("yrep_prior"), 
                         sample=1, 
                         n.chains=1, 
                         summarise=FALSE)

 yrep_prior[i,] <- coda::as.mcmc(out)
 }
```

```{r, include = FALSE, echo = FALSE,message = FALSE}
jags.data <- list(x.i = kidiq$mom_iq, 
                  hs_index = as.numeric(kidiq$mom_hs + 1),
                  n = nrow(kidiq),
                  m = 2)

yrep_prior <- matrix(NA, nrow = 50, ncol = nrow(kidiq))

for(i in 1:50)
  {
  out <- runjags::run.jags(lrmodel_prior, 
                         data = jags.data,monitor=c("yrep_prior"), 
                         sample=1, 
                         n.chains=1, 
                         summarise=FALSE)

  yrep_prior[i,] <- coda::as.mcmc(out)
}
```

## Prior predictive checks {style="font-size: 60%;"}

We can then use the PPC functions from the `bayesplot` package by providing replicates `yrep_prior` instead of `y_rep`

```{r, echo = TRUE, message = FALSE,fig.height=2.5, fig.width=5}
ppc_hist(y, yrep_prior[1:5, ])
```

## Prior vs Posterior for parameters ($\alpha_j$) {style="font-size: 60%;"}

```{r, echo = FALSE, message = FALSE, warning=FALSE}
m1 <- mod$BUGSoutput$sims.matrix

par_samps <- m1 %>% spread_draws(alpha.j[1:2],beta.j[1:2]) 

par_dat <- tibble(prior_alpha = rnorm(1000,80,30),
              post_alpha1 = par_samps %>% filter(`1:2` == 1) %>% pull(alpha.j) %>% sample(1000),
              post_alpha2 = par_samps %>% filter(`1:2` == 2) %>% pull(alpha.j) %>% sample(1000))

ggplot(par_dat) +
  geom_histogram(aes(x = prior_alpha, fill = "prior (alpha_j)"), alpha = 0.4) +
  geom_histogram(aes(x = post_alpha1, fill = "posterior (alpha1)"),alpha = 0.4) +
  geom_histogram(aes(x = post_alpha2, fill = "posterior (alpha2)"),alpha = 0.4) +
  labs(fill = "") +
  xlab("beta")


```

## Prior vs Posterior for parameters ($\beta_j$) {style="font-size: 60%;"}

```{r, echo = FALSE, message = FALSE, warning=FALSE}
par_dat <- tibble(prior_beta = rnorm(1000,0,2),
              post_beta1 = par_samps %>% filter(`1:2` == 1) %>% pull(beta.j) %>% sample(1000),
              post_beta2 = par_samps %>% filter(`1:2` == 2) %>% pull(beta.j) %>% sample(1000))

ggplot(par_dat) +
  geom_histogram(aes(x = prior_beta, fill = "prior (beta_j)"), alpha = 0.4) +
  geom_histogram(aes(x = post_beta1, fill = "posterior (beta1)"),alpha = 0.4) +
  geom_histogram(aes(x = post_beta2, fill = "posterior (beta2)"),alpha = 0.4) +
  labs(fill = "") +
  xlab("beta")

```
