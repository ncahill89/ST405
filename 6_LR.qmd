---
title: "Bayesian Data Analysis"
subtitle: "Bayesian Linear Regression with JAGS"
author: "Prof. Niamh Cahill (she/her)"
format: revealjs
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(tidybayes)
```

## Data: Cognitive Test Scores {style="font-size: 70%;"}

Data are available on the cognitive test scores of three- and four-year-old children in the USA.

-   The sample contains 434 observations

-   Information also provided about his/her mother's IQ and whether or not the mother graduated from highschool.

```{r,include = FALSE}
library(rstanarm)
data(kidiq)
```

```{r, fig.height=3, fig.width=3}
p <- ggplot(kidiq, aes(x = mom_iq, y = kid_score, 
                  colour = as.factor(mom_hs))) +
  geom_point() +
  labs(colour = "mom_hs") +
  theme(legend.position="top")

p_extra <- ggExtra::ggMarginal(p, type = "histogram", margins = "y") 
p_extra
```

## Simple Linear Regression Model {style="font-size: 70%;"}

We will assume a normal model for the data such that

$y_i|\mu,\sigma^2 \sim N(\mu_i, \sigma^2)$

-   Let's start simple and consider the expected value for $y_i$ as a function of one explanatory variable (mother's IQ) such that:

$\mu_i = \alpha + \beta x_i$

-   This could also be specified as:

$y_i = \mu_i + \epsilon_i$

$\epsilon_i \sim N(0, \sigma^2)$

-   To give the intercept more meaning we could mean center the predictor:

$\mu_i = \alpha + \beta (x_i - \bar{x})$

-   $\alpha$ is now the expected value of $y$ at the average $x$.

## Prior Choice {style="font-size: 70%;"}

We will need to specify priors for $\alpha$, $\beta$ and $\sigma$.

-   Before seeing the data would could assume that the value of the intercept ($\alpha$) is unlikely to be to beyond the range of possible IQ values (1, 200). So let’s assume an expected value of 100 and a standard deviation of 30.

::: columns
::: {.column width="50%"}
$\alpha \sim N(100,30^2)$
:::

::: {.column width="50%"}
```{r}
# Set parameters for the normal distribution
mean_value <- 100
sd_value <- 30

# Define a range for the x-axis based on the mean and standard deviation
x <- seq(mean_value - 4 * sd_value, mean_value + 4 * sd_value, length = 100)

# Calculate the density values for each x
y <- dnorm(x, mean = mean_value, sd = sd_value)

# Plot the normal density curve
plot(x, y, type = "l", lwd = 2, col = "blue",
     main = "Normal Distribution (Mean = 100, SD = 30)",
     xlab = "X", ylab = "Density")

# Add a vertical line at the mean
abline(v = mean_value, col = "red", lty = 2)

```
:::
:::

## Prior Choice {style="font-size: 70%;"}

We will need to specify priors for $\alpha$, $\beta$ and $\sigma$.

-   For the slope ($\beta$), before seeing the data I have no idea about whether the mother's IQ affects the kid's IQ, so it is reasonable to consider a value of 0 for the slope. Realistically I think we are unlikely to see the value of the slope going anywhere beyond the range (-4,4), so let's assume a standard deviation for the slope of 2.

::: columns
::: {.column width="50%"}
$\beta \sim N(0,2^2)$
:::

::: {.column width="50%"}
```{r}
# Set parameters for the normal distribution
mean_value <- 0
sd_value <- 2

# Define a range for the x-axis based on the mean and standard deviation
x <- seq(mean_value - 4 * sd_value, mean_value + 4 * sd_value, length = 100)

# Calculate the density values for each x
y <- dnorm(x, mean = mean_value, sd = sd_value)

# Plot the normal density curve
plot(x, y, type = "l", lwd = 2, col = "blue",
     main = "Normal Distribution (Mean = 100, SD = 30)",
     xlab = "X", ylab = "Density")

# Add a vertical line at the mean
abline(v = mean_value, col = "red", lty = 2)

```
:::
:::

## Prior Choice {style="font-size: 70%;"}

For $\sigma$ we can consider some uninformative or weakly informative priors commonly used for variance parameters:

-   Gamma prior $gamma(\epsilon, \epsilon)$ on the precision ($\tau = \frac{1}{\sigma^2}$)

$\tau \sim Ga(0.1,0.1)$

-   Uniform prior on the standard deviation

$\sigma \sim U(0,50)$

-   Cauchy (half-t) prior on the standard deviation

$\sigma \sim ht(30,10^2,1)$

For more information see [this paper by Andrew Gelman.](https://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-3/Prior-distributions-for-variance-parameters-in-hierarchical-models-comment-on/10.1214/06-BA117A.full)

## JAGS model specification {style="font-size: 70%;"}

::: columns
::: {.column width="50%"}
-   Model specification (likelihood and priors):

**Data Model (likelihood)** \begin{align*}
 y_i|\mu_i, \sigma &\sim   Normal(\mu_i, \sigma) \\
 \mu_i &= \alpha + \beta*(x_i - \bar{x})\\
\end{align*}

**Priors** \begin{align*}
 \alpha  &\sim Normal(100, 30^2)\\
 \beta  &\sim Normal(0, 2^2)\\
 \sigma &\sim half-t(30, 10^2,1)\\
\end{align*}
:::

::: {.column width="50%"}
-   JAGS model specification:

```{r, echo = TRUE, include=TRUE}
#| code-line-numbers: "3-8|9-13|1,2,3,14,15"

lrmodel1 ="
model{

# likelihood
	for (i in 1:n){
 		y.i[i] ~ dnorm(mu.i[i], sigma^-2)
 		mu.i[i] <- alpha + beta*(x.i[i] - mean(x.i))
	}

# priors
alpha ~ dnorm(100, 30^-2) 
beta ~ dnorm(0, 2^-2) 
sigma ~ dunif(0,50)
}
"
```
:::
:::

## Model Fitting {style="font-size: 70%;"}

```{r, echo = TRUE, message=FALSE}
library(rjags)
library(R2jags)

jags.data1 <- list(y.i = kidiq$kid_score, 
                  x.i = kidiq$mom_iq, 
                  n = nrow(kidiq))

parnames1 <- c("alpha","beta","sigma","mu.i")

mod1 <- jags(data = jags.data1, 
             parameters.to.save=parnames1, 
             model.file = textConnection(lrmodel1),
             n.iter = 4000,
             n.burnin = 2000)
```

## Output - Parameter Uncertainty {style="font-size: 70%;"}

```{r, echo = TRUE}
library(tidybayes)

mcmc.matrix1 <- mod1$BUGSoutput$sims.matrix
par_samps1 <- mcmc.matrix1 %>% spread_draws(alpha,beta,sigma) 
par_samps1

par_summary1 <- mcmc.matrix1 %>% 
                gather_rvars(alpha,beta,sigma) %>% 
                median_qi(.value, .width = c(.95,0.9))
par_summary1
```

## Output - parameter Uncertainty {style="font-size: 70%;"}

```{r, echo = FALSE}
library(ggdist)

p1 <- ggplot(par_samps1, aes(x = alpha)) +
  stat_halfeye(aes(fill = stat(cut_cdf_qi(cdf, .width = c(.68, .95, .99)))),
                    slab_color = "steelblue4") +
  scale_fill_brewer(direction = -1, na.translate = FALSE) +
  labs(fill = "Intervals")

p2 <- ggplot(par_samps1, aes(x = beta)) +
  stat_halfeye( aes(fill = stat(cut_cdf_qi(cdf, .width = c(.68, .95, .99)))),
                    slab_color = "steelblue4") +
  scale_fill_brewer(direction = -1, na.translate = FALSE) +
  labs(fill = "Intervals")

p3 <- ggplot(par_samps1 , aes(x = sigma)) +
  stat_halfeye( aes(fill = stat(cut_cdf_qi(cdf, .width = c(.68, .95, .99)))),
                    slab_color = "steelblue4") +
  scale_fill_brewer(direction = -1, na.translate = FALSE) +
  labs(fill = "Intervals")

ggpubr::ggarrange(p1,p2,p3,nrow = 1, common.legend = TRUE)
```

## Posterior Draws of $\mu_i$ {style="font-size: 70%;"}

For each sample draw (simulation) $s= 1, \ldots , S$ of the parameters from the posterior distribution we can obtain $\mu_i^{(s)} = \beta_0^{(s)} + \beta_1^{(s)}(x_{i} - \bar{x})$

::: columns
::: {.column width="50%"}
```{r, echo = FALSE}
mu_ind <- 1:nrow(kidiq)
mu_samps1 <- mcmc.matrix1 %>% spread_draws(mu.i[mu_ind]) 
mu_samps1
mu_samps1$traj <- rep(1:dim(mcmc.matrix1)[1],nrow(kidiq))
mu_samps1$x <- rep(kidiq$mom_iq,each = dim(mcmc.matrix1)[1])
mu_samps1 <- mu_samps1 %>% arrange(x) %>% mutate(traj = as.factor(traj))
```
:::

::: {.column width="50%"}
**5 posterior draws**

```{r,fig.width=7,fig.height=4}
ggplot(kidiq, aes(x = mom_iq, y = kid_score)) +
  geom_point(alpha = 0.3) +
  geom_line(data = mu_samps1 %>% filter(traj == c(1:4)),aes(x = x, y = mu.i, group = traj, colour = "Posterior draws of mu"),alpha = 0.3) +
  theme_bw() +
  labs(colour = "") 
```
:::
:::

## Posterior Draws of $\mu_i$ {style="font-size: 70%;"}

For each draw (simulation) $s= 1, \ldots , S$ of the parameters from the posterior distribution we can obtain $\mu_i^{(s)} = \beta_0^{(s)} + \beta_1^{(s)}(x_{i} - \bar{x})$

::: columns
::: {.column width="50%"}
```{r, echo = FALSE}
mu_ind <- 1:nrow(kidiq)
mu_samps1 <- mcmc.matrix1 %>% spread_draws(mu.i[mu_ind]) 
mu_samps1
mu_samps1$traj <- rep(1:dim(mcmc.matrix1)[1],nrow(kidiq))
mu_samps1$x <- rep(kidiq$mom_iq,each = dim(mcmc.matrix1)[1])
mu_samps1 <- mu_samps1 %>% arrange(x) %>% mutate(traj = as.factor(traj))
```
:::

::: {.column width="50%"}
**200 posterior draws**

```{r, fig.width=7,fig.height=4}
ggplot(kidiq, aes(x = mom_iq, y = kid_score)) +
  geom_point(alpha = 0.3) +
  geom_line(data = mu_samps1 %>% filter(traj == c(1:200)),aes(x = x, y = mu.i, group = traj, colour = "Posterior draws of mu"),alpha = 0.1) +
  theme_bw() +
  labs(colour = "")  +
  ylab("Kid's IQ") +
  xlab("Mother's IQ")

```
:::
:::

## Summary for $\mu_i$ - Point Estimate + Credible Interval {style="font-size: 70%;"}

```{r, echo = TRUE, include=TRUE}
#| code-line-numbers: ""
#| output-location: column-fragment

mu_ind <- 1:nrow(kidiq)
mu_samps1 <- mcmc.matrix1 %>% spread_draws(mu.i[mu_ind]) 
mu_summary1 <- mu_samps1 %>% 
                median_qi(.width = 0.95)
mu_summary1
```

```{r, echo = TRUE, include=TRUE}
#| code-line-numbers: ""
#| output-location: column-fragment

kidiq <- kidiq %>% mutate(mu.i = mu_summary1$mu.i,
                          lower = mu_summary1$.lower,
                          upper = mu_summary1$.upper)

ggplot(kidiq, aes(x = mom_iq, y = kid_score)) +
  geom_point() +
  geom_line(aes(x = mom_iq, y = mu.i),colour = "blue") +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = "CI"), alpha = 0.3) +
  theme_bw() +
  labs(fill = "") +
  ylab("Kid's IQ") +
  xlab("Mother's IQ")

```

## Predictive Distribution {style="font-size: 70%;"}

The posterior predictive distribution is the distribution of possible unobserved values conditional on the observed values.

```{r, include=FALSE}
mu_ind <- 1:nrow(kidiq)
mu_samps1 <- mcmc.matrix1 %>% spread_draws(mu.i[mu_ind]) 
mu_samps1$traj <- rep(1:dim(mcmc.matrix1)[1],nrow(kidiq))
mu_samps1$x <- rep(kidiq$mom_iq,each = dim(mcmc.matrix1)[1])
mu_samps1 <- mu_samps1 %>% arrange(x) %>% mutate(traj = as.factor(traj))
```

```{r, fig.width=7,fig.height=4}
## breaks: where you want to compute densities
breaks <- c(60,80,100,120,140)
kidiq$group <- cut(kidiq$mom_iq, breaks)
kidiq$res <- kidiq$kid_score - mod1$BUGSoutput$mean$mu.i
kidiq$fit <-  mod1$BUGSoutput$mean$mu.i
kidiq$fit_sd <- mod1$BUGSoutput$sd$mu.i

## Compute densities for each section, and flip the axes, and add means of sections
## Note: the densities need to be scaled in relation to the section size (2000 here)
dens <- do.call(rbind, lapply(split(kidiq, kidiq$group), function(x) {
  ## Get some data for normal lines as well
  xs <- seq(min(x$res), max(x$res), len=50)
  res <- data.frame(y = xs + mean(x$fit),
                               x = min(x$mom_iq) + 700*dnorm(xs, 0, mod1$BUGSoutput$mean$sigma))
  res
}))
dens$group <- rep(levels(kidiq$group), each=50)
dens <- dens %>% filter(group != "(60,80]")


ggplot(kidiq, aes(x = mom_iq, y = kid_score)) +
  geom_point(alpha = 0.3) +
  geom_line(data = mu_samps1 %>% filter(traj == c(1:200)),aes(x = x, y = mu.i, group = traj, colour = "Posterior draws of mu"),alpha = 0.1) +
  theme_bw() + 
  #theme(legend.position = "none") +
  geom_path(data=dens, aes(x, y,group=group, colour = "Predictive distribution \n given posterior mean"), lwd=1.1) +
  geom_vline(xintercept=breaks[-1], lty=2) +
  labs(colour = "") +
  ylab("Kid's IQ") +
  xlab("Mother's IQ")
```

## Bayesian Credible intervals {style="font-size: 70%;"}

::: callout-note
## Bayesian Credible Interval

A **Bayesian credible interval** represents the range within which a parameter value (such as a mean, a proportion, or a regression coefficient) is expected to fall with a specified probability, given the observed data.
:::

-   **Interpretation**: If we calculate a 95% credible interval for a parameter, we can interpret this as having a 95% probability that the parameter lies within that interval, conditional on the observed data and the chosen prior.

-   **Usage**: Bayesian credible intervals are particularly used to estimate the uncertainty of unknown model parameters (e.g., the mean of a distribution, a regression coefficient).

For example, if we calculate a 95% credible interval for a mean $\mu$ Kid IQ given the Mothers IQ is 120, it means there’s a 95% probability that $mu$ is within this interval, given the observed data and the prior belief about $\mu$.

## Bayesian Prediction Intervals {style="font-size: 70%;"}

::: callout-note
## Prediction Interval

A **prediction interval** represents the range within which a future, single observation is expected to fall, given the current model and data.
:::

-   **Interpretation**: A 95% prediction interval indicates that there is a 95% probability that a new, future observation will lie within this interval, taking into account both the uncertainty about the parameter estimates and the natural variability in future data points.

-   **Usage**: Prediction intervals are often used to estimate the range in which future values of a variable are likely to fall, such as forecasting sales figures, temperatures, or test scores.

For example, if we have a 95% prediction interval for a future observation $y$ in a Bayesian setting, it means there is a 95% probability that a new data point, drawn under the same conditions, will lie within this interval.

## Key Differences Between Bayesian Credible Intervals and Prediction Intervals {style="font-size: 60%;"}

| Aspect                 | Bayesian Credible Interval                                        | Prediction Interval                                           |
|----------------|-----------------------------|---------------------------|
| **Purpose**            | To estimate the range of a parameter with specified credibility   | To predict the range of a future observation                  |
| **Focus**              | The value of the parameter, given observed data                   | A single future observation, given the model and data         |
| **Variability Source** | Only parameter uncertainty, conditioned on data and prior beliefs | Both parameter uncertainty and data variability               |
| **Interpretation**     | Probability of the parameter being within the interval            | Probability of a future observation being within the interval |
| **Application**        | Parameter estimation in Bayesian inference                        | Forecasting or prediction for future observations             |

<!-- - A Bayesian “credible interval”, is an interval associated with the posterior distribution of a parameter, for example, the \underline{expected value} of the Kid's IQ given a mother's IQ of 120 ($\mu$). If I give an 80% interval there should be an 80% chance that the expected value of the Kid IQ lies within the interval.  -->

<!-- - A prediction interval is an interval associated with a random variable yet to be observed, for example, an \underline{unobserved} Kid's IQ, given a mother's IQ of 120 ($\tilde{y})$. If I give an 80% interval there should be an 80% chance that the actual Kid IQ lies within the interval.  -->

## Add a predictive distribution to JAGS {style="font-size: 70%;"}

**For a single** $x_{pred} = 120$

```{r,echo = TRUE}
lrmodel1 ="
model{
....
## predictive distribution
  mu_pred <- alpha + beta*(120 - mean(x.i))
 	ytilde ~ dnorm(mu_pred, sigma^-2)
}
"
```

**For multiple** $x_{pred} = 120,130,140,150$

```{r,echo = TRUE}
lrmodel1 ="
model{
....
## predictive distribution

for(j in 1:N_pred)
{
  mu_pred[j] <- alpha + beta*(x_pred[j] - mean(x.i))
 	ytilde[j] ~ dnorm(mu_pred[j], sigma^-2)
}
}
"
```

```{r, echo = FALSE, include=FALSE}
lrmodel1 ="
model{
	for (i in 1:n){
 		y.i[i] ~ dnorm(mu.i[i], sigma^-2)
 		mu.i[i] <- alpha + beta*(x.i[i] - mean(x.i))
	}

#Priors
alpha ~ dnorm(80, 30^-2) 
beta ~ dnorm(0, 2^-2) 
sigma ~ dt(30,10^-2,1)T(0,) #truncated t-distribution

## predictive distribution
  mu_pred <- alpha + beta*(120 - mean(x.i))
 	ytilde ~ dnorm(mu_pred, sigma^-2)
}
"
```

```{r, message=FALSE, include=FALSE}
jags.data1 <- list(y.i = kidiq$kid_score, 
                  x.i = kidiq$mom_iq, 
                  n = nrow(kidiq))

parnames1 <- c("alpha","beta","mu.i","mu_pred","ytilde")

mod1 <- jags(data = jags.data1, 
            parameters.to.save=parnames1, 
            model.file = textConnection(lrmodel1))
mcmc.matrix1 <- mod1$BUGSoutput$sims.matrix

```

## Credible interval vs prediction interval {style="font-size: 70%;"}

Let's consider the difference between the credible interval and the prediction interval for the Kid's IQ when the Mother's IQ = 120.

```{r, echo = TRUE, warning=FALSE}
par_samps1 <- mcmc.matrix1 %>% spread_draws(mu_pred,ytilde) 
par_samps1

par_summary1 <- mcmc.matrix1 %>% 
                gather_rvars(mu_pred,ytilde) %>% 
                median_qi(.value)
par_summary1
```

What do you notice about the width of the intervals?

## Extending the regression model {style="font-size: 70%;"}

Suppose now we want to see if there's a "highschool" effect on the relationship between Mother's IQ and Kid's IQ.

Some possible choices:

-   Model 2A: Add the highschool variable in as a covariate, this will lead to a varying intercepts model with $\alpha_1$ for `highschool = no` and $\alpha_2$ for `highschool = yes`.

-   Model 2B: Add the highschool variable in as a covariate and as an interaction with Mother's IQ, this will lead to a varying intercepts and slopes model with $\alpha_1$ and $\beta_1$ for `highschool = no` and $\alpha_2$ and $\beta_2$ for `highscool = yes`.

Let's consider model 2B. The specification of this model could be written as

$y_i|\mu,\sigma^2 \sim N(\mu_i, \sigma^2)$

$\mu_i = \underset{\text{j[i] = HS for obs i}}{\alpha_{j[i]} + \beta_{j[i]}}(x_{i} - \bar{x})$

$\alpha_j \sim N(100,30^2), \hspace{0.5em} \text{for } j=1,2$

$\beta_j \sim N(0,2^2), \hspace{0.5em} \text{for } j=1,2$

## JAGS model specification {style="font-size: 70%;"}

::: columns
::: {.column width="50%"}
$y_i|\mu,\sigma^2 \sim N(\mu_i, \sigma^2)$

$\mu_i = \underset{\text{j[i] = HS for obs i}}{\alpha_{j[i]} + \beta_{j[i]}}(x_{i} - \bar{x})$

$\alpha_j \sim N(100,30^2), \hspace{0.5em} \text{for } j=1,2$

$\beta_j \sim N(0,2^2), \hspace{0.5em} \text{for } j=1,2$

$\sigma \sim U(0,50)$
:::

::: {.column width="50%"}
```{r, echo = TRUE, include=TRUE}
#| code-line-numbers: "3-8|9-15|1,2,16,17"

lrmodel2 ="
model{
# likelihood
	for (i in 1:n){
 		y.i[i] ~ dnorm(mu.i[i], sigma^-2)
 		mu.i[i] <- alpha.j[hs_index[i]] + beta.j[hs_index[i]]*(x.i[i] - mean(x.i))
	}

#Priors
for(j in 1:m)
{
alpha.j[j] ~ dnorm(80, 30^-2) 
beta.j[j] ~ dnorm(0, 2^-2) 
}
sigma ~ dunif(0,50)
}
"
```
:::
:::

## Model Fitting {style="font-size: 70%;"}

```{r, echo = TRUE, message=FALSE}
library(rjags)
library(R2jags)

jags.data2 <- list(y.i = kidiq$kid_score, 
                  x.i = kidiq$mom_iq, 
                  hs_index = as.numeric(kidiq$mom_hs + 1),
                  n = nrow(kidiq),
                  m = 2)

parnames2 <- c("alpha.j","beta.j","sigma","mu.i")
mod2 <- jags(data = jags.data2, 
            parameters.to.save=parnames2, 
            model.file = textConnection(lrmodel2),
            n.iter = 4000,
            n.burnin = 2000)

```

## Output {style="font-size: 70%;"}

```{r, echo = TRUE}
mcmc.matrix2 <- mod2$BUGSoutput$sims.matrix

par_summary2 <- mcmc.matrix2 %>% 
                gather_rvars(alpha.j[1:2],beta.j[1:2]) %>% 
                median_qi(.value)
par_summary2

mu_ind <- 1:nrow(kidiq)
mu_samps2 <- mcmc.matrix2 %>% spread_draws(mu.i[mu_ind]) 
mu_summary2 <- mu_samps2 %>% 
                median_qi(.width = 0.95)

mu_summary2
```

## Visualising the results {style="font-size: 70%;"}

```{r, echo = TRUE, include=TRUE}
#| code-line-numbers: ""

kidiq <- kidiq %>% mutate(mu.i = mu_summary2$mu.i,
                          lower = mu_summary2$.lower,
                          upper = mu_summary2$.upper,
                          mom_hs = factor(mom_hs, 
                                          labels = c("no", "yes")))

ggplot(kidiq, aes(x = mom_iq, y = kid_score, colour = mom_hs)) +
  geom_point(alpha = 0.3) +
  geom_line(aes(x = mom_iq, y = mu.i)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  theme_bw() +
  labs(colour = "Highschool") +
  ylab("Kid's IQ") +
  xlab("Mother's IQ")

```

## Which model do we choose? {style="font-size: 70%;"}

Firstly, let's have a look at the residuals for each model

```{r, fig.height=4,fig.width=6}
mu_ind <- 1:nrow(kidiq)

mu_summary1 <- mcmc.matrix1 %>% spread_draws(mu.i[mu_ind]) %>% 
                median_qi(.width = 0.8)

mu_summary2 <- mcmc.matrix2 %>% spread_draws(mu.i[mu_ind]) %>% 
                median_qi(.width = 0.8)


kidiq <- kidiq %>% mutate(mu1 = mu_summary1$mu.i,
                          mu2 = mu_summary2$mu.i,) 

p1 <- ggplot(kidiq, aes(x = mu1, y = kid_score - mu1)) +
  geom_point()

p2 <- ggplot(kidiq, aes(x = mu2, y = kid_score - mu2)) +
  geom_point()

ggpubr::ggarrange(p1,p2)
```

## Which model do we choose? {style="font-size: 70%;"}

Let's also consider the observed values vs the model-based estimates.

```{r,fig.height=4,fig.width=6}

p1 <- ggplot(kidiq, aes(x = kid_score, y = mu1)) +
  geom_point() +
  geom_line(aes(x = kid_score, y = kid_score))

p2 <- ggplot(kidiq, aes(x = kid_score, y = mu2)) +
  geom_point() +
  geom_line(aes(x = kid_score, y = kid_score))

ggpubr::ggarrange(p1,p2)

```

## Model information criteria {style="font-size: 70%;"}

You might have come across these before: Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC)

-   The general idea is that the score on the likelihood is a good measure of model fit, except for the fact that more complex models will generally have higher likelihood scores

-   If we penalise these scores by some measure of the complexity of the model then we can compare models across complexities

-   The usual measure of complexity is some function of the number of parameters

-   Because these are relative model comparisons, the best model according to an IC might still be useless.

## Model information criteria {style="font-size: 70%;"}

To calculate an IC, the likelihood score gets transformed into the deviance (remember JAGS monitors the "deviance" parameter), which is minus twice the log-likelihood score and then add a model complexity term is added.

-   The two most common ICs are:

AIC : -2 log $\hat{L}$ + 2p

BIC : -2 log $\hat{L}$ + p log(n)

where p is the number of parameters and n is the number of observations

-   Smaller values indicate the preferred model.

## Model information criteria {style="font-size: 70%;"}

For Bayesian models it’s hard to know which value of L to use, seeing as at each iteration we get a different likelihood score. Two specific versions of IC have been developed.

-   The first, called the Deviance Information Criteria (DIC) is calculated via:

DIC: -2 log $p(y|\hat{\theta}) + 2 p_D$

where $p_D$ = 2(log $p(y|\hat{\theta})$ $- E_{post}$log $p(y|\theta))$ is the effective number of parameters

-   The second called the Widely Applicable Information Criterion (WAIC) which is calculated as:

WAIC: -2 log $p(y|\hat{\theta}) + 2 p_{WAIC}$

Here $p_{WAIC}$ - the effective number of parameters - is a measure of the variability of the likelihood scores

## Which IC to use? {style="font-size: 70%;"}

-   WAIC and DIC are built for Bayesian hierarchical models

-   DIC is included by default in the R2jags package

-   WAIC is included in the loo package which is installed alongside Stan

-   WAIC is considered superior as it also provides uncertainties on the values. Most of the others just give a single value

## Obtaining DIC and WAIC from JAGS {style="font-size: 70%;"}

-   DIC is easy to obtain from JAGS

```{r, echo = TRUE}
DIC.m1 = mod1$BUGSoutput$DIC
DIC.m1

DIC.m2 = mod2$BUGSoutput$DIC
DIC.m2
```

-   WAIC takes a little bit more work but there's some code [here](https://gist.github.com/oliviergimenez/68ad17910a62635ff6a062f8ec34292f#file-compute-waic-in-jags-r) that illustrates how to do it.

```{r}
samples.m1 <- jags.samples(mod1$model, 
                           c("WAIC","deviance"), 
                           type = "mean", 
                           n.iter = 5000,
                           n.burnin = 1000,
                           n.thin = 1)
samples.m1$p_waic <- samples.m1$WAIC
samples.m1$waic <- samples.m1$deviance + samples.m1$p_waic
tmp <- sapply(samples.m1, sum)
waic.m1 <- round(c(waic1 = tmp[["waic"]], p_waic1 = tmp[["p_waic"]]),1)
waic.m1

samples.m2 <- jags.samples(mod2$model, 
                           c("WAIC","deviance"), 
                           type = "mean", 
                           n.iter = 5000,
                           n.burnin = 1000,
                           n.thin = 1)
samples.m2$p_waic <- samples.m2$WAIC
samples.m2$waic <- samples.m2$deviance + samples.m2$p_waic
tmp <- sapply(samples.m2, sum)
waic.m2 <- round(c(waic2 = tmp[["waic"]], p_waic2 = tmp[["p_waic"]]),1)
waic.m2

```

## Cross Validation {style="font-size: 50%;"}

**Cross Validation (CV)** is a method for assessing model performance by evaluating how well it generalizes to new, unseen data.

### How Cross Validation Works:

1.  **Partition the Data**: Split the dataset into two parts: training and validation.
2.  **Train the Model**: Fit the model on the training subset.
3.  **Predict on Validation Set**: Use the trained model to predict values for the validation subset.
4.  **Evaluate Performance**: Compare predicted values with the actual values from the validation set to assess model accuracy.

### Types of Cross Validation:

-   **K-Fold Cross Validation**:
    -   Split the data into $k$ equal-sized "folds."
    -   Train the model $k$ times, each time leaving out one fold as the validation set and training on the remaining $k - 1$ folds.
    -   Average the performance across all folds for a more stable estimate.
-   **Leave-One-Out Cross Validation (LOO-CV)**:
    -   For small datasets, use LOO-CV, where each data point is left out one at a time as the validation set.
    -   This maximizes data usage for training, though it can be computationally intensive for large datasets.

<!-- ## Cross Validation {style="font-size: 70%;"} -->

<!-- Cross validation (CV) works by: -->

<!-- 1. Removing part of the data -->

<!-- 2. Fitting the model to the remaining part -->

<!-- 3. Predicting the values of the removed part -->

<!-- 4. Comparing the predictions with the true (left-out) values -->

<!-- It’s often fitted repeatedly -->

<!--   - as in k-fold CV where the data are divided up into k groups, and each group is left out in turn. -->

<!--   - In smaller data sets, people perform leave-one-out cross-validation (LOO-CV) -->
