[
  {
    "objectID": "0-Information.html#information",
    "href": "0-Information.html#information",
    "title": "Bayesian Data Analysis",
    "section": "Information",
    "text": "Information\n🏢 Office: Room 223, Logic House\n📧 Email: niamh.cahill@mu.ie\n📅 Lectures:\n\n📍 Mondays @11am in ELT\n\n📍 Tuesdays @10am in Physics Hall (PH)\n\n📚 Tutorials:\n\n📍 Wednesdays @3pm, 4pm, 5pm in GFLAB, starting Week 4 (Oct 14th)"
  },
  {
    "objectID": "0-Information.html#information-1",
    "href": "0-Information.html#information-1",
    "title": "Bayesian Data Analysis",
    "section": "Information",
    "text": "Information\n📝 Assessment:\n\n4 Assignments worth 10%\n1 Midterm worth 15% (November 11th)\nFinal Exam worth 75% (Date TBC)\n\n📖 Textbooks:\n\nDoing Bayesian Data Analysis by J. K. Kruschke\n\nBayesian Data Analysis by A. Gelman et al.\n\n⏰ Office Hours: By appointment (and please do make an appointment if needed)"
  },
  {
    "objectID": "0-Information.html#lecturing-format",
    "href": "0-Information.html#lecturing-format",
    "title": "Bayesian Data Analysis",
    "section": "Lecturing Format",
    "text": "Lecturing Format\n\n📝 Lecture notes will be provided, but feel free to take additional notes for your understanding.\n🌐 All course materials will be available on Moodle for easy access.\nLectures will be a mix of theory and practical examples, to help you apply what you learn.\n💻 R code will be provided along with lecture notes whenever relevant.\n❓ Please ask questions! Interaction is encouraged to help clarify any doubts."
  },
  {
    "objectID": "0-Information.html#what-is-this-course-about",
    "href": "0-Information.html#what-is-this-course-about",
    "title": "Bayesian Data Analysis",
    "section": "What is this Course About?",
    "text": "What is this Course About?\n\n🎯 The goal of this course is to introduce you to Bayesian Data Analysis (BDA).\n📚 BDA relies on two foundational ideas:\n\n🔄 The first idea is the updating of uncertainty across possibilities through Bayesian inference.\n🎲 The second idea is that parameter values are given probability distributions, known as “prior” distributions."
  },
  {
    "objectID": "0-Information.html#what-topics-will-we-cover",
    "href": "0-Information.html#what-topics-will-we-cover",
    "title": "Bayesian Data Analysis",
    "section": "What topics will we cover?",
    "text": "What topics will we cover?\nThis course will cover (but is not limited to) the following topics:\n\nBayes’ Rule\nInferring a Binomial Probability\nPrior Distributions\nMarkov Chain Monte Carlo\nGibbs Sampler\nAssessing Model Convergence\nJAGS\nSimple Models with JAGS\nHierarchical Models with JAGS\nModel Checking"
  },
  {
    "objectID": "0-Information.html#diversity-inclusion",
    "href": "0-Information.html#diversity-inclusion",
    "title": "Bayesian Data Analysis",
    "section": "Diversity & inclusion",
    "text": "Diversity & inclusion\nIt is my intent to present materials and activities that are respectful of diversity: gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, and culture. I may not always get this right so please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nTo help with this:\n\nIf you have a name that differs from those that appear in your official University records, please let me know!\nPlease let me know your preferred pronouns if you wish to do so.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "1a-BayesRule.html#bayes-rule",
    "href": "1a-BayesRule.html#bayes-rule",
    "title": "Bayesian Data Analysis",
    "section": "Bayes’ rule",
    "text": "Bayes’ rule\nThomas Bayes’ famous theorem was published in 1763.\n\nFor events A and B:\n\\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\]\n\nThe branch of statistics that you are probably most familiar with up to now is called frequentist statistics.\nBayesian statistics uses Bayes’ rule for inference and decision making, frequentist statistics does not."
  },
  {
    "objectID": "1a-BayesRule.html#crimes-example",
    "href": "1a-BayesRule.html#crimes-example",
    "title": "Bayesian Data Analysis",
    "section": "Crimes Example",
    "text": "Crimes Example\n🔍 A Crime Investigation (adapted from Kruschke):\n\nYou are investigating a crime and have 4 suspects: A, B, C, and D.\n\n👤 Suspect A\n\n👤 Suspect B\n\n👤 Suspect C\n\n👤 Suspect D\n\n🕵‍♂️ You are 100% sure the offender is one of these suspects, and initially, each one is considered equally likely to have committed the crime.\n❗ During your investigation, you discover that C did not commit the crime."
  },
  {
    "objectID": "1a-BayesRule.html#crimes-example-1",
    "href": "1a-BayesRule.html#crimes-example-1",
    "title": "Bayesian Data Analysis",
    "section": "Crimes Example",
    "text": "Crimes Example\nQuestion: Can you apply a Bayesian crime investigation to quantify the “information” about who committed the crime using probability statements?\n\n\nPrior Information\n\n👤 Suspect A\n\n👤 Suspect B\n\n👤 Suspect C\n👤 Suspect D\n\n\nNew Data\n\n👤 Suspect A\n\n👤 Suspect B\n\n❌👤 Suspect C (Ruled out)\n👤 Suspect D\n\n\nGiven what we know about C, what is the probability that A committed the crime?\n\n\nBefore data “all 4 equally likely”: Prob(A) = 1/4\nAfter data “C did not do it”: Prob(A|C did not do it) = 1/3"
  },
  {
    "objectID": "1a-BayesRule.html#crimes-example-2",
    "href": "1a-BayesRule.html#crimes-example-2",
    "title": "Bayesian Data Analysis",
    "section": "Crimes example",
    "text": "Crimes example\nIs that Bayesian learning? Yes!\n\n\nStep 1: Set prior distribution Prob(A) = Prob(B) = Prob(C) = Prob(D) = 1/4\nStep 2: Update using data “C did not do it” using Bayes’ rule\n\n\\[P(A|\\text{not } C) = \\frac{P(\\text{not } C|A)P(A)}{P(\\text{not } C)} = \\frac{1 \\times 1/4}{3/4}  = 1/3\\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example",
    "href": "1a-BayesRule.html#sleep-example",
    "title": "Bayesian Data Analysis",
    "section": "Sleep example",
    "text": "Sleep example\nSuppose that you are interested in the probability, that at any given time in the night between 8pm and 4am, you are asleep.\nLet’s assume we already have the following information on your sleep over this time period:\n\nThis is the probability I am asleep taking into account only the time."
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-1",
    "href": "1a-BayesRule.html#sleep-example-1",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\n\n\n\n\n\n💡 What if we know the time and have additional evidence?\n\n\n\nImagine this scenario: It’s 11pm, and we have extra evidence — my bedroom light is on. How would this affect the probability that I am asleep? \n\nThis is where we use Bayes’ Rule to update our sleep estimate. If we know information about the light, we can apply Bayes’ equation to refine the probability estimate:\nFor example, at 11pm, we can use the probability of me being asleep (our prior) and then adjust it using Bayes’ Rule based on the light being on: \\[\n  P(\\text{sleep}|\\text{light}) = \\frac{P(\\text{light}|\\text{sleep})P(\\text{sleep})}{P(\\text{light})}\n  \\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-2",
    "href": "1a-BayesRule.html#sleep-example-2",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\n💡 We added some information (data) about the light!\n\n\nTo update our sleep probability, we need to know the likelihood of observing this new information, given what we know about sleep habits.\n🔍 Key Probabilities Based on My Habits:\n\nIf I am asleep, the probability that my bedroom light is on is very low (1%):\n\\[P(\\text{light|sleep}) = 0.01\\]\nIf I am awake, the probability that my bedroom light is on is quite high (80%):\n\\[P(\\text{light|−sleep}) = 0.8\\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-3",
    "href": "1a-BayesRule.html#sleep-example-3",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\n💡 We had our prior information about sleep and added some information (data) about the light!\n\nThe final piece of the equation is \\(P(\\text{light})\\), representing the total probability my light is on.\nThere are two possible conditions where my light could be on:\n\nI am asleep\n\nI am awake\n\n\nTo calculate the total probability, we use:\n\\[\nP(\\text{light}) = P(\\text{light|sleep})P(\\text{sleep}) + P(\\text{light|−sleep})P(\\text{−sleep})\n\\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-4",
    "href": "1a-BayesRule.html#sleep-example-4",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\nNow, let’s put it all together with Bayes’ Rule:\n\\(\\underset{P(A|B)}{P(\\text{sleep}|\\text{light})} =\\)\n\n\n\\[\n\\frac{\\underset{P(B|A)}{P(\\text{light}|\\text{sleep})} \\cdot \\underset{P(A)}{P(\\text{sleep})}}{\\underset{P(B)}{P(\\text{light})}}\n\\]\n\n\\[\n\\frac{\\underset{0.01}{P(\\text{light}|\\text{sleep})} \\cdot \\underset{0.27}{P(\\text{sleep})}}{\\underset{0.59}{P(\\text{light})}}\n\\]\n\n🎯 This allows us to update our belief about the probability that I’m asleep based on the new evidence (the light being on)!"
  },
  {
    "objectID": "1a-BayesRule.html#likelihood-prior-posterior",
    "href": "1a-BayesRule.html#likelihood-prior-posterior",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood, Prior & Posterior",
    "text": "Likelihood, Prior & Posterior"
  },
  {
    "objectID": "1a-BayesRule.html#bayes-rule-applied-to-parameters-and-data",
    "href": "1a-BayesRule.html#bayes-rule-applied-to-parameters-and-data",
    "title": "Bayesian Data Analysis",
    "section": "Bayes’ rule applied to parameters and data",
    "text": "Bayes’ rule applied to parameters and data\nGiven a set of observed data points \\(Y\\) and a set of parameters \\(\\theta\\), we write Bayes’ rule as \\[\\underset{\\text{posterior}}{P(\\theta|Y)} = \\frac{\\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}}{\\underset{\\text{marginal likelihood}}{P(Y)}}\\]\nWhere the denominator is\n\n\\(P(Y) = \\sum_{\\theta^*}P(Y|\\theta^*)P(\\theta^*)\\) for discrete-valued variables, or\n\\(P(Y) = \\int P(Y|\\theta^*)P(\\theta^*) d\\theta^*\\) for continuous variables.\n\n\\(P(Y)\\) is often difficult to calculate (more on this later) and Baye’s rule is often written more simply as a proportional statement \\[\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\]"
  },
  {
    "objectID": "1a-BayesRule.html#likelihood-prior-posterior-1",
    "href": "1a-BayesRule.html#likelihood-prior-posterior-1",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood, Prior & Posterior",
    "text": "Likelihood, Prior & Posterior\n\\[\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\]\n\n\n\n\\(P(Y|\\theta)\\) - the likelihood. The likelihood represents the evidence that we have based on data.\n\\(P(\\theta)\\) - the prior. The prior represents what we know about the parameters before the data are observed.\n\\(P(\\theta|Y)\\) - the posterior. The posterior represents our updated knowledge about parameters after the data are observed."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#recall-bayes-rule",
    "href": "1b-InferringBinomialProp.html#recall-bayes-rule",
    "title": "Bayesian Data Analysis",
    "section": "Recall: Bayes’ rule",
    "text": "Recall: Bayes’ rule\nGiven a set of observed data points \\(y\\) and a set of parameters \\(\\theta\\), we write Bayes’ rule as\n\\[\\underset{\\text{posterior}}{P(\\theta|y)} = \\frac{\\underset{\\text{likelihood}}{P(y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}}{\\underset{\\text{marginal likelihood}}{P(y)}}\\] and as a proportional statement\n\\[\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\]\nWe will now consider an example that will build some intuition for how prior distributions and data interact to produce posterior distributions."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#what-proportion-of-earths-surface-is-covered-with-water",
    "href": "1b-InferringBinomialProp.html#what-proportion-of-earths-surface-is-covered-with-water",
    "title": "Bayesian Data Analysis",
    "section": "What proportion of Earth’s surface is covered with water?",
    "text": "What proportion of Earth’s surface is covered with water?"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth",
    "href": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth",
    "title": "Bayesian Data Analysis",
    "section": "Estimating the Proportion of Water on Earth",
    "text": "Estimating the Proportion of Water on Earth\nImagine you want to estimate how much of the Earth’s surface is covered in water. 🌍💧\n\n🟢 The Experiment: You throw a blow-up globe into the air, and wherever your index finger lands, you record an observation of either:\n\nWater 🌊\nLand 🌍\n\n🔄 Repeat this process multiple times, collecting a dataset of binary outcomes (water or land).\n📊 Your Data: As you accumulate observations, you’ll begin to estimate the proportion of the Earth that’s covered by water based on how often your finger lands on water.\n\nThis simple, hands-on experiment can give you an idea of the Earth’s water coverage!"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth-1",
    "href": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth-1",
    "title": "Bayesian Data Analysis",
    "section": "Estimating the Proportion of Water on Earth",
    "text": "Estimating the Proportion of Water on Earth"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#water-on-the-globe-example",
    "href": "1b-InferringBinomialProp.html#water-on-the-globe-example",
    "title": "Bayesian Data Analysis",
    "section": "Water on the Globe Example",
    "text": "Water on the Globe Example\n🎯 What is our goal? Estimate the proportion of water on the globe, denoted as θ (theta).\n📊 What data do we have?\n\nData Collected: L, W, L, L, W, W, W, L, W, W\nTotal Throws: n = 10\nWater Observations: y = 6\n\n🔍 How do we perform Bayesian inference for θ?\n\nModel the Data: Choose a descriptive model for the data, known as the likelihood, which includes θ (the proportion of water).\nPrior Information: Summarize existing knowledge about θ using a prior probability distribution.\nUpdate with Data: Combine the prior with the collected data using Bayes’ rule to obtain the posterior distribution for θ. This refines our estimate of the proportion of water on the globe based on the evidence we’ve gathered!"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#brief-recap-types-of-statistical-distributions",
    "href": "1b-InferringBinomialProp.html#brief-recap-types-of-statistical-distributions",
    "title": "Bayesian Data Analysis",
    "section": "Brief Recap: Types of Statistical Distributions",
    "text": "Brief Recap: Types of Statistical Distributions\nDifferent types of distributions are used to describe data (or parameters), here’s some examples:\n\n\nNormal Distribution\n\nDescription: Bell-shaped curve, symmetric around the mean.\nExample: Heights of people, IQ scores.\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\nDescription: Discrete distribution of the # of successes in a fixed # of trials.\nExample: Presence/absence of a disease\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Distribution\n\nDescription: Discrete distribution for the # of events in a fixed interval.\nExample: # of emails received in an hour."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#likelihood-function---pytheta",
    "href": "1b-InferringBinomialProp.html#likelihood-function---pytheta",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood Function - \\(p(y|\\theta)\\)",
    "text": "Likelihood Function - \\(p(y|\\theta)\\)\nWe have data: n = 10, y = 6. We now need a data model.\nWe’ll model this particular type of data using a Binomial Distribution.\n\nAssumption: \\(y\\) follows a Binomial distribution with parameters \\((\\theta, n)\\), where:\n\\[p(y|\\theta) = c\\theta^y(1-\\theta)^{n-y} \\text{ with } c = {n \\choose y}\\]\nExplanation:\n\n\\(p(y|\\theta)\\): The likelihood function shows the probability of observing y water outcomes out of n throws, given the proportion of water θ.\n\\(c\\): The combinatorial factor that accounts for the number of ways to choose y successes out of n trials.\n\n\nWhy This Matters: This function tells us how our observed data (water outcomes) relate to the parameter we want to estimate (θ)."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#prior-distribution---ptheta",
    "href": "1b-InferringBinomialProp.html#prior-distribution---ptheta",
    "title": "Bayesian Data Analysis",
    "section": "Prior distribution - \\(p(\\theta)\\)",
    "text": "Prior distribution - \\(p(\\theta)\\)\nNow that we’ve defined the data model, the next step is to establish a prior distribution over the parameter values.\n\nLet’s start simple and assume \\(\\theta\\) can only take on values k = \\(0,0.25,0.5,0.75,1\\).\nSuppose that we believe that \\(\\theta\\) is most likely to be 0.5 and we assign lower weight to \\(\\theta\\) values far above or below 0.5.\nA prior distribution incorporating these beliefs might look like:"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#likelihood-prior",
    "href": "1b-InferringBinomialProp.html#likelihood-prior",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood & Prior",
    "text": "Likelihood & Prior\nGiven that y = 6 and n = 10 with \\(\\frac{y}{n} = 0.6\\), which \\(\\theta\\) out of \\(0,0.25,0.5,0.75,1\\) do you expect to have the largest value of the likelihood function?"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#posterior-distribution---undersettextposteriorpthetay-propto-undersettextlikelihoodpythetaundersettextpriorptheta",
    "href": "1b-InferringBinomialProp.html#posterior-distribution---undersettextposteriorpthetay-propto-undersettextlikelihoodpythetaundersettextpriorptheta",
    "title": "Bayesian Data Analysis",
    "section": "Posterior distribution - \\(\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\)",
    "text": "Posterior distribution - \\(\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\)"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#changing-prior-assumptions-1",
    "href": "1b-InferringBinomialProp.html#changing-prior-assumptions-1",
    "title": "Bayesian Data Analysis",
    "section": "Changing prior assumptions (1)",
    "text": "Changing prior assumptions (1)\nInstead of the “triangular” prior let’s make a different assumption where we assume 0.75 is most likely and 0.5 is somewhat likely."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#changing-prior-assumptions-2",
    "href": "1b-InferringBinomialProp.html#changing-prior-assumptions-2",
    "title": "Bayesian Data Analysis",
    "section": "Changing prior assumptions (2)",
    "text": "Changing prior assumptions (2)\nInstead of the “triangular” prior let’s make a more uniform assumption. So for \\(k = 0,0.25,0.5,0.75,1\\), \\(Pr(\\theta = k) = 1/5\\) (i.e., all are equally likely)."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#marginal-likelihood---py",
    "href": "1b-InferringBinomialProp.html#marginal-likelihood---py",
    "title": "Bayesian Data Analysis",
    "section": "Marginal likelihood - \\(p(y)\\)",
    "text": "Marginal likelihood - \\(p(y)\\)\nRecall: \\[\\underset{\\text{posterior}}{P(\\theta|y)} = \\frac{\\underset{\\text{likelihood}}{P(y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}}{\\underset{\\text{marginal likelihood}}{P(y)}}\\]\nWhat is \\(P(y)\\)?\n\\[P(y) = \\sum_{\\theta^*} P(y|\\theta^*)P(\\theta^*)\\]\nSo for \\(k = 0,0.25,0.5,0.75,1\\), \\(Pr(\\theta = k) = 1/5\\) (i.e., all are equally likely)\n\\(P(y) = p(y|\\theta = 0)Pr(\\theta = 0) + P(y|\\theta = 0.25)Pr(\\theta = 0.25) + \\ldots = 0.073\\)\nTo do this in R:\n\nn_grid = 5\ntheta &lt;- seq(0,1,length = n_grid) \np_y &lt;- (1/n_grid)*(sum(dbinom(6, 10, prob = theta)))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "",
    "text": "Welcome to the course website for ST405 (code share ST645) Bayesian Data Analysis!\nModule information\nLecture material (slides, notes, videos) are licensed under CC-BY-NC 4.0.\nContact: Niamh Cahill (niamh.cahill@mu.ie)"
  },
  {
    "objectID": "index.html#lecture-slides",
    "href": "index.html#lecture-slides",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Lecture Slides",
    "text": "Lecture Slides\n\nWeek 1\n1a: Bayes Rule\n1b: Inferring a Binomial Probability\n\n\nWeek 2\n2a: Beta Binomial\n2b: Bayesian Inference"
  },
  {
    "objectID": "2a-BetaBinomial.html#scenario",
    "href": "2a-BetaBinomial.html#scenario",
    "title": "Bayesian Data Analysis",
    "section": "Scenario:",
    "text": "Scenario:\nSuppose females, aged 65+ in a general social survey were asked about being happy.\nIf this is a representative sample of the population of women, what is the probability that a 65+ woman is happy?"
  },
  {
    "objectID": "2a-BetaBinomial.html#what-is-our-goal-what-data-do-we-have",
    "href": "2a-BetaBinomial.html#what-is-our-goal-what-data-do-we-have",
    "title": "Bayesian Data Analysis",
    "section": "What is Our Goal? What Data do We Have?",
    "text": "What is Our Goal? What Data do We Have?\n🎯 Goal:\nTo estimate the probability that a 65+ woman is happy.\n\nThis is an unknown parameter we’ll call \\(\\theta\\).\n\\(\\theta\\) represents the probability of a 65+ woman being happy.\n\n📊 Data:\n\nSample Size (n): 20 women\nReported Happy (y): 14 women reported being happy"
  },
  {
    "objectID": "2a-BetaBinomial.html#bayesian-inference-for-theta",
    "href": "2a-BetaBinomial.html#bayesian-inference-for-theta",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Inference for \\(\\theta\\)",
    "text": "Bayesian Inference for \\(\\theta\\)\n🔍 Steps in Bayesian Inference:\n\nLikelihood (Model the Data):\n\nWe define a model for the data, describing how the probability \\(\\theta\\) (probability a 65+ woman is happy) fits.\n\nPrior Information:\n\nBefore looking at data, we summarize information about \\(\\theta\\) in a prior distribution.\n\nUpdating with Data:\n\nBayes’ Rule helps us update our prior with the data to obtain the posterior distribution, reflecting what we know about \\(\\theta\\) after considering the data."
  },
  {
    "objectID": "2a-BetaBinomial.html#the-bayesian-formula",
    "href": "2a-BetaBinomial.html#the-bayesian-formula",
    "title": "Bayesian Data Analysis",
    "section": "The Bayesian Formula",
    "text": "The Bayesian Formula\n\\[\n\\text{Posterior}(\\theta | \\text{data}) \\propto \\text{Likelihood}(\\text{data} | \\theta) \\cdot \\text{Prior}(\\theta)\n\\]\nLater we can visualize the prior, likelihood, and posterior distributions to see how Bayesian updating works in practice, for this example.\nFirst let’s workout what the posterior is, based on assuming an appropriate likelihood and prior.\n❓ Questions:\n\nWhat likelihood would you choose here?\nWhat are some constraints we need to think about when choosing a prior for \\(\\theta\\)?"
  },
  {
    "objectID": "2a-BetaBinomial.html#the-happiness-example",
    "href": "2a-BetaBinomial.html#the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "The Happiness example",
    "text": "The Happiness example\nFor the Happiness example:\n\nData: n = 20 women, y = 14 women reported being happy\n\\(y \\sim Binomial(n = 20, \\theta)\\)\n\n\\[p(y|\\theta) = c\\theta^y(1-\\theta)^{n-y} \\text{ with } c = {n \\choose y}\\]\n\nWe want to find the posterior distribution for \\(\\theta\\)\n\nNow we will consider defining the prior, \\(p(\\theta)\\), with a known probability distribution, such that:\n\\[\\theta \\sim Beta(a,b)\\]"
  },
  {
    "objectID": "2a-BetaBinomial.html#the-beta-prior",
    "href": "2a-BetaBinomial.html#the-beta-prior",
    "title": "Bayesian Data Analysis",
    "section": "The Beta Prior",
    "text": "The Beta Prior\nA Beta distribution is defined on the interval [0,1] and has two parameters, \\(a\\) and \\(b\\). The density function is defined as:\n\\[p(\\theta|a,b) = \\frac{1}{B(a,b)}\\theta^{a-1}(1-\\theta)^{b-1}\\]\nwhere \\(B(a,b)\\) is a normalising constant that insures a valid probability density function.\nIf \\(\\theta \\sim Be(a,b)\\) then \\(E(\\theta) = \\frac{a}{a+b}\\) and \\(Var(\\theta) = \\frac{ab}{(a+b)^2(a+b+1)}\\)\nNote \\(B(a,b)\\) is not a function of \\(\\theta\\), so we can write\n\\[p(\\theta|a,b) \\propto \\theta^{a-1}(1-\\theta)^{b-1}\\]\nThis will become useful later."
  },
  {
    "objectID": "2a-BetaBinomial.html#the-beta-prior-with-a1-and-b1",
    "href": "2a-BetaBinomial.html#the-beta-prior-with-a1-and-b1",
    "title": "Bayesian Data Analysis",
    "section": "The Beta Prior with a=1 and b=1",
    "text": "The Beta Prior with a=1 and b=1\nLet’s use Bayes’ theorem now to find the form of the posterior distribution for \\(\\theta\\) assuming \\(\\theta \\sim Beta(a=1,b=1)\\).\nThis means we’ve assumed a prior mean and variance for \\(\\theta\\) of \\(\\frac{1}{2}\\) and \\(\\frac{1}{12}\\) respectively.\nSo the posterior is\n\\[\\underset{\\text{posterior}}{p(\\theta|y)} \\propto \\underset{\\text{likelihood}}{\\theta^y(1-\\theta)^{n-y} }\\underset{\\text{prior}}{\\theta^{a-1}(1-\\theta)^{b-1}}\\]\nand given a = 1 and b = 1\n\\[\\underset{\\text{posterior}}{p(\\theta|y)} \\propto {\\theta^y(1-\\theta)^{n-y}}\\]\nThis posterior actually takes the form of another Beta distribution with parameters \\(y+1\\) and \\(n-y+1\\). So, \\[\\theta|y \\sim Beta(y+1, n-y +1)\\]"
  },
  {
    "objectID": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example",
    "href": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "What does this look like for the Happiness example?",
    "text": "What does this look like for the Happiness example?\n\nVisualise the PriorVisualise the LikelihoodVisualise the Posteior\n\n\n\n\n# Beta parameters (prior)\na &lt;- 1\nb &lt;- 1\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nprior &lt;- dbeta(theta,a,b) # get the prior distribution\n\n# create a dataset\nptheta_dat &lt;- tibble::tibble(theta, prior)\n\n# plot prior\nggplot2::ggplot(ptheta_dat, aes(x = theta, y = prior)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Data\ny &lt;- 14\nn &lt;- 20\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nlikelihood &lt;- dbinom(y,n,prob = theta) # get the likelihood distribution\n\n# create a dataset\nltheta_dat &lt;- tibble::tibble(theta, likelihood)\n\n# plot likelihood\nggplot2::ggplot(ltheta_dat, aes(x = theta, y = likelihood)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Data\ny &lt;- 14\nn &lt;- 20\n\n# Beta parameters (posterior)\na_post &lt;- y+1\nb_post &lt;- n-y+1\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nposterior &lt;- dbeta(theta,a_post,b_post) # get the posterior distribution\n\n# create a dataset\nposttheta_dat &lt;- tibble::tibble(theta, posterior)\n\n# plot posterior\nggplot2::ggplot(posttheta_dat, aes(x = theta, y = posterior)) +\n  geom_line()"
  },
  {
    "objectID": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example-1",
    "href": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example-1",
    "title": "Bayesian Data Analysis",
    "section": "What does this look like for the Happiness example?",
    "text": "What does this look like for the Happiness example?"
  },
  {
    "objectID": "2a-BetaBinomial.html#more-on-the-binomial-likelihood-and-the-beta-prior",
    "href": "2a-BetaBinomial.html#more-on-the-binomial-likelihood-and-the-beta-prior",
    "title": "Bayesian Data Analysis",
    "section": "More on the Binomial Likelihood and the Beta Prior",
    "text": "More on the Binomial Likelihood and the Beta Prior\nIt turns out anytime you use a Binomial likelihood and a Beta prior, such that:\n\\[\\theta \\sim Be(a,b)\\]\n\\[y \\sim Binomal(n,\\theta)\\]\nthen you get a posterior distribution which is also Beta, where\n\\[\\theta|y \\sim Beta(y+a, n-y +b)\\]\nWhen the posterior is the same form as the prior, the prior is said to be a conjugate prior. The Beta prior is a conjugate prior for the Binomial likelihood."
  },
  {
    "objectID": "2a-BetaBinomial.html#expressing-prior-knowledge",
    "href": "2a-BetaBinomial.html#expressing-prior-knowledge",
    "title": "Bayesian Data Analysis",
    "section": "Expressing prior knowledge",
    "text": "Expressing prior knowledge\nSuppose for the Happiness example, you want to express your underlying belief about \\(\\theta\\) - the probability a woman age 65+ is happy.\n\nYour beliefs may be based on previous studies or perhaps expert opinion.\nSo for example, suppose you want your prior to reflect beliefs that the proportion is 0.6 \\(\\pm\\) 0.1.\nHow do we express this belief in the Beta distribution?\n\n\n\n🛠️ Moment Matching: Our Tool to Match Beliefs to a Distribution\n\nWe use a technique called moment matching to convert these beliefs\n(mean = 0.6, sd = 0.1) into the parameters of a Beta distribution."
  },
  {
    "objectID": "2a-BetaBinomial.html#moment-matching",
    "href": "2a-BetaBinomial.html#moment-matching",
    "title": "Bayesian Data Analysis",
    "section": "Moment Matching",
    "text": "Moment Matching\nRecall if \\(\\theta \\sim Be(a,b)\\) then \\(E(\\theta) = \\frac{a}{a+b}\\) and \\(Var(\\theta) = \\frac{ab}{(a+b)^2(a+b+1)}\\)\nBased on our prior beliefs we want:\n\\(E(\\theta) = \\frac{a}{a+b} = 0.6\\)\n\\(Var(\\theta) = \\frac{ab}{(a+b)^2(a+b+1)} = 0.1^2\\)\nWe can use these equations to solve for \\(a\\) and \\(b\\), the parameters of the Beta prior.\n\nVisualise the Prior\n\n\n\n\n# Beta parameters (prior)\na &lt;- 23\nb &lt;- 16\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nprior &lt;- dbeta(theta,a,b) # get the prior distribution\n\n# create a dataset\nptheta_dat &lt;- tibble::tibble(theta, prior)\n\n# plot prior\nggplot2::ggplot(ptheta_dat, aes(x = theta, y = prior)) +\n  geom_line()"
  },
  {
    "objectID": "2a-BetaBinomial.html#how-does-this-prior-impact-the-posterior",
    "href": "2a-BetaBinomial.html#how-does-this-prior-impact-the-posterior",
    "title": "Bayesian Data Analysis",
    "section": "How does this prior impact the posterior?",
    "text": "How does this prior impact the posterior?"
  },
  {
    "objectID": "2a-BetaBinomial.html#the-posterior-is-a-compromise-of-prior-and-likelihood",
    "href": "2a-BetaBinomial.html#the-posterior-is-a-compromise-of-prior-and-likelihood",
    "title": "Bayesian Data Analysis",
    "section": "The posterior is a compromise of prior and likelihood",
    "text": "The posterior is a compromise of prior and likelihood\nThe posterior distribution is always a compromise between the prior distribution and the likelihood function.\n\nWe can illustrate this easily with the Beta-Binomial example.\nWe’ve seen that for a \\(Be(a,b)\\) prior and a \\(Binomial(n,\\theta)\\) likelihood that the posterior will be of the form:\n\n\\[\\theta|y \\sim Beta(y+a, n-y +b)\\]\nand so the posterior mean is \\(E(\\theta|y) = \\frac{y+a}{n + a + b}\\).\n\nThis can be written as a weighted sum of the prior mean (\\(\\frac{a}{a+b}\\)) and the data proportion (\\(\\frac{y}{n}\\)), as follows: \\(E(\\theta|y) = \\underbrace{\\small\\frac{y}{n}}_{data}\\underbrace{\\small\\frac{n}{n+a+b}}_{weight} + \\underbrace{\\small\\frac{a}{a+b}}_{prior}\\underbrace{\\small\\frac{a+b}{n+a+b}}_{weight}\\)"
  },
  {
    "objectID": "2b-BayesianInference.html#bayesian-inference",
    "href": "2b-BayesianInference.html#bayesian-inference",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\n\nBayesian point estimates are often given by:\n\nthe posterior mean \\(E(\\theta|y)\\)\nor the posterior median \\(\\theta^*\\) with \\(P(\\theta &lt; \\theta^*|y) = 0.5\\)\n\nUncertainty is quantified with credible intervals (CIs)\n\nAn interval is a 95% credible interval if the posterior probability that \\(\\theta\\) is in the interval is 0.95.\nOften quantile based, given by posterior quantiles with \\(P(\\theta &lt; \\theta_{\\alpha/2}|y) = P(\\theta &gt; \\theta_{1-\\alpha/2}|y) = \\alpha/2\\)"
  },
  {
    "objectID": "2b-BayesianInference.html#bayesian-inference-for-the-happiness-example",
    "href": "2b-BayesianInference.html#bayesian-inference-for-the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Inference for the Happiness example",
    "text": "Bayesian Inference for the Happiness example\n\n\\(\\theta|y \\sim Beta(y+a,n-y+b)\\)\nPosterior mean \\(E(\\theta|y) = \\frac{y+a}{n+a+b}\\)\nFor quantile estimates we can use qbeta() in R\n\n\nR Code\n\n\n\n\n## data \nn = 20; y = 14\n\n## prior parameters\na = 1; b = 1\n\n## posterior parameters\na_post = y + a; b_post = n - y + b\n\n## posterior mean\np_mean &lt;- a_post/(a_post + b_post) # (y+a)/(n+a+b)\n\n## quantiles\np_quant &lt;- qbeta(c(0.025,0.5,0.975),a_post,b_post)\n\np_mean; p_quant\n\n\n[1] 0.6818182\n\n\n[1] 0.4782489 0.6874159 0.8541231"
  },
  {
    "objectID": "2b-BayesianInference.html#simulation-based-inference",
    "href": "2b-BayesianInference.html#simulation-based-inference",
    "title": "Bayesian Data Analysis",
    "section": "Simulation-based inference",
    "text": "Simulation-based inference\n\nThe general idea in simulation-based inference: We can make inference about a parameter \\(\\theta\\) , using a sample \\(\\{\\theta^{(1)}\\ldots\\theta^{(S)}\\}\\) from its probability distribution.\nAssessing the properties of a target (e.g., posterior) distribution by generating representative samples is called Monte Carlo simulation.\nBased on the law of large numbers we know that:\n\\(\\frac{1}{S}\\sum_{s=1}^{S}\\theta^{(s)} = E(\\theta)\\)\nas sample size \\(S \\to \\infty\\)\n\nThe error in the MC approximation goes to zero as \\(S \\to \\infty\\) because \\(\\frac{var(\\theta)}{S} \\to 0\\)\n\nJust about any aspect of the distribution of \\(\\theta\\) can be approximated arbitrarily exactly with a large enough Monte Carlo sample, e.g.\n\nthe \\(\\alpha\\)-percentile of the distribution of \\(\\theta\\)\n\\(Pr(\\theta \\geq x)\\) for any constant \\(x\\)"
  },
  {
    "objectID": "2b-BayesianInference.html#simulation-based-inference-for-the-happiness-example",
    "href": "2b-BayesianInference.html#simulation-based-inference-for-the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "Simulation-based inference for the Happiness example",
    "text": "Simulation-based inference for the Happiness example\nFor the Happiness example, we can approximate the mean and quantiles of \\(\\theta\\) using samples from a \\(Be(y+a,n-y+b)\\) distribution (i.e., the posterior)\n\nR Code\n\n\n\n\n## data \nn = 20; y = 14\n\n## prior parameters\na = 1; b = 1\n\n## posterior parameters\na_post = y + a; b_post = n - y + b\n\n## sample \nsamp_theta &lt;- rbeta(1000,a_post,b_post)\n\n## sample mean and quantiles\np_mean &lt;- mean(samp_theta)\np_quant &lt;- quantile(samp_theta, \n                    probs = c(0.025,0.5,0.975))\n\np_mean; p_quant\n\n\n[1] 0.6795585\n\n\n     2.5%       50%     97.5% \n0.4658872 0.6827372 0.8493420"
  },
  {
    "objectID": "2b-BayesianInference.html#monte-carlo-approximation-some-more-details",
    "href": "2b-BayesianInference.html#monte-carlo-approximation-some-more-details",
    "title": "Bayesian Data Analysis",
    "section": "Monte Carlo approximation: some more details",
    "text": "Monte Carlo approximation: some more details\n\nWith a simulation, it also becomes very easy to analyze the distributions of any function of your parameter,\n\ne.g. the distribution of the odds \\(\\frac{\\theta}{1-\\theta}\\) by using samples from \\(\\frac{\\theta^{(s)}}{1-\\theta^{(s)}}\\)\n\n\n\nR Code\n\n\n\n\n## sample \nsamp_theta &lt;- rbeta(1000,a_post,b_post)\n\n## get odds based on samples\nsamp_odds &lt;- samp_theta/(1-samp_theta)\n\n## sample mean and quantiles\np_mean_odds &lt;- mean(samp_odds)\np_quant_odds &lt;- quantile(samp_odds, \n                         probs = c(0.025,0.5,0.975))\n\np_mean_odds; p_quant_odds\n\n\n[1] 2.553325\n\n\n     2.5%       50%     97.5% \n0.8895412 2.1687181 6.5592201"
  }
]