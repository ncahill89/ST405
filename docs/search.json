[
  {
    "objectID": "0-Information.html#information",
    "href": "0-Information.html#information",
    "title": "Bayesian Data Analysis",
    "section": "Information",
    "text": "Information\n🏢 Office: Room 223, Logic House\n📧 Email: niamh.cahill@mu.ie\n📅 Lectures:\n\n📍 Mondays @11am in ELT\n\n📍 Tuesdays @10am in Physics Hall (PH)\n\n📚 Tutorials:\n\n📍 Wednesdays @3pm, 4pm, 5pm in GFLAB, starting Week 4 (Oct 14th)"
  },
  {
    "objectID": "0-Information.html#information-1",
    "href": "0-Information.html#information-1",
    "title": "Bayesian Data Analysis",
    "section": "Information",
    "text": "Information\n📝 Assessment:\n\n4 Assignments worth 10%\n1 Midterm worth 15% (November 11th)\nFinal Exam worth 75% (Date TBC)\n\n📖 Textbooks:\n\nDoing Bayesian Data Analysis by J. K. Kruschke\n\nBayesian Data Analysis by A. Gelman et al.\n\n⏰ Office Hours: By appointment (and please do make an appointment if needed)"
  },
  {
    "objectID": "0-Information.html#lecturing-format",
    "href": "0-Information.html#lecturing-format",
    "title": "Bayesian Data Analysis",
    "section": "Lecturing Format",
    "text": "Lecturing Format\n\n📝 Lecture notes will be provided, but feel free to take additional notes for your understanding.\n🌐 All course materials will be available on Moodle for easy access.\nLectures will be a mix of theory and practical examples, to help you apply what you learn.\n💻 R code will be provided along with lecture notes whenever relevant.\n❓ Please ask questions! Interaction is encouraged to help clarify any doubts."
  },
  {
    "objectID": "0-Information.html#what-is-this-course-about",
    "href": "0-Information.html#what-is-this-course-about",
    "title": "Bayesian Data Analysis",
    "section": "What is this Course About?",
    "text": "What is this Course About?\n\n🎯 The goal of this course is to introduce you to Bayesian Data Analysis (BDA).\n📚 BDA relies on two foundational ideas:\n\n🔄 The first idea is the updating of uncertainty across possibilities through Bayesian inference.\n🎲 The second idea is that parameter values are given probability distributions, known as “prior” distributions."
  },
  {
    "objectID": "0-Information.html#what-topics-will-we-cover",
    "href": "0-Information.html#what-topics-will-we-cover",
    "title": "Bayesian Data Analysis",
    "section": "What topics will we cover?",
    "text": "What topics will we cover?\nThis course will cover (but is not limited to) the following topics:\n\nBayes’ Rule\nInferring a Binomial Probability\nPrior Distributions\nMarkov Chain Monte Carlo\nGibbs Sampler\nAssessing Model Convergence\nJAGS\nSimple Models with JAGS\nHierarchical Models with JAGS\nModel Checking"
  },
  {
    "objectID": "0-Information.html#diversity-inclusion",
    "href": "0-Information.html#diversity-inclusion",
    "title": "Bayesian Data Analysis",
    "section": "Diversity & inclusion",
    "text": "Diversity & inclusion\nIt is my intent to present materials and activities that are respectful of diversity: gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, and culture. I may not always get this right so please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nTo help with this:\n\nIf you have a name that differs from those that appear in your official University records, please let me know!\nPlease let me know your preferred pronouns if you wish to do so.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "1a-BayesRule.html#bayes-rule",
    "href": "1a-BayesRule.html#bayes-rule",
    "title": "Bayesian Data Analysis",
    "section": "Bayes’ rule",
    "text": "Bayes’ rule\nThomas Bayes’ famous theorem was published in 1763.\n\nFor events A and B:\n\\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\]\n\nThe branch of statistics that you are probably most familiar with up to now is called frequentist statistics.\nBayesian statistics uses Bayes’ rule for inference and decision making, frequentist statistics does not."
  },
  {
    "objectID": "1a-BayesRule.html#crimes-example",
    "href": "1a-BayesRule.html#crimes-example",
    "title": "Bayesian Data Analysis",
    "section": "Crimes Example",
    "text": "Crimes Example\n🔍 A Crime Investigation (adapted from Kruschke):\n\nYou are investigating a crime and have 4 suspects: A, B, C, and D.\n\n👤 Suspect A\n\n👤 Suspect B\n\n👤 Suspect C\n\n👤 Suspect D\n\n🕵‍♂️ You are 100% sure the offender is one of these suspects, and initially, each one is considered equally likely to have committed the crime.\n❗ During your investigation, you discover that C did not commit the crime."
  },
  {
    "objectID": "1a-BayesRule.html#crimes-example-1",
    "href": "1a-BayesRule.html#crimes-example-1",
    "title": "Bayesian Data Analysis",
    "section": "Crimes Example",
    "text": "Crimes Example\nQuestion: Can you apply a Bayesian crime investigation to quantify the “information” about who committed the crime using probability statements?\n\n\nPrior Information\n\n👤 Suspect A\n\n👤 Suspect B\n\n👤 Suspect C\n👤 Suspect D\n\n\nNew Data\n\n👤 Suspect A\n\n👤 Suspect B\n\n❌👤 Suspect C (Ruled out)\n👤 Suspect D\n\n\nGiven what we know about C, what is the probability that A committed the crime?\n\n\nBefore data “all 4 equally likely”: Prob(A) = 1/4\nAfter data “C did not do it”: Prob(A|C did not do it) = 1/3"
  },
  {
    "objectID": "1a-BayesRule.html#crimes-example-2",
    "href": "1a-BayesRule.html#crimes-example-2",
    "title": "Bayesian Data Analysis",
    "section": "Crimes example",
    "text": "Crimes example\nIs that Bayesian learning? Yes!\n\n\nStep 1: Set prior distribution Prob(A) = Prob(B) = Prob(C) = Prob(D) = 1/4\nStep 2: Update using data “C did not do it” using Bayes’ rule\n\n\\[P(A|\\text{not } C) = \\frac{P(\\text{not } C|A)P(A)}{P(\\text{not } C)} = \\frac{1 \\times 1/4}{3/4}  = 1/3\\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example",
    "href": "1a-BayesRule.html#sleep-example",
    "title": "Bayesian Data Analysis",
    "section": "Sleep example",
    "text": "Sleep example\nSuppose that you are interested in the probability, that at any given time in the night between 8pm and 4am, you are asleep.\nLet’s assume we already have the following information on your sleep over this time period:\n\nThis is the probability I am asleep taking into account only the time."
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-1",
    "href": "1a-BayesRule.html#sleep-example-1",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\n\n\n\n\n\n💡 What if we know the time and have additional evidence?\n\n\n\nImagine this scenario: It’s 11pm, and we have extra evidence — my bedroom light is on. How would this affect the probability that I am asleep? \n\nThis is where we use Bayes’ Rule to update our sleep estimate. If we know information about the light, we can apply Bayes’ equation to refine the probability estimate:\nFor example, at 11pm, we can use the probability of me being asleep (our prior) and then adjust it using Bayes’ Rule based on the light being on: \\[\n  P(\\text{sleep}|\\text{light}) = \\frac{P(\\text{light}|\\text{sleep})P(\\text{sleep})}{P(\\text{light})}\n  \\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-2",
    "href": "1a-BayesRule.html#sleep-example-2",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\n💡 We added some information (data) about the light!\n\n\nTo update our sleep probability, we need to know the likelihood of observing this new information, given what we know about sleep habits.\n🔍 Key Probabilities Based on My Habits:\n\nIf I am asleep, the probability that my bedroom light is on is very low (1%):\n\\[P(\\text{light|sleep}) = 0.01\\]\nIf I am awake, the probability that my bedroom light is on is quite high (80%):\n\\[P(\\text{light|−sleep}) = 0.8\\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-3",
    "href": "1a-BayesRule.html#sleep-example-3",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\n💡 We had our prior information about sleep and added some information (data) about the light!\n\nThe final piece of the equation is \\(P(\\text{light})\\), representing the total probability my light is on.\nThere are two possible conditions where my light could be on:\n\nI am asleep\n\nI am awake\n\n\nTo calculate the total probability, we use:\n\\[\nP(\\text{light}) = P(\\text{light|sleep})P(\\text{sleep}) + P(\\text{light|−sleep})P(\\text{−sleep})\n\\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-4",
    "href": "1a-BayesRule.html#sleep-example-4",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\nNow, let’s put it all together with Bayes’ Rule:\n\\(\\underset{P(A|B)}{P(\\text{sleep}|\\text{light})} =\\)\n\n\n\\[\n\\frac{\\underset{P(B|A)}{P(\\text{light}|\\text{sleep})} \\cdot \\underset{P(A)}{P(\\text{sleep})}}{\\underset{P(B)}{P(\\text{light})}}\n\\]\n\n\\[\n\\frac{\\underset{0.01}{P(\\text{light}|\\text{sleep})} \\cdot \\underset{0.27}{P(\\text{sleep})}}{\\underset{0.59}{P(\\text{light})}}\n\\]\n\n🎯 This allows us to update our belief about the probability that I’m asleep based on the new evidence (the light being on)!"
  },
  {
    "objectID": "1a-BayesRule.html#likelihood-prior-posterior",
    "href": "1a-BayesRule.html#likelihood-prior-posterior",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood, Prior & Posterior",
    "text": "Likelihood, Prior & Posterior"
  },
  {
    "objectID": "1a-BayesRule.html#bayes-rule-applied-to-parameters-and-data",
    "href": "1a-BayesRule.html#bayes-rule-applied-to-parameters-and-data",
    "title": "Bayesian Data Analysis",
    "section": "Bayes’ rule applied to parameters and data",
    "text": "Bayes’ rule applied to parameters and data\nGiven a set of observed data points \\(Y\\) and a set of parameters \\(\\theta\\), we write Bayes’ rule as \\[\\underset{\\text{posterior}}{P(\\theta|Y)} = \\frac{\\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}}{\\underset{\\text{marginal likelihood}}{P(Y)}}\\]\nWhere the denominator is\n\n\\(P(Y) = \\sum_{\\theta^*}P(Y|\\theta^*)P(\\theta^*)\\) for discrete-valued variables, or\n\\(P(Y) = \\int P(Y|\\theta^*)P(\\theta^*) d\\theta^*\\) for continuous variables.\n\n\\(P(Y)\\) is often difficult to calculate (more on this later) and Baye’s rule is often written more simply as a proportional statement \\[\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\]"
  },
  {
    "objectID": "1a-BayesRule.html#likelihood-prior-posterior-1",
    "href": "1a-BayesRule.html#likelihood-prior-posterior-1",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood, Prior & Posterior",
    "text": "Likelihood, Prior & Posterior\n\\[\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\]\n\n\n\n\\(P(Y|\\theta)\\) - the likelihood. The likelihood represents the evidence that we have based on data.\n\\(P(\\theta)\\) - the prior. The prior represents what we know about the parameters before the data are observed.\n\\(P(\\theta|Y)\\) - the posterior. The posterior represents our updated knowledge about parameters after the data are observed."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#recall-bayes-rule",
    "href": "1b-InferringBinomialProp.html#recall-bayes-rule",
    "title": "Bayesian Data Analysis",
    "section": "Recall: Bayes’ rule",
    "text": "Recall: Bayes’ rule\nGiven a set of observed data points \\(y\\) and a set of parameters \\(\\theta\\), we write Bayes’ rule as\n\\[\\underset{\\text{posterior}}{P(\\theta|y)} = \\frac{\\underset{\\text{likelihood}}{P(y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}}{\\underset{\\text{marginal likelihood}}{P(y)}}\\] and as a proportional statement\n\\[\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\]\nWe will now consider an example that will build some intuition for how prior distributions and data interact to produce posterior distributions."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#what-proportion-of-earths-surface-is-covered-with-water",
    "href": "1b-InferringBinomialProp.html#what-proportion-of-earths-surface-is-covered-with-water",
    "title": "Bayesian Data Analysis",
    "section": "What proportion of Earth’s surface is covered with water?",
    "text": "What proportion of Earth’s surface is covered with water?"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth",
    "href": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth",
    "title": "Bayesian Data Analysis",
    "section": "Estimating the Proportion of Water on Earth",
    "text": "Estimating the Proportion of Water on Earth\nImagine you want to estimate how much of the Earth’s surface is covered in water. 🌍💧\n\n🟢 The Experiment: You throw a blow-up globe into the air, and wherever your index finger lands, you record an observation of either:\n\nWater 🌊\nLand 🌍\n\n🔄 Repeat this process multiple times, collecting a dataset of binary outcomes (water or land).\n📊 Your Data: As you accumulate observations, you’ll begin to estimate the proportion of the Earth that’s covered by water based on how often your finger lands on water.\n\nThis simple, hands-on experiment can give you an idea of the Earth’s water coverage!"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth-1",
    "href": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth-1",
    "title": "Bayesian Data Analysis",
    "section": "Estimating the Proportion of Water on Earth",
    "text": "Estimating the Proportion of Water on Earth"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#water-on-the-globe-example",
    "href": "1b-InferringBinomialProp.html#water-on-the-globe-example",
    "title": "Bayesian Data Analysis",
    "section": "Water on the Globe Example",
    "text": "Water on the Globe Example\n🎯 What is our goal? Estimate the proportion of water on the globe, denoted as θ (theta).\n📊 What data do we have?\n\nData Collected: L, W, L, L, W, W, W, L, W, W\nTotal Throws: n = 10\nWater Observations: y = 6\n\n🔍 How do we perform Bayesian inference for θ?\n\nModel the Data: Choose a descriptive model for the data, known as the likelihood, which includes θ (the proportion of water).\nPrior Information: Summarize existing knowledge about θ using a prior probability distribution.\nUpdate with Data: Combine the prior with the collected data using Bayes’ rule to obtain the posterior distribution for θ. This refines our estimate of the proportion of water on the globe based on the evidence we’ve gathered!"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#brief-recap-types-of-statistical-distributions",
    "href": "1b-InferringBinomialProp.html#brief-recap-types-of-statistical-distributions",
    "title": "Bayesian Data Analysis",
    "section": "Brief Recap: Types of Statistical Distributions",
    "text": "Brief Recap: Types of Statistical Distributions\nDifferent types of distributions are used to describe data (or parameters), here’s some examples:\n\n\nNormal Distribution\n\nDescription: Bell-shaped curve, symmetric around the mean.\nExample: Heights of people, IQ scores.\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\nDescription: Discrete distribution of the # of successes in a fixed # of trials.\nExample: Presence/absence of a disease\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Distribution\n\nDescription: Discrete distribution for the # of events in a fixed interval.\nExample: # of emails received in an hour."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#likelihood-function---pytheta",
    "href": "1b-InferringBinomialProp.html#likelihood-function---pytheta",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood Function - \\(p(y|\\theta)\\)",
    "text": "Likelihood Function - \\(p(y|\\theta)\\)\nWe have data: n = 10, y = 6. We now need a data model.\nWe’ll model this particular type of data using a Binomial Distribution.\n\nAssumption: \\(y\\) follows a Binomial distribution with parameters \\((\\theta, n)\\), where:\n\\[p(y|\\theta) = c\\theta^y(1-\\theta)^{n-y} \\text{ with } c = {n \\choose y}\\]\nExplanation:\n\n\\(p(y|\\theta)\\): The likelihood function shows the probability of observing y water outcomes out of n throws, given the proportion of water θ.\n\\(c\\): The combinatorial factor that accounts for the number of ways to choose y successes out of n trials.\n\n\nWhy This Matters: This function tells us how our observed data (water outcomes) relate to the parameter we want to estimate (θ)."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#prior-distribution---ptheta",
    "href": "1b-InferringBinomialProp.html#prior-distribution---ptheta",
    "title": "Bayesian Data Analysis",
    "section": "Prior distribution - \\(p(\\theta)\\)",
    "text": "Prior distribution - \\(p(\\theta)\\)\nNow that we’ve defined the data model, the next step is to establish a prior distribution over the parameter values.\n\nLet’s start simple and assume \\(\\theta\\) can only take on values k = \\(0,0.25,0.5,0.75,1\\).\nSuppose that we believe that \\(\\theta\\) is most likely to be 0.5 and we assign lower weight to \\(\\theta\\) values far above or below 0.5.\nA prior distribution incorporating these beliefs might look like:"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#likelihood-prior",
    "href": "1b-InferringBinomialProp.html#likelihood-prior",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood & Prior",
    "text": "Likelihood & Prior\nGiven that y = 6 and n = 10 with \\(\\frac{y}{n} = 0.6\\), which \\(\\theta\\) out of \\(0,0.25,0.5,0.75,1\\) do you expect to have the largest value of the likelihood function?"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#posterior-distribution---undersettextposteriorpthetay-propto-undersettextlikelihoodpythetaundersettextpriorptheta",
    "href": "1b-InferringBinomialProp.html#posterior-distribution---undersettextposteriorpthetay-propto-undersettextlikelihoodpythetaundersettextpriorptheta",
    "title": "Bayesian Data Analysis",
    "section": "Posterior distribution - \\(\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\)",
    "text": "Posterior distribution - \\(\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\)"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#changing-prior-assumptions-1",
    "href": "1b-InferringBinomialProp.html#changing-prior-assumptions-1",
    "title": "Bayesian Data Analysis",
    "section": "Changing prior assumptions (1)",
    "text": "Changing prior assumptions (1)\nInstead of the “triangular” prior let’s make a different assumption where we assume 0.75 is most likely and 0.5 is somewhat likely."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#changing-prior-assumptions-2",
    "href": "1b-InferringBinomialProp.html#changing-prior-assumptions-2",
    "title": "Bayesian Data Analysis",
    "section": "Changing prior assumptions (2)",
    "text": "Changing prior assumptions (2)\nInstead of the “triangular” prior let’s make a more uniform assumption. So for \\(k = 0,0.25,0.5,0.75,1\\), \\(Pr(\\theta = k) = 1/5\\) (i.e., all are equally likely)."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#marginal-likelihood---py",
    "href": "1b-InferringBinomialProp.html#marginal-likelihood---py",
    "title": "Bayesian Data Analysis",
    "section": "Marginal likelihood - \\(p(y)\\)",
    "text": "Marginal likelihood - \\(p(y)\\)\nRecall: \\[\\underset{\\text{posterior}}{P(\\theta|y)} = \\frac{\\underset{\\text{likelihood}}{P(y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}}{\\underset{\\text{marginal likelihood}}{P(y)}}\\]\nWhat is \\(P(y)\\)?\n\\[P(y) = \\sum_{\\theta^*} P(y|\\theta^*)P(\\theta^*)\\]\nSo for \\(k = 0,0.25,0.5,0.75,1\\), \\(Pr(\\theta = k) = 1/5\\) (i.e., all are equally likely)\n\\(P(y) = p(y|\\theta = 0)Pr(\\theta = 0) + P(y|\\theta = 0.25)Pr(\\theta = 0.25) + \\ldots = 0.073\\)\nTo do this in R:\n\nn_grid = 5\ntheta &lt;- seq(0,1,length = n_grid) \np_y &lt;- (1/n_grid)*(sum(dbinom(6, 10, prob = theta)))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "",
    "text": "Welcome to the course website for ST405 (code share ST645) Bayesian Data Analysis!\nModule information\nLecture material (slides, notes, videos) are licensed under CC-BY-NC 4.0.\nContact: Niamh Cahill (niamh.cahill@mu.ie)"
  },
  {
    "objectID": "index.html#lecture-slides",
    "href": "index.html#lecture-slides",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Lecture Slides",
    "text": "Lecture Slides\n\nWeek 1\n1a: Bayes Rule\n1b: Inferring a Binomial Probability\n\n\nWeek 2\n2a: Beta Binomial\n2b: Bayesian Inference\n\n\nWeek 3\n3: Single Parameter Normal\n\n\nWeek 4\n4: MCMC Sampling"
  },
  {
    "objectID": "2a-BetaBinomial.html#scenario",
    "href": "2a-BetaBinomial.html#scenario",
    "title": "Bayesian Data Analysis",
    "section": "Scenario:",
    "text": "Scenario:\nSuppose females, aged 65+ in a general social survey were asked about being happy.\nIf this is a representative sample of the population of women, what is the probability that a 65+ woman is happy?"
  },
  {
    "objectID": "2a-BetaBinomial.html#what-is-our-goal-what-data-do-we-have",
    "href": "2a-BetaBinomial.html#what-is-our-goal-what-data-do-we-have",
    "title": "Bayesian Data Analysis",
    "section": "What is Our Goal? What Data do We Have?",
    "text": "What is Our Goal? What Data do We Have?\n🎯 Goal:\nTo estimate the probability that a 65+ woman is happy.\n\nThis is an unknown parameter we’ll call \\(\\theta\\).\n\\(\\theta\\) represents the probability of a 65+ woman being happy.\n\n📊 Data:\n\nSample Size (n): 20 women\nReported Happy (y): 14 women reported being happy"
  },
  {
    "objectID": "2a-BetaBinomial.html#bayesian-inference-for-theta",
    "href": "2a-BetaBinomial.html#bayesian-inference-for-theta",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Inference for \\(\\theta\\)",
    "text": "Bayesian Inference for \\(\\theta\\)\n🔍 Steps in Bayesian Inference:\n\nLikelihood (Model the Data):\n\nWe define a model for the data, describing how the probability \\(\\theta\\) (probability a 65+ woman is happy) fits.\n\nPrior Information:\n\nBefore looking at data, we summarize information about \\(\\theta\\) in a prior distribution.\n\nUpdating with Data:\n\nBayes’ Rule helps us update our prior with the data to obtain the posterior distribution, reflecting what we know about \\(\\theta\\) after considering the data."
  },
  {
    "objectID": "2a-BetaBinomial.html#the-bayesian-formula",
    "href": "2a-BetaBinomial.html#the-bayesian-formula",
    "title": "Bayesian Data Analysis",
    "section": "The Bayesian Formula",
    "text": "The Bayesian Formula\n\\[\n\\text{Posterior}(\\theta | \\text{data}) \\propto \\text{Likelihood}(\\text{data} | \\theta) \\cdot \\text{Prior}(\\theta)\n\\]\nLater we can visualize the prior, likelihood, and posterior distributions to see how Bayesian updating works in practice, for this example.\nFirst let’s workout what the posterior is, based on assuming an appropriate likelihood and prior.\n❓ Questions:\n\nWhat likelihood would you choose here?\nWhat are some constraints we need to think about when choosing a prior for \\(\\theta\\)?"
  },
  {
    "objectID": "2a-BetaBinomial.html#the-happiness-example",
    "href": "2a-BetaBinomial.html#the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "The Happiness example",
    "text": "The Happiness example\nFor the Happiness example:\n\nData: n = 20 women, y = 14 women reported being happy\n\\(y \\sim Binomial(n = 20, \\theta)\\)\n\n\\[p(y|\\theta) = c\\theta^y(1-\\theta)^{n-y} \\text{ with } c = {n \\choose y}\\]\n\nWe want to find the posterior distribution for \\(\\theta\\)\n\nNow we will consider defining the prior, \\(p(\\theta)\\), with a known probability distribution, such that:\n\\[\\theta \\sim Beta(a,b)\\]"
  },
  {
    "objectID": "2a-BetaBinomial.html#the-beta-prior",
    "href": "2a-BetaBinomial.html#the-beta-prior",
    "title": "Bayesian Data Analysis",
    "section": "The Beta Prior",
    "text": "The Beta Prior\nA Beta distribution is defined on the interval [0,1] and has two parameters, \\(a\\) and \\(b\\). The density function is defined as:\n\\[p(\\theta|a,b) = \\frac{1}{B(a,b)}\\theta^{a-1}(1-\\theta)^{b-1}\\]\nwhere \\(B(a,b)\\) is a normalising constant that insures a valid probability density function.\nIf \\(\\theta \\sim Be(a,b)\\) then \\(E(\\theta) = \\frac{a}{a+b}\\) and \\(Var(\\theta) = \\frac{ab}{(a+b)^2(a+b+1)}\\)\nNote \\(B(a,b)\\) is not a function of \\(\\theta\\), so we can write\n\\[p(\\theta|a,b) \\propto \\theta^{a-1}(1-\\theta)^{b-1}\\]\nThis will become useful later."
  },
  {
    "objectID": "2a-BetaBinomial.html#the-beta-prior-with-a1-and-b1",
    "href": "2a-BetaBinomial.html#the-beta-prior-with-a1-and-b1",
    "title": "Bayesian Data Analysis",
    "section": "The Beta Prior with a=1 and b=1",
    "text": "The Beta Prior with a=1 and b=1\nLet’s use Bayes’ theorem now to find the form of the posterior distribution for \\(\\theta\\) assuming \\(\\theta \\sim Beta(a=1,b=1)\\).\nThis means we’ve assumed a prior mean and variance for \\(\\theta\\) of \\(\\frac{1}{2}\\) and \\(\\frac{1}{12}\\) respectively.\nSo the posterior is\n\\[\\underset{\\text{posterior}}{p(\\theta|y)} \\propto \\underset{\\text{likelihood}}{\\theta^y(1-\\theta)^{n-y} }\\underset{\\text{prior}}{\\theta^{a-1}(1-\\theta)^{b-1}}\\]\nand given a = 1 and b = 1\n\\[\\underset{\\text{posterior}}{p(\\theta|y)} \\propto {\\theta^y(1-\\theta)^{n-y}}\\]\nThis posterior actually takes the form of another Beta distribution with parameters \\(y+1\\) and \\(n-y+1\\). So, \\[\\theta|y \\sim Beta(y+1, n-y +1)\\]"
  },
  {
    "objectID": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example",
    "href": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "What does this look like for the Happiness example?",
    "text": "What does this look like for the Happiness example?\n\nVisualise the PriorVisualise the LikelihoodVisualise the Posteior\n\n\n\n\n# Beta parameters (prior)\na &lt;- 1\nb &lt;- 1\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nprior &lt;- dbeta(theta,a,b) # get the prior distribution\n\n# create a dataset\nptheta_dat &lt;- tibble::tibble(theta, prior)\n\n# plot prior\nggplot2::ggplot(ptheta_dat, aes(x = theta, y = prior)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Data\ny &lt;- 14\nn &lt;- 20\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nlikelihood &lt;- dbinom(y,n,prob = theta) # get the likelihood distribution\n\n# create a dataset\nltheta_dat &lt;- tibble::tibble(theta, likelihood)\n\n# plot likelihood\nggplot2::ggplot(ltheta_dat, aes(x = theta, y = likelihood)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Data\ny &lt;- 14\nn &lt;- 20\n\n# Beta parameters (posterior)\na_post &lt;- y+1\nb_post &lt;- n-y+1\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nposterior &lt;- dbeta(theta,a_post,b_post) # get the posterior distribution\n\n# create a dataset\nposttheta_dat &lt;- tibble::tibble(theta, posterior)\n\n# plot posterior\nggplot2::ggplot(posttheta_dat, aes(x = theta, y = posterior)) +\n  geom_line()"
  },
  {
    "objectID": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example-1",
    "href": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example-1",
    "title": "Bayesian Data Analysis",
    "section": "What does this look like for the Happiness example?",
    "text": "What does this look like for the Happiness example?"
  },
  {
    "objectID": "2a-BetaBinomial.html#more-on-the-binomial-likelihood-and-the-beta-prior",
    "href": "2a-BetaBinomial.html#more-on-the-binomial-likelihood-and-the-beta-prior",
    "title": "Bayesian Data Analysis",
    "section": "More on the Binomial Likelihood and the Beta Prior",
    "text": "More on the Binomial Likelihood and the Beta Prior\nIt turns out anytime you use a Binomial likelihood and a Beta prior, such that:\n\\[\\theta \\sim Be(a,b)\\]\n\\[y \\sim Binomal(n,\\theta)\\]\nthen you get a posterior distribution which is also Beta, where\n\\[\\theta|y \\sim Beta(y+a, n-y +b)\\]\nWhen the posterior is the same form as the prior, the prior is said to be a conjugate prior. The Beta prior is a conjugate prior for the Binomial likelihood."
  },
  {
    "objectID": "2a-BetaBinomial.html#expressing-prior-knowledge",
    "href": "2a-BetaBinomial.html#expressing-prior-knowledge",
    "title": "Bayesian Data Analysis",
    "section": "Expressing prior knowledge",
    "text": "Expressing prior knowledge\nSuppose for the Happiness example, you want to express your underlying belief about \\(\\theta\\) - the probability a woman age 65+ is happy.\n\nYour beliefs may be based on previous studies or perhaps expert opinion.\nSo for example, suppose you want your prior to reflect beliefs that the proportion is 0.6 \\(\\pm\\) 0.1.\nHow do we express this belief in the Beta distribution?\n\n\n\n🛠️ Moment Matching: Our Tool to Match Beliefs to a Distribution\n\nWe use a technique called moment matching to convert these beliefs\n(mean = 0.6, sd = 0.1) into the parameters of a Beta distribution."
  },
  {
    "objectID": "2a-BetaBinomial.html#moment-matching",
    "href": "2a-BetaBinomial.html#moment-matching",
    "title": "Bayesian Data Analysis",
    "section": "Moment Matching",
    "text": "Moment Matching\nRecall if \\(\\theta \\sim Be(a,b)\\) then \\(E(\\theta) = \\frac{a}{a+b}\\) and \\(Var(\\theta) = \\frac{ab}{(a+b)^2(a+b+1)}\\)\nBased on our prior beliefs we want:\n\\(E(\\theta) = \\frac{a}{a+b} = 0.6\\)\n\\(Var(\\theta) = \\frac{ab}{(a+b)^2(a+b+1)} = 0.1^2\\)\nWe can use these equations to solve for \\(a\\) and \\(b\\), the parameters of the Beta prior.\n\nVisualise the Prior\n\n\n\n\n# Beta parameters (prior)\na &lt;- 23\nb &lt;- 16\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nprior &lt;- dbeta(theta,a,b) # get the prior distribution\n\n# create a dataset\nptheta_dat &lt;- tibble::tibble(theta, prior)\n\n# plot prior\nggplot2::ggplot(ptheta_dat, aes(x = theta, y = prior)) +\n  geom_line()"
  },
  {
    "objectID": "2a-BetaBinomial.html#how-does-this-prior-impact-the-posterior",
    "href": "2a-BetaBinomial.html#how-does-this-prior-impact-the-posterior",
    "title": "Bayesian Data Analysis",
    "section": "How does this prior impact the posterior?",
    "text": "How does this prior impact the posterior?"
  },
  {
    "objectID": "2a-BetaBinomial.html#the-posterior-is-a-compromise-of-prior-and-likelihood",
    "href": "2a-BetaBinomial.html#the-posterior-is-a-compromise-of-prior-and-likelihood",
    "title": "Bayesian Data Analysis",
    "section": "The posterior is a compromise of prior and likelihood",
    "text": "The posterior is a compromise of prior and likelihood\nThe posterior distribution is always a compromise between the prior distribution and the likelihood function.\n\nWe can illustrate this easily with the Beta-Binomial example.\nWe’ve seen that for a \\(Be(a,b)\\) prior and a \\(Binomial(n,\\theta)\\) likelihood that the posterior will be of the form:\n\n\\[\\theta|y \\sim Beta(y+a, n-y +b)\\]\nand so the posterior mean is \\(E(\\theta|y) = \\frac{y+a}{n + a + b}\\).\n\nThis can be written as a weighted sum of the prior mean (\\(\\frac{a}{a+b}\\)) and the data proportion (\\(\\frac{y}{n}\\)), as follows: \\(E(\\theta|y) = \\underbrace{\\small\\frac{y}{n}}_{data}\\underbrace{\\small\\frac{n}{n+a+b}}_{weight} + \\underbrace{\\small\\frac{a}{a+b}}_{prior}\\underbrace{\\small\\frac{a+b}{n+a+b}}_{weight}\\)"
  },
  {
    "objectID": "2b-BayesianInference.html#bayesian-inference",
    "href": "2b-BayesianInference.html#bayesian-inference",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\n\nBayesian point estimates are often given by:\n\nthe posterior mean \\(E(\\theta|y)\\)\nor the posterior median \\(\\theta^*\\) with \\(P(\\theta &lt; \\theta^*|y) = 0.5\\)\n\nUncertainty is quantified with credible intervals (CIs)\n\nAn interval is a 95% credible interval if the posterior probability that \\(\\theta\\) is in the interval is 0.95.\nOften quantile based, given by posterior quantiles with \\(P(\\theta &lt; \\theta_{\\alpha/2}|y) = P(\\theta &gt; \\theta_{1-\\alpha/2}|y) = \\alpha/2\\)"
  },
  {
    "objectID": "2b-BayesianInference.html#bayesian-inference-for-the-happiness-example",
    "href": "2b-BayesianInference.html#bayesian-inference-for-the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Inference for the Happiness example",
    "text": "Bayesian Inference for the Happiness example\n\n\\(\\theta|y \\sim Beta(y+a,n-y+b)\\)\nPosterior mean \\(E(\\theta|y) = \\frac{y+a}{n+a+b}\\)\nFor quantile estimates we can use qbeta() in R\n\n\nR Code\n\n\n\n\n## data \nn = 20; y = 14\n\n## prior parameters\na = 1; b = 1\n\n## posterior parameters\na_post = y + a; b_post = n - y + b\n\n## posterior mean\np_mean &lt;- a_post/(a_post + b_post) # (y+a)/(n+a+b)\n\n## quantiles\np_quant &lt;- qbeta(c(0.025,0.5,0.975),a_post,b_post)\n\np_mean; p_quant\n\n\n[1] 0.6818182\n\n\n[1] 0.4782489 0.6874159 0.8541231"
  },
  {
    "objectID": "2b-BayesianInference.html#simulation-based-inference",
    "href": "2b-BayesianInference.html#simulation-based-inference",
    "title": "Bayesian Data Analysis",
    "section": "Simulation-based inference",
    "text": "Simulation-based inference\n\nThe general idea in simulation-based inference: We can make inference about a parameter \\(\\theta\\) , using a sample \\(\\{\\theta^{(1)}\\ldots\\theta^{(S)}\\}\\) from its probability distribution.\nAssessing the properties of a target (e.g., posterior) distribution by generating representative samples is called Monte Carlo simulation.\nBased on the law of large numbers we know that:\n\\(\\frac{1}{S}\\sum_{s=1}^{S}\\theta^{(s)} = E(\\theta)\\)\nas sample size \\(S \\to \\infty\\)\n\nThe error in the MC approximation goes to zero as \\(S \\to \\infty\\) because \\(\\frac{var(\\theta)}{S} \\to 0\\)\n\nJust about any aspect of the distribution of \\(\\theta\\) can be approximated arbitrarily exactly with a large enough Monte Carlo sample, e.g.\n\nthe \\(\\alpha\\)-percentile of the distribution of \\(\\theta\\)\n\\(Pr(\\theta \\geq x)\\) for any constant \\(x\\)"
  },
  {
    "objectID": "2b-BayesianInference.html#simulation-based-inference-for-the-happiness-example",
    "href": "2b-BayesianInference.html#simulation-based-inference-for-the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "Simulation-based inference for the Happiness example",
    "text": "Simulation-based inference for the Happiness example\nFor the Happiness example, we can approximate the mean and quantiles of \\(\\theta\\) using samples from a \\(Be(y+a,n-y+b)\\) distribution (i.e., the posterior)\n\nR Code\n\n\n\n\n## data \nn = 20; y = 14\n\n## prior parameters\na = 1; b = 1\n\n## posterior parameters\na_post = y + a; b_post = n - y + b\n\n## sample \nsamp_theta &lt;- rbeta(1000,a_post,b_post)\n\n## sample mean and quantiles\np_mean &lt;- mean(samp_theta)\np_quant &lt;- quantile(samp_theta, \n                    probs = c(0.025,0.5,0.975))\n\np_mean; p_quant\n\n\n[1] 0.6752533\n\n\n     2.5%       50%     97.5% \n0.4739717 0.6825371 0.8589662"
  },
  {
    "objectID": "2b-BayesianInference.html#monte-carlo-approximation-some-more-details",
    "href": "2b-BayesianInference.html#monte-carlo-approximation-some-more-details",
    "title": "Bayesian Data Analysis",
    "section": "Monte Carlo approximation: some more details",
    "text": "Monte Carlo approximation: some more details\n\nWith a simulation, it also becomes very easy to analyze the distributions of any function of your parameter,\n\ne.g. the distribution of the odds \\(\\frac{\\theta}{1-\\theta}\\) by using samples from \\(\\frac{\\theta^{(s)}}{1-\\theta^{(s)}}\\)\n\n\n\nR Code\n\n\n\n\n## sample \nsamp_theta &lt;- rbeta(1000,a_post,b_post)\n\n## get odds based on samples\nsamp_odds &lt;- samp_theta/(1-samp_theta)\n\n## sample mean and quantiles\np_mean_odds &lt;- mean(samp_odds)\np_quant_odds &lt;- quantile(samp_odds, \n                         probs = c(0.025,0.5,0.975))\n\np_mean_odds; p_quant_odds\n\n\n[1] 2.525918\n\n\n     2.5%       50%     97.5% \n0.9277052 2.2027345 5.6566618"
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution",
    "href": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\mu\\) from a Normal Distribution",
    "text": "Estimating \\(\\mu\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\mu\\) - example in blue with \\(\\mu \\sim N(4,0.5^2)\\)\nCollect data, \\(y\\), assume y has a mean \\(\\bar{y} = 0\\) and that n = 10.\nDefine the relationship between \\(y\\) and \\(\\mu\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu, \\sigma = 1)\\).\n\nwe’re assuming \\(\\sigma\\) is known here.\n\nUse Bayes’ rule to update the prior into the posterior distribution \\(p(\\mu|y) \\sim N(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-1",
    "href": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-1",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\mu\\) from a Normal Distribution",
    "text": "Estimating \\(\\mu\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\mu\\) - example in blue with \\(\\mu \\sim N(4,0.5^2)\\)\nCollect data, \\(y\\), assume y has a mean \\(\\bar{y} = 0\\) and that n = 10.\n\nDefine the relationship between \\(y\\) and \\(\\mu\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu, \\sigma = 2)\\).\n\nwe’re assuming \\(\\sigma\\) is known here.\n\nUse Bayes’ rule to update the prior into the posterior distribution \\(p(\\mu|y) \\sim N(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-2",
    "href": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-2",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\mu\\) from a Normal Distribution",
    "text": "Estimating \\(\\mu\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\mu\\) - example in blue with \\(\\mu \\sim N(4,0.5^2)\\)\nCollect data, \\(y\\), assume y has a mean \\(\\bar{y} = 0\\) and that n = 500.\nDefine the relationship between \\(y\\) and \\(\\mu\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu, \\sigma = 1)\\).\n\nwe’re assuming \\(\\sigma\\) is known here.\n\nUse Bayes’ rule to update the prior into the posterior distribution \\(p(\\mu|y) \\sim N(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-3",
    "href": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-3",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\mu\\) from a Normal Distribution",
    "text": "Estimating \\(\\mu\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\mu\\) - example in blue with \\(\\mu \\sim N(4,0.5^2)\\)\nCollect data, \\(y\\), assume y has a mean \\(\\bar{y} = 0\\) and that n = 500.\nDefine the relationship between \\(y\\) and \\(\\mu\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu, \\sigma = 2)\\).\n\nwe’re assuming \\(\\sigma\\) is known here.\n\nUse Bayes’ rule to update the prior into the posterior distribution \\(p(\\mu|y) \\sim N(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-sigma-text-tau-1sigma2-from-a-normal-distribution",
    "href": "3a-SingleParameterNormal.html#estimating-sigma-text-tau-1sigma2-from-a-normal-distribution",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\sigma \\text{ } (\\tau = 1/\\sigma^2)\\) from a Normal Distribution",
    "text": "Estimating \\(\\sigma \\text{ } (\\tau = 1/\\sigma^2)\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\tau = 1/\\sigma^2\\) - example in blue with \\(\\tau = 1/\\sigma^2 \\sim gamma(2,1)\\)\nCollect data, \\(y\\), assume y has a standard deviation \\(s = 2, 1/s^2 = 0.5\\) and that n = 10.\n\nDefine the relationship between \\(y\\) and \\(\\tau\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu = 0, \\sigma^2 = 1/\\tau)\\).\n\nwe’re assuming \\(\\mu\\) is known here.\n\nUse Bayes’ rule to update the prior into the posterior distribution \\(p(\\tau|y) \\sim gamma(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-sigma-text-tau-1sigma2-from-a-normal-distribution-1",
    "href": "3a-SingleParameterNormal.html#estimating-sigma-text-tau-1sigma2-from-a-normal-distribution-1",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\sigma \\text{ } (\\tau = 1/\\sigma^2)\\) from a Normal Distribution",
    "text": "Estimating \\(\\sigma \\text{ } (\\tau = 1/\\sigma^2)\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\tau = 1/\\sigma^2\\) - example in blue with \\(\\tau = 1/\\sigma^2 \\sim gamma(8,4)\\)\nCollect data, \\(y\\), assume y has a standard deviation \\(s = 2, 1/s^2 = 0.5\\) and that n = 10.\n\nDefine the relationship between \\(y\\) and \\(\\tau\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu = 0, \\sigma^2 = 1/\\tau)\\).\n\nwe’re assuming \\(\\mu\\) is known here.\n\nUse Bayes’ rule to update the prior into the posterior distribution \\(p(\\tau|y) \\sim gamma(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#example-cognitive-test-scores",
    "href": "3a-SingleParameterNormal.html#example-cognitive-test-scores",
    "title": "Bayesian Data Analysis",
    "section": "Example: Cognitive Test Scores",
    "text": "Example: Cognitive Test Scores\nData (y) are available on the cognitive test scores of three- and four-year-old children in the USA. The sample contains \\(n=434\\) observations."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#normal-distribution-with-known-variance",
    "href": "3a-SingleParameterNormal.html#normal-distribution-with-known-variance",
    "title": "Bayesian Data Analysis",
    "section": "Normal distribution with known variance",
    "text": "Normal distribution with known variance\nWe will assume a normal model for the data where \\(y_i|\\mu,\\sigma^2 \\sim N(\\mu, \\sigma^2)\\). Assume \\(\\sigma^2\\) is known.\n\nSpecify the likelihood for \\(\\mu\\)\n\n\\(p(y|\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt {2\\pi\\sigma^2}}exp \\bigg(-\\frac{1}{2\\sigma^2}(y_i - \\mu)^2\\bigg)\\)\n\nSpecify a prior for \\(\\mu\\)\n\n\\(\\mu \\sim N(\\mu_0, \\sigma^2_{0})\\)\n\nUse Bayes’ rule to obtain the posterior distribution\n\n\\(p(\\mu|y) \\propto p(y|\\mu)p(\\mu)\\)"
  },
  {
    "objectID": "3a-SingleParameterNormal.html#normal-distribution-with-known-variance-1",
    "href": "3a-SingleParameterNormal.html#normal-distribution-with-known-variance-1",
    "title": "Bayesian Data Analysis",
    "section": "Normal distribution with known variance",
    "text": "Normal distribution with known variance\n\nAs it turns out, the posterior is also a normal distribution\n\n\\[\\mu|y \\sim N \\bigg(\\frac{n\\bar{y}/\\sigma^2 + \\mu_0/\\sigma^2_{0}}{n/\\sigma^2 + 1/\\sigma^2_{0}}, {\\frac{1}{n/\\sigma^2 + 1/\\sigma^2_{0}}}\\bigg)\\]\n\nFor the Kid IQ example, assuming \\(\\mu \\sim N(\\mu_0 = 80, \\sigma_0 = 10)\\), then \\(\\mu|y \\sim N(86.7, 0.97)\\)"
  },
  {
    "objectID": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean",
    "href": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean",
    "title": "Bayesian Data Analysis",
    "section": "Normal distribution with known mean",
    "text": "Normal distribution with known mean\nWe will assume a normal model for the data where \\(y_i|\\mu,\\sigma^2 \\sim N(\\mu, \\sigma^2)\\). Assume \\(\\mu\\) is known.\n\nUsually work with precision i.e., \\(\\tau = 1/\\sigma^2\\)\nSpecify a prior for \\(\\tau\\)\n\nPopular prior for the precision of a normal distribution is a gamma prior e.g., \\(\\tau \\sim Gamma(a, b)\\) where \\(E[\\tau] = \\frac{a}{b}\\) and \\(Var[\\tau] = \\frac{a}{b^2}\\)\n\\(p(\\tau|a,b) = \\frac{b^a}{\\Gamma(a)}\\tau^{a-1}e^{-b\\tau}\\) for \\(\\tau &gt;0\\) and \\(a,b &gt; 0\\)\n\nUse Bayes’ rule to obtain the posterior distribution\n\n\\(p(\\tau|y) \\propto p(y|\\tau)p(\\tau)\\)"
  },
  {
    "objectID": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean-1",
    "href": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean-1",
    "title": "Bayesian Data Analysis",
    "section": "Normal distribution with known mean",
    "text": "Normal distribution with known mean\n\nAs it turns out, the posterior is also a gamma distribution\n\n\\(\\tau|y \\sim Gamma \\bigg(a + n/2, b + 1/2\\sum_{i=1}^n (y_i - \\mu)^2\\bigg)\\)\n\nFor the Kid IQ example, assuming \\(\\tau \\sim gamma(a = 1, b = 1)\\), then \\(\\tau|y \\sim gamma(218, 90203)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean-2",
    "href": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean-2",
    "title": "Bayesian Data Analysis",
    "section": "Normal distribution with known mean",
    "text": "Normal distribution with known mean\n\nConverting back from the precision to the standard deviation, the posterior for \\(\\sigma\\) for this example will look more like:"
  },
  {
    "objectID": "Tutorial1.html",
    "href": "Tutorial1.html",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "This tutorial explores Bayesian inference using binomial and Poisson models, focusing on deriving posterior distributions, credible intervals, and the impact of prior assumptions. You will also perform simulations and compare posterior results with analytical calculations.\n\n\n\nConsider the maternal condition known as placenta previa, an uncommon condition where the placenta obstructs the fetus, leading to possible complications during delivery.\nAn early study in Germany collected data on placenta previa births, finding that out of 980 births, 437 were female.\n\n\n\nPosterior Distribution\nAssuming a Binomial likelihood for the data and a Beta(1,1) prior distribution for the probability of a female birth, derive the posterior distribution.\nPosterior Mean\nCalculate the posterior mean for the probability of a girl birth in placenta previa pregnancies.\n95% Credible Interval\nCompute the 95% credible interval for the probability of a girl birth in placenta previa pregnancies.\n\n\n\n\n\n\nIn many cases, direct calculation of the posterior density function may not be feasible. Simulation offers a practical solution for obtaining inference from the posterior distribution.\n\n\n\nSimulating 1000 Samples\nSimulate 1000 samples from the posterior distribution derived in Question 1.\nHistogram\nCreate a histogram of the simulated samples.\n95% Credible Interval from Simulated Data\nCalculate the 95% credible interval based on the simulated samples.\nComparison with Analytical Interval\nCompare the credible interval obtained from the simulation to the one calculated analytically in Question 1.\n\n\n\n\n\n\nNow, assume we change the prior distribution. Suppose the prior distribution is Beta(6,20) instead of Beta(1,1).\n\n\n\nNew Posterior Distribution\nDerive the new posterior distribution given the updated Beta(6,20) prior.\nPosterior Mean and Credible Interval\nCompute the new posterior mean and 95% credible interval for the probability of a girl birth in placenta previa pregnancies.\n\n\n\n\n\n\nAssume you are modeling data \\(y_{i},\\; i=1,\\ldots,n\\) that arise from a Poisson distribution with rate parameter \\(\\lambda\\). (Note: Poisson distributions are often used for count data.) Use Bayesian methods to estimate \\(\\lambda\\), with a Gamma(\\(a\\), \\(b\\)) distribution as the prior for \\(\\lambda\\).\n\n\n\nLikelihood Function\nWrite down the likelihood for the Poisson model (up to the constant of proportionality).\nAppropriateness of Gamma Prior\nWhy is the Gamma distribution an appropriate prior to use for the rate parameter \\(\\lambda\\) in this case?\nDetermining Parameters of the Gamma Prior\nFind the values of \\(a\\) and \\(b\\) for the Gamma prior that reflect the belief that \\(\\lambda\\) is \\(10 \\pm 1\\) (i.e., has mean = 10 and standard deviation = 1).\nPosterior Distribution\nDerive the posterior distribution for \\(\\lambda\\).\nPosterior Mean Estimate\nIf the observed data are \\(\\mathbf{y} = (17, 25, 25, 21, 13, 22, 23)\\), find the posterior mean estimate for \\(\\lambda\\).\nConjugacy of the Prior\nIs the Gamma prior a conjugate prior for the Poisson likelihood? Explain your answer.\nComparing Posterior, Prior, and Data Means\nComment on how the posterior mean estimate compares to the prior mean and the mean of the observed data. What insights do you gain from this comparison?\n\n\n\n\n\n\nThrough these exercises, you will explore how prior beliefs shape posterior distributions, utilize simulation techniques to derive credible intervals, and understand the impact of different priors on Bayesian inference. The additional question introduces a new distribution, expanding your skills in Bayesian data analysis using Poisson likelihoods and Gamma priors.\n\n\n\n\n\nYou can use the following information about the Binomial, Beta, Poisson and Gamma distributions to help you with the questions\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distribution\nPMF/PDF\nE(X)\nVar(X)\n\n\n\n\nBinomial\n\\[{n \\choose x}\\theta^x(1-\\theta)^{n-x},\\ x \\in \\{0,1,2,\\ldots, n\\}\\]\n\\[n\\theta\\]\n\\[n\\theta(1-\\theta)\\]\n\n\nBeta\n\\[\\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1},\\ x \\in (0,1)\\]\n\\[\\frac{a}{a+b}\\]\n\\[\\frac{ab}{(a+b)^2(a+b+1)}\\]\n\n\nPoisson\n\\[\\lambda^x \\frac{e^{-\\lambda}}{x!},\\ x = 0,1,\\ldots\\]\n\\[\\lambda\\]\n\\[\\lambda\\]\n\n\nGamma\n\\[\\frac{b^a}{\\Gamma(a)}x^{a-1}e^{-bx},\\ x\\geq 0\\]\n\\[\\frac{a}{b}\\]\n\\[\\frac{a}{b^2}\\]\n\n\n\n\n\n\nFor simulations, use rbeta() in R for Beta-distributed samples.\nFor plotting histograms and posterior distributions, use ggplot2 or base R (hist()) plotting functions."
  },
  {
    "objectID": "Tutorial1.html#introduction",
    "href": "Tutorial1.html#introduction",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "This tutorial explores Bayesian inference using binomial and Poisson models, focusing on deriving posterior distributions, credible intervals, and the impact of prior assumptions. You will also perform simulations and compare posterior results with analytical calculations.\n\n\n\nConsider the maternal condition known as placenta previa, an uncommon condition where the placenta obstructs the fetus, leading to possible complications during delivery.\nAn early study in Germany collected data on placenta previa births, finding that out of 980 births, 437 were female.\n\n\n\nPosterior Distribution\nAssuming a Binomial likelihood for the data and a Beta(1,1) prior distribution for the probability of a female birth, derive the posterior distribution.\nPosterior Mean\nCalculate the posterior mean for the probability of a girl birth in placenta previa pregnancies.\n95% Credible Interval\nCompute the 95% credible interval for the probability of a girl birth in placenta previa pregnancies.\n\n\n\n\n\n\nIn many cases, direct calculation of the posterior density function may not be feasible. Simulation offers a practical solution for obtaining inference from the posterior distribution.\n\n\n\nSimulating 1000 Samples\nSimulate 1000 samples from the posterior distribution derived in Question 1.\nHistogram\nCreate a histogram of the simulated samples.\n95% Credible Interval from Simulated Data\nCalculate the 95% credible interval based on the simulated samples.\nComparison with Analytical Interval\nCompare the credible interval obtained from the simulation to the one calculated analytically in Question 1.\n\n\n\n\n\n\nNow, assume we change the prior distribution. Suppose the prior distribution is Beta(6,20) instead of Beta(1,1).\n\n\n\nNew Posterior Distribution\nDerive the new posterior distribution given the updated Beta(6,20) prior.\nPosterior Mean and Credible Interval\nCompute the new posterior mean and 95% credible interval for the probability of a girl birth in placenta previa pregnancies.\n\n\n\n\n\n\nAssume you are modeling data \\(y_{i},\\; i=1,\\ldots,n\\) that arise from a Poisson distribution with rate parameter \\(\\lambda\\). (Note: Poisson distributions are often used for count data.) Use Bayesian methods to estimate \\(\\lambda\\), with a Gamma(\\(a\\), \\(b\\)) distribution as the prior for \\(\\lambda\\).\n\n\n\nLikelihood Function\nWrite down the likelihood for the Poisson model (up to the constant of proportionality).\nAppropriateness of Gamma Prior\nWhy is the Gamma distribution an appropriate prior to use for the rate parameter \\(\\lambda\\) in this case?\nDetermining Parameters of the Gamma Prior\nFind the values of \\(a\\) and \\(b\\) for the Gamma prior that reflect the belief that \\(\\lambda\\) is \\(10 \\pm 1\\) (i.e., has mean = 10 and standard deviation = 1).\nPosterior Distribution\nDerive the posterior distribution for \\(\\lambda\\).\nPosterior Mean Estimate\nIf the observed data are \\(\\mathbf{y} = (17, 25, 25, 21, 13, 22, 23)\\), find the posterior mean estimate for \\(\\lambda\\).\nConjugacy of the Prior\nIs the Gamma prior a conjugate prior for the Poisson likelihood? Explain your answer.\nComparing Posterior, Prior, and Data Means\nComment on how the posterior mean estimate compares to the prior mean and the mean of the observed data. What insights do you gain from this comparison?\n\n\n\n\n\n\nThrough these exercises, you will explore how prior beliefs shape posterior distributions, utilize simulation techniques to derive credible intervals, and understand the impact of different priors on Bayesian inference. The additional question introduces a new distribution, expanding your skills in Bayesian data analysis using Poisson likelihoods and Gamma priors.\n\n\n\n\n\nYou can use the following information about the Binomial, Beta, Poisson and Gamma distributions to help you with the questions\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distribution\nPMF/PDF\nE(X)\nVar(X)\n\n\n\n\nBinomial\n\\[{n \\choose x}\\theta^x(1-\\theta)^{n-x},\\ x \\in \\{0,1,2,\\ldots, n\\}\\]\n\\[n\\theta\\]\n\\[n\\theta(1-\\theta)\\]\n\n\nBeta\n\\[\\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1},\\ x \\in (0,1)\\]\n\\[\\frac{a}{a+b}\\]\n\\[\\frac{ab}{(a+b)^2(a+b+1)}\\]\n\n\nPoisson\n\\[\\lambda^x \\frac{e^{-\\lambda}}{x!},\\ x = 0,1,\\ldots\\]\n\\[\\lambda\\]\n\\[\\lambda\\]\n\n\nGamma\n\\[\\frac{b^a}{\\Gamma(a)}x^{a-1}e^{-bx},\\ x\\geq 0\\]\n\\[\\frac{a}{b}\\]\n\\[\\frac{a}{b^2}\\]\n\n\n\n\n\n\nFor simulations, use rbeta() in R for Beta-distributed samples.\nFor plotting histograms and posterior distributions, use ggplot2 or base R (hist()) plotting functions."
  },
  {
    "objectID": "Tutorial1_soln.html",
    "href": "Tutorial1_soln.html",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "Given:\n- The number of placenta previa births: \\(n = 980\\)\n- The number of female births: \\(x = 437\\)\n- Prior distribution: \\(\\theta \\sim \\text{Beta}(1,1)\\) (which is equivalent to a uniform prior).\n\n\n\nPosterior Distribution\nThe binomial likelihood for the number of female births \\(x\\) out of \\(n\\) total births is:\n\\[p(\\theta | y) = {n \\choose y} \\theta^x (1 - \\theta)^{n - y}\\]\nThe prior distribution is \\(\\theta \\sim \\text{Beta}(1,1)\\). The Beta-Binomial conjugacy allows us to combine the likelihood and the prior to get the posterior distribution:\n\\[\\theta | y \\sim \\text{Beta}(437 + 1, 980 - 437 + 1) = \\text{Beta}(438, 544)\\]\nPosterior Mean\nThe mean of a Beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\) is given by:\n\\[E(\\theta | x) = \\frac{\\alpha}{\\alpha + \\beta}\\]\nSubstituting \\(\\alpha = 438\\) and \\(\\beta = 544\\):\n\\[E(\\theta | x) = \\frac{438}{438 + 544} = \\frac{438}{982} \\approx 0.446\\]\n95% Credible Interval\nThe 95% credible interval for \\(\\theta\\) can be obtained using the quantiles of the Beta distribution. In R, you can use:\nqbeta(c(0.025, 0.975), 438, 544)\nThis gives the interval \\([0.415, 0.478]\\), meaning that with 95% probability, the true value of \\(\\theta\\) lies between 0.415 and 0.478.\n\n\n\n\n\n\n\n\n\nSimulating 1000 Samples\nTo simulate 1000 samples from the posterior \\(\\text{Beta}(438, 544)\\) distribution, you can use the rbeta() function in R:\nsamples &lt;- rbeta(1000, 438, 544)\nHistogram\nYou can create a histogram of the samples to visualize the posterior distribution:\nhist(samples, main = \"Posterior Distribution\", xlab = \"Theta\", col = \"lightblue\", border = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval from Simulated Data\nThe 95% credible interval based on the samples can be calculated as:\nquantile(samples, c(0.025, 0.975))\nThis should give an interval close to \\([0.415, 0.478]\\) obtained from Q1.\nComparison with Analytical Interval\nThe credible interval from the simulation should closely match the analytical interval obtained in Question 1. This shows that the simulation provides a good approximation of the posterior distribution.\n\n\n\n\n\n\nNow, assume the prior distribution is \\(\\theta \\sim \\text{Beta}(6,20)\\).\n\n\n\nNew Posterior Distribution\nThe prior is \\(\\text{Beta}(6,20)\\), and the likelihood is still binomial. Using the conjugacy property of the Beta distribution, the posterior becomes:\n\\[\\theta | x \\sim \\text{Beta}(437 + 6, 980 - 437 + 20) = \\text{Beta}(443, 563)\\]\nPosterior Mean and Credible Interval\nThe posterior mean is:\n\\[E(\\theta | x) = \\frac{443}{443 + 563} = \\frac{443}{1006} \\approx 0.440\\]\nThe 95% credible interval can be obtained similarly using R:\nqbeta(c(0.025, 0.975), 443, 563)\nThis gives the interval \\([0.408, 0.473]\\).\nComparing this to the result from the prior \\(\\text{Beta}(1,1)\\), we observe that using the Beta(6,20) prior shifts the posterior slightly, reflecting the stronger prior information.\n\n\n\n\n\n\nNote: Students have not seen an example of using these distributions in class.\n\n\n\nLikelihood Function\nThe likelihood for a Poisson model is:\n\\[p(\\lambda | \\mathbf{y}) = \\prod_{i=1}^n \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\\] Ignoring constants, the likelihood is proportional to:\n\\[p(\\lambda | \\mathbf{y}) \\propto \\lambda^{\\sum y_i} e^{-n \\lambda}\\]\nAppropriateness of Gamma Prior\nThe Gamma distribution is a conjugate prior for the Poisson likelihood because the posterior distribution remains in the Gamma family. This simplifies computation and provides an intuitive interpretation of the prior parameters.\nFinding \\(a\\) and \\(b\\) for Prior\nThe mean and standard deviation of the Gamma distribution are given by:\n\\[\\mu = \\frac{\\alpha}{\\beta}, \\quad \\sigma^2 = \\frac{\\alpha}{\\beta^2}\\] Given that \\(\\mu = 10\\) and \\(\\sigma = 1\\), solve for \\(\\alpha\\) and \\(\\beta\\):\n\\[\\alpha = \\frac{10^2}{1^2} = 100, \\quad \\beta = \\frac{10}{1^2} = 10\\] Hence, \\(\\lambda \\sim \\text{Gamma}(100,10)\\).\nPosterior Distribution\n\n\\(p(\\lambda|y) \\propto p(y|\\lambda)p(\\lambda)\\)\nThe prior: \\(p(\\lambda) \\propto \\lambda^{a-1}e^{-b\\lambda}\\)\n(note: the \\(\\frac{b^a}{\\Gamma(a)}\\) can be removed under the proportionality)\nThe posterior: \\(p(\\lambda|y) \\propto \\lambda^{a-1}e^{-b\\lambda}\\lambda^{\\sum{y_i}}e^{-n\\lambda} = \\lambda^{\\boxed{a + \\sum{y_i}}-1}e^{-\\boxed{(b+n)}\\lambda}\\)\n(note: this has the same form as a gamma distribution with \\(a_{new} = a + \\sum{y_i}\\) and \\(b_{new} = b+n\\))\nTherefore \\(\\lambda|y \\sim gamma(a + \\sum{y_i},b+n)\\)\nWith the data \\(\\mathbf{y} = (17, 25, 25, 21, 13, 22, 23)\\), we have \\(\\sum y_i = 146\\), \\(n = 7\\). So, the posterior is \\(\\lambda | \\mathbf{y} \\sim \\text{Gamma}(246, 17)\\)\n\nPosterior Mean Estimate\nThe posterior mean is:\n\\[E(\\lambda | \\mathbf{y}) = \\frac{246}{17} \\approx 14.47\\]\nConjugacy of the Prior\nYes, the Gamma distribution is a conjugate prior for the Poisson likelihood, as the posterior distribution remains a Gamma distribution.\nComparison of Posterior, Prior, and Data Means\n\nThe prior mean was 10.\nThe posterior mean is \\(14.47\\).\nThe sample mean of the observed data \\(\\mathbf{y}\\) is:\n\\[\\bar{y} = \\frac{146}{7} \\approx 20.86\\]\n\nThe posterior mean is a compromise between the prior mean (10) and the sample mean (20.86), indicating that the data has moved the posterior away from the prior, but not completely to the observed sample mean. This reflects the influence of both the prior belief and the observed data."
  },
  {
    "objectID": "Tutorial1_soln.html#solutions",
    "href": "Tutorial1_soln.html#solutions",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "Given:\n- The number of placenta previa births: \\(n = 980\\)\n- The number of female births: \\(x = 437\\)\n- Prior distribution: \\(\\theta \\sim \\text{Beta}(1,1)\\) (which is equivalent to a uniform prior).\n\n\n\nPosterior Distribution\nThe binomial likelihood for the number of female births \\(x\\) out of \\(n\\) total births is:\n\\[p(\\theta | y) = {n \\choose y} \\theta^x (1 - \\theta)^{n - y}\\]\nThe prior distribution is \\(\\theta \\sim \\text{Beta}(1,1)\\). The Beta-Binomial conjugacy allows us to combine the likelihood and the prior to get the posterior distribution:\n\\[\\theta | y \\sim \\text{Beta}(437 + 1, 980 - 437 + 1) = \\text{Beta}(438, 544)\\]\nPosterior Mean\nThe mean of a Beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\) is given by:\n\\[E(\\theta | x) = \\frac{\\alpha}{\\alpha + \\beta}\\]\nSubstituting \\(\\alpha = 438\\) and \\(\\beta = 544\\):\n\\[E(\\theta | x) = \\frac{438}{438 + 544} = \\frac{438}{982} \\approx 0.446\\]\n95% Credible Interval\nThe 95% credible interval for \\(\\theta\\) can be obtained using the quantiles of the Beta distribution. In R, you can use:\nqbeta(c(0.025, 0.975), 438, 544)\nThis gives the interval \\([0.415, 0.478]\\), meaning that with 95% probability, the true value of \\(\\theta\\) lies between 0.415 and 0.478.\n\n\n\n\n\n\n\n\n\nSimulating 1000 Samples\nTo simulate 1000 samples from the posterior \\(\\text{Beta}(438, 544)\\) distribution, you can use the rbeta() function in R:\nsamples &lt;- rbeta(1000, 438, 544)\nHistogram\nYou can create a histogram of the samples to visualize the posterior distribution:\nhist(samples, main = \"Posterior Distribution\", xlab = \"Theta\", col = \"lightblue\", border = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval from Simulated Data\nThe 95% credible interval based on the samples can be calculated as:\nquantile(samples, c(0.025, 0.975))\nThis should give an interval close to \\([0.415, 0.478]\\) obtained from Q1.\nComparison with Analytical Interval\nThe credible interval from the simulation should closely match the analytical interval obtained in Question 1. This shows that the simulation provides a good approximation of the posterior distribution.\n\n\n\n\n\n\nNow, assume the prior distribution is \\(\\theta \\sim \\text{Beta}(6,20)\\).\n\n\n\nNew Posterior Distribution\nThe prior is \\(\\text{Beta}(6,20)\\), and the likelihood is still binomial. Using the conjugacy property of the Beta distribution, the posterior becomes:\n\\[\\theta | x \\sim \\text{Beta}(437 + 6, 980 - 437 + 20) = \\text{Beta}(443, 563)\\]\nPosterior Mean and Credible Interval\nThe posterior mean is:\n\\[E(\\theta | x) = \\frac{443}{443 + 563} = \\frac{443}{1006} \\approx 0.440\\]\nThe 95% credible interval can be obtained similarly using R:\nqbeta(c(0.025, 0.975), 443, 563)\nThis gives the interval \\([0.408, 0.473]\\).\nComparing this to the result from the prior \\(\\text{Beta}(1,1)\\), we observe that using the Beta(6,20) prior shifts the posterior slightly, reflecting the stronger prior information.\n\n\n\n\n\n\nNote: Students have not seen an example of using these distributions in class.\n\n\n\nLikelihood Function\nThe likelihood for a Poisson model is:\n\\[p(\\lambda | \\mathbf{y}) = \\prod_{i=1}^n \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\\] Ignoring constants, the likelihood is proportional to:\n\\[p(\\lambda | \\mathbf{y}) \\propto \\lambda^{\\sum y_i} e^{-n \\lambda}\\]\nAppropriateness of Gamma Prior\nThe Gamma distribution is a conjugate prior for the Poisson likelihood because the posterior distribution remains in the Gamma family. This simplifies computation and provides an intuitive interpretation of the prior parameters.\nFinding \\(a\\) and \\(b\\) for Prior\nThe mean and standard deviation of the Gamma distribution are given by:\n\\[\\mu = \\frac{\\alpha}{\\beta}, \\quad \\sigma^2 = \\frac{\\alpha}{\\beta^2}\\] Given that \\(\\mu = 10\\) and \\(\\sigma = 1\\), solve for \\(\\alpha\\) and \\(\\beta\\):\n\\[\\alpha = \\frac{10^2}{1^2} = 100, \\quad \\beta = \\frac{10}{1^2} = 10\\] Hence, \\(\\lambda \\sim \\text{Gamma}(100,10)\\).\nPosterior Distribution\n\n\\(p(\\lambda|y) \\propto p(y|\\lambda)p(\\lambda)\\)\nThe prior: \\(p(\\lambda) \\propto \\lambda^{a-1}e^{-b\\lambda}\\)\n(note: the \\(\\frac{b^a}{\\Gamma(a)}\\) can be removed under the proportionality)\nThe posterior: \\(p(\\lambda|y) \\propto \\lambda^{a-1}e^{-b\\lambda}\\lambda^{\\sum{y_i}}e^{-n\\lambda} = \\lambda^{\\boxed{a + \\sum{y_i}}-1}e^{-\\boxed{(b+n)}\\lambda}\\)\n(note: this has the same form as a gamma distribution with \\(a_{new} = a + \\sum{y_i}\\) and \\(b_{new} = b+n\\))\nTherefore \\(\\lambda|y \\sim gamma(a + \\sum{y_i},b+n)\\)\nWith the data \\(\\mathbf{y} = (17, 25, 25, 21, 13, 22, 23)\\), we have \\(\\sum y_i = 146\\), \\(n = 7\\). So, the posterior is \\(\\lambda | \\mathbf{y} \\sim \\text{Gamma}(246, 17)\\)\n\nPosterior Mean Estimate\nThe posterior mean is:\n\\[E(\\lambda | \\mathbf{y}) = \\frac{246}{17} \\approx 14.47\\]\nConjugacy of the Prior\nYes, the Gamma distribution is a conjugate prior for the Poisson likelihood, as the posterior distribution remains a Gamma distribution.\nComparison of Posterior, Prior, and Data Means\n\nThe prior mean was 10.\nThe posterior mean is \\(14.47\\).\nThe sample mean of the observed data \\(\\mathbf{y}\\) is:\n\\[\\bar{y} = \\frac{146}{7} \\approx 20.86\\]\n\nThe posterior mean is a compromise between the prior mean (10) and the sample mean (20.86), indicating that the data has moved the posterior away from the prior, but not completely to the observed sample mean. This reflects the influence of both the prior belief and the observed data."
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Tutorials",
    "text": "Tutorials\nTutorial Sheet 1: Bayesian inference using binomial and Poisson models"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "Please submit your answers to this assignment by October 23rd, 4:00 PM. Ensure that all calculations are clearly shown, and include explanations where necessary. You may refer to the provided distribution information to help with your answers."
  },
  {
    "objectID": "Assignment1.html#instructions",
    "href": "Assignment1.html#instructions",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "Please submit your answers to this assignment by October 23rd, 4:00 PM. Ensure that all calculations are clearly shown, and include explanations where necessary. You may refer to the provided distribution information to help with your answers."
  },
  {
    "objectID": "Assignment1.html#question",
    "href": "Assignment1.html#question",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "Question",
    "text": "Question\nThe energy of eight(8) particles being emitted form a radioactive source was recorded as:\n\\(y = 50, 60, 60, 80, 40, 40, 80, 70\\text{ MeV}\\).\nWe assume the particles have the following data model:\n\\(y_i \\sim N(\\mu,\\rho \\mu^2)\\),\nwhere \\(\\rho = 1\\)\nA nuclear physicist provides an expert opinion that particles will be emitted with an average energy of 80 MeV. However, she acknowledges uncertainty in this value and suggests the average energy could lie anywhere between 50 MeV and 110 MeV.\n\nShow that the likelihood for \\({\\bf y}\\) is of the form:\n\n\\[p({\\bf y}|\\mu) \\propto \\frac{1}{\\mu^8} \\exp \\left[ -\\frac{15300}{\\mu^2} + \\frac{480}{\\mu} \\right]\\]\n\nAssume a prior for \\(\\mu\\), such that \\(\\mu \\sim gamma(a,b)\\). Use moment matching to determine the values of \\(a\\) and \\(b\\) such that this prior reflects the nuclear physicist’s opinions about the average energy of the particles. Specifically, assume that the range she provided (50 MeV to 110 MeV) is \\(\\pm\\) 3 standard deviations from the mean.\nWrite down the prior probability density function \\(p(\\mu)\\), up to a constant.\nWrite down the posterior probability density function \\(p(\\mu |y)\\), up to a constant.\n\n\nDistribution Information\n\nYou can use the following information about the Normal and Gamma distributions to help you with the question:\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distribution\nPMF/PDF\nE(X)\nVar(X)\n\n\n\n\nNormal\n\\[\\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{1}{2\\sigma^2}(x_i - \\mu)^2\\right)\\]\n\\[\\mu\\]\n\\[\\sigma^2\\]\n\n\nGamma\n\\[\\frac{b^a}{\\Gamma(a)}x^{a-1}e^{-b x},\\ x \\geq 0\\]\n\\[\\frac{a}{b}\\]\n\\[\\frac{a}{b^2}\\]"
  },
  {
    "objectID": "2b-BayesianInference.html",
    "href": "2b-BayesianInference.html",
    "title": "Bayesian Data Analysis",
    "section": "",
    "text": "Bayesian point estimates are often given by:\n\nthe posterior mean \\(E(\\theta|y)\\)\nor the posterior median \\(\\theta^*\\) with \\(P(\\theta &lt; \\theta^*|y) = 0.5\\)\n\nUncertainty is quantified with credible intervals (CIs)\n\nAn interval is a 95% credible interval if the posterior probability that \\(\\theta\\) is in the interval is 0.95.\nOften quantile based, given by posterior quantiles with \\(P(\\theta &lt; \\theta_{\\alpha/2}|y) = P(\\theta &gt; \\theta_{1-\\alpha/2}|y) = \\alpha/2\\)"
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Assignments",
    "text": "Assignments\nAssignment 1: Bayesian Inference on Particle Emission Energy"
  },
  {
    "objectID": "4-MCMC.html#what-do-we-know-so-far",
    "href": "4-MCMC.html#what-do-we-know-so-far",
    "title": "Bayesian Data Analysis",
    "section": "What do we know so far?",
    "text": "What do we know so far?\nWe can use Bayes’ rule to get posterior densities\n\nExample for single parameter Normal - \\(\\mu\\)\n\n\\(p(\\mu|y) \\propto p(y|\\mu)p(\\mu)\\)\n\nExample for single parameter Normal - \\(\\tau\\)\n\n\\(p(\\tau|y) \\propto p(y|\\tau)p(\\tau)\\)\n\nWe could also do this for multiple parameters\n\n\\(p(\\mu, \\tau|y) \\propto p(y|\\mu,\\tau)p(\\mu,\\tau)\\)\n\n\nBUT…\n\nRealistic problems with multiple parameters, data points, and common choices of priors don’t get a closed-form expression for \\(p(\\mu, \\tau|y)\\)\nSampling to the rescue!!"
  },
  {
    "objectID": "4-MCMC.html#simulation-based-inference-recap",
    "href": "4-MCMC.html#simulation-based-inference-recap",
    "title": "Bayesian Data Analysis",
    "section": "Simulation-based inference (recap)",
    "text": "Simulation-based inference (recap)\n\nThe general idea in simulation-based inference: We can make inference about a parameter \\(\\theta\\) or \\(\\mu\\) or \\(\\tau\\) , using a sample e.g., \\(\\{\\theta^{(1)}\\ldots\\theta^{(S)}\\}\\), from its probability distribution.\nAssessing the properties of a target (e.g., posterior) distribution by generating representative samples is called Monte Carlo simulation.\nBased on the law of large numbers we know that:\n\\(\\frac{1}{S}\\sum_{s=1}^{S}\\theta^{(s)} = E(\\theta)\\)\nas sample size \\(S \\to \\infty\\)\n\nThe error in the MC approximation goes to zero as \\(S \\to \\infty\\) because \\(\\frac{var(\\theta)}{S} \\to 0\\)\n\nJust about any aspect of the distribution of \\(\\theta\\) can be approximated arbitrarily exactly with a large enough Monte Carlo sample, e.g.,\n\nthe \\(\\alpha\\)-percentile of the distribution of \\(\\theta\\)\n\\(Pr(\\theta \\geq x)\\) for any constant \\(x\\)"
  },
  {
    "objectID": "4-MCMC.html#markov-chain-monte-carlo-mcmc",
    "href": "4-MCMC.html#markov-chain-monte-carlo-mcmc",
    "title": "Bayesian Data Analysis",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\nWe’ve already discussed Monte Carlo sampling. Now we’re going to consider Markov Chain Monte Carlo (MCMC) sampling. MCMC algorithms are used for sampling from a target distribution.\n\n\n\n\n\n\nMarkov Chain\n\n\n\nAny process in which each step has no memory of states before the current state is called a (first-order) Markov process and a sucession of such steps is a Markov chain.\nThe stationary distribution of the Markov chain is the target distribution.\n\n\n\n\nUsing MCMC we can construct a set of samples from an unknown target (e.g., posterior) distribution."
  },
  {
    "objectID": "4-MCMC.html#a-simple-markov-chain-example",
    "href": "4-MCMC.html#a-simple-markov-chain-example",
    "title": "Bayesian Data Analysis",
    "section": "A simple Markov chain example",
    "text": "A simple Markov chain example\nWhat is the probability of sunny \\(P(S)\\) and rainy \\(P(R)\\) weather?\n\n\n\n\nAssume we have the following “transition” probabilities:\n\\(P(S_{t+1}|R_t) = 0.5\\)\n\\(P(R_{t+1}|R_t) = 0.5\\)\n\\(P(R_{t+1}|S_t) = 0.1\\)\n\\(P(S_{t+1}|S_t) = 0.9\\)\nNote the future state of the weather only depends on the current state."
  },
  {
    "objectID": "4-MCMC.html#creating-the-markov-chain",
    "href": "4-MCMC.html#creating-the-markov-chain",
    "title": "Bayesian Data Analysis",
    "section": "Creating the Markov chain",
    "text": "Creating the Markov chain\n\n\nThe “algorithm”\n\nWe have to initialize i.e., a randomly chosen start point of sunny or rainy.\nWe can sample the weather, W, (coded as 1 for sunny and 0 for rainy) from a Bernoulli distribution (Binomial with \\(n=1\\)).\n\n\nThe probability distribution will change depending on the current state of the chain, i.e., if W = 1, or W =0.\n\n\nBased on the samples of \\(W\\) we can estimate \\(P(W=1) = P(S)\\) and \\(P(W=0) = P(R)\\).\n\nWe find after enough iterations (samples) \\(P(S) = 0.83\\).\n\nR code\n\n# 0 = rainy, 1 = sunny\n\n# 1. initialize\nW_current &lt;- rbinom(1,1,0.5) \n\n# 2. run the Markov Chain\n# number of Monte Carlo sampling interations\nn_iter &lt;- 10000\n# save the samples\nW_new &lt;- p_sunny &lt;- rep(NA, n_iter)\n\nfor(i in 1:n_iter)\n{\nW_new[i] &lt;- ifelse(W_current == 0,\n       rbinom(1,1,0.5),\n       rbinom(1,1,0.9))\n\nW_current = W_new[i]\n\n# 3. get the probability of sunny\np_sunny[i] &lt;- sum(W_new, na.rm = TRUE)/i\n\n}\n\nplot(p_sunny, type = \"l\", xlab = \"Iteration\", ylab = \"P(S)\")"
  },
  {
    "objectID": "4-MCMC.html#stationary-distribution",
    "href": "4-MCMC.html#stationary-distribution",
    "title": "Bayesian Data Analysis",
    "section": "Stationary Distribution",
    "text": "Stationary Distribution\nAs we progress through time, the probability of being in certain states (e.g., rainy or sunny) are more likely than others. Over the long run, the distribution will reach an equilibrium with an associated probability of being in each state.\nThis is known as the Stationary Distribution.\n\n\n\n\n\n\nThe stationary distribution of a Markov chain\n\n\nThe stationary distribution of a Markov chain is a probability distribution over the states of the chain that remains unchanged as time progresses.\nMathematically, if \\(\\pi\\) represents the stationary distribution (a row vector of probabilities) and \\(P\\) is the transition matrix of the Markov chain, the stationary distribution satisfies the equation:\n\\[\\pi = \\pi P\\]\nThis means that when the transition matrix \\(P\\) is applied to \\(\\pi\\), the result is still \\(\\pi\\), indicating that the distribution has stabilized and doesn’t change over time."
  },
  {
    "objectID": "4-MCMC.html#the-metropolis-algorithm",
    "href": "4-MCMC.html#the-metropolis-algorithm",
    "title": "Bayesian Data Analysis",
    "section": "The Metropolis Algorithm",
    "text": "The Metropolis Algorithm\n\nSuppose we have a target distribution \\(p(\\theta|y)\\) from which we would like to generate a representative sample.\nSample values from the target distribution can be generated by taking a random walk through the parameter space.\n\n\n\n\n\n\n\nThe metropolis algorithm\n\n\n\nStart at some arbitrary parameter value (initial value).\nPropose a move to a new value in the parameter space.\nCalculate the acceptance ratio \\(r = min \\bigg(1, \\frac{ p(y|\\theta_{pro})p(\\theta_{pro})}{p(y|\\theta_{cur})p(\\theta_{cur})}\\bigg)\\).\nDraw a random number, \\(u\\) between 0 and 1. If \\(u &lt; r\\) then the move is accepted.\nRepeat until a representative sample from the target distribution has been generated (more on this later)."
  },
  {
    "objectID": "4-MCMC.html#metropolis-algorithm-for-the-happiness-example",
    "href": "4-MCMC.html#metropolis-algorithm-for-the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "Metropolis Algorithm for the Happiness example",
    "text": "Metropolis Algorithm for the Happiness example\nRecall for the Happiness example: n = 20 women, y = 14 women reported being happy.\n\n\nThe Metropolis algorithm\n\\(y \\sim Binomial(n = 20, \\theta)\\)\n\\(\\theta|a,b \\sim Beta(a = 1 ,b = 1)\\)\n\nLet’s initialise using \\(\\theta_{cur}\\) = 0.5\nPropose a new move using a Normal proposal distribution such that \\(\\theta_{pro} = \\theta_{cur} + N(0,\\sigma)\\)\n\\(r = \\text{min} \\bigg(1, \\frac{ dbinom(y,n,\\theta_{pro})dbeta(\\theta_{pro},1,1)}{dbinom(y,n,\\theta_{cur})dbeta(\\theta_{cur},1,1)}\\bigg)\\)\nCompare \\(u \\sim Uniform(0,1)\\) with \\(r\\) and accept move if \\(u &lt; r\\)\nRepeat\n\nVisualise the samples and look for a “fuzzy caterpiller” 🐛\n\nR code\n\n# data \ny &lt;- 14; N &lt;- 20\n# beta prior parameters\na &lt;- 1; b &lt;- 1\n# number of samples to generate\nn_iter &lt;- 10000\n\n# 0. \ntheta_cur &lt;- rep(NA, n_iter)\ntheta_cur[1] &lt;- 0.5\n\nfor(i in 1:(n_iter-1)){\n# 1.\ntheta_pro &lt;- theta_cur[i] + rnorm(1,0,sd = 0.2)\n# 2. \nif(theta_pro&lt;0|theta_pro&gt;1){r &lt;- 0 } # set to zero if theta outside [0,1]\nelse {\n  r &lt;- \nmin(1,dbinom(14,20,theta_pro)*dbeta(theta_pro,a,b)/\n      dbinom(14,20,theta_cur[i])*dbeta(theta_cur[i],a,b))\n  }\n# 3. \nu &lt;- runif(1,0,1)\naccept &lt;- u &lt; r\ntheta_cur[i+1]&lt;- ifelse(accept,theta_pro,theta_cur[i])\n} # end i loop\n\nplot(theta_cur, type = \"l\")"
  },
  {
    "objectID": "4-MCMC.html#trace-plot-and-chains-for-theta",
    "href": "4-MCMC.html#trace-plot-and-chains-for-theta",
    "title": "Bayesian Data Analysis",
    "section": "Trace plot and chains for \\(\\theta\\)",
    "text": "Trace plot and chains for \\(\\theta\\)\n\nTraceplots provide a visual tool for monitoring convergence of an MCMC chain towards a target distribution (i.e., the posterior).\nIn general we look for a stationary plot where the sample values display a random scatter around a mean value.\nWe typically initialise MCMC algorithms at 3 different start points to see if all chains end up in (converge to) the same place."
  },
  {
    "objectID": "4-MCMC.html#metropolis-algorithm-for-the-kid-iq-example",
    "href": "4-MCMC.html#metropolis-algorithm-for-the-kid-iq-example",
    "title": "Bayesian Data Analysis",
    "section": "Metropolis algorithm for the Kid IQ example",
    "text": "Metropolis algorithm for the Kid IQ example\nRecall that data (y) are available on the cognitive test scores of three- and four-year-old children in the USA. The sample contains 434 observations.\n\\(y_i|\\mu,\\sigma^2 \\sim N(\\mu, \\sigma^2)\\). Assume \\(\\sigma^2\\) is known where \\(\\sigma = 20.4\\)\n\\(\\mu|\\mu_0,\\sigma_0 \\sim N(\\mu_0 = 80, \\sigma^2_{_0} = 10^2)\\)\n\nLet’s initialise using \\(\\mu_{cur}\\) = 80.\nPropose a new move using a Normal proposal distribution such that \\(\\mu_{pro} = \\mu_{cur} + N(0,1)\\).\nCalculate the acceptance ratio on the log scale (avoids numerical instability).\n\n\\(log(r) = \\text{min}(0, \\sum_i log(dnorm(y_i,\\mu_{pro},20.4)) + log(dnorm(\\mu_{pro},80,10))\\) \\(- \\sum_i log(dnorm(y_i,\\mu_{cur},20.4)) - log(dnorm(\\mu_{cur},80,10)))\\)\n\nCompare \\(u \\sim Uniform(0,1)\\) with \\(r\\) and accept move if \\(log(u) &lt; r\\).\n\nTask: Code this in R and produce a trace plot for \\(\\mu\\)."
  },
  {
    "objectID": "4-MCMC.html#bayesian-inference-for-mu-and-sigma-the-kid-iq-example",
    "href": "4-MCMC.html#bayesian-inference-for-mu-and-sigma-the-kid-iq-example",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Inference for \\(\\mu\\) and \\(\\sigma\\) (the Kid IQ example)",
    "text": "Bayesian Inference for \\(\\mu\\) and \\(\\sigma\\) (the Kid IQ example)\n\nIt is more realistic to assume \\(\\mu\\) and \\(\\sigma\\) are unknown.\nIn this case we need priors for both parameters and we think about them jointly.\nThen from Bayes’ rule we can get the joint posterior \\(p(\\mu, \\sigma |y) \\propto p(y|\\mu, \\sigma)p(\\mu,\\sigma)\\)\nProblem: most choices of prior will not result in a closed from expression for the posterior.\nSolution: If we can sample from the target (posterior) distribution we can still do inference.\nThe Metropolis method is very useful but can be inefficient. Another sampling method that’s often used for models with multiple parameters is Gibbs sampling."
  },
  {
    "objectID": "4-MCMC.html#the-gibbs-sampler",
    "href": "4-MCMC.html#the-gibbs-sampler",
    "title": "Bayesian Data Analysis",
    "section": "The Gibbs Sampler",
    "text": "The Gibbs Sampler\nWe can use Gibbs Sampling when we can sample directly from the conditional posterior distributions for each model parameter.\nSo instead of trying to sample directly from a joint posterior distribution, we sample parameters sequentially from their complete conditional distributions (conditioning on data as well as all other model parameters).\n\n\n\n\n\n\nThe Gibbs sampling algorithm for \\(\\mu\\) and \\(\\tau\\)\n\n\n\nAssign initial values to the parameters \\(\\mu^{(1)}\\) and \\(\\tau^{(1)}\\)\nGiven starting values \\(\\mu^{(1)}\\) and \\(\\tau^{(1)}\\), draw samples s = 2, 3,….,`some large number’ as follows:\n1.1. sample \\(\\tau^{(s)}\\) from \\(p(\\tau|y,\\mu^{(s-1)})\\).\n1.2. sample \\(\\mu^{(s)}\\) from \\(p(\\mu|y,\\tau^{(s)})\\)\nRepeat and this will (eventually) generate samples from \\(p(\\mu,\\sigma|y)\\), which is what we want!"
  },
  {
    "objectID": "4-MCMC.html#remember-those-complete-conditional-distributions",
    "href": "4-MCMC.html#remember-those-complete-conditional-distributions",
    "title": "Bayesian Data Analysis",
    "section": "Remember those complete conditional distributions?",
    "text": "Remember those complete conditional distributions?\nRecall for the Normal likelihood \\(y_i|\\mu,\\sigma^2 \\sim N(\\mu, \\sigma^2)\\) where \\(\\sigma\\) is known and a Normal prior \\(\\mu \\sim N(\\mu_0, \\sigma^2_{_0})\\), we get a posterior for \\(\\mu\\) that is also a normal distribution (i.e., we used a conjugate prior)\n\\[\\mu|y \\sim N \\bigg(\\frac{\\mu_0/\\sigma^2_{0}+ n\\bar{y}/\\sigma^2}{1/\\sigma^2_{0}+n/\\sigma^2}, {\\frac{1}{1/\\sigma^2_{0}+n/\\sigma^2}}\\bigg)\\]\nRecall for the Normal likelihood \\(y_i|\\mu,\\sigma^2 \\sim N(\\mu, \\sigma^2)\\) where \\(\\mu\\) is known and a Gamma prior \\(\\frac{1}{\\sigma^2} = \\tau \\sim Gamma(a, b)\\) we get a posterior for \\(\\tau\\) that will also be a gamma distribution\n\\[\\tau|y \\sim Gamma \\bigg(a + n/2, b + 1/2\\sum_{i=1}^n (y_i - \\mu)^2\\bigg)\\]\nThese are the complete conditionals. We know the posterior distribution for one parameter conditional on knowning the other parameter(s). Now we can use Gibbs sampling."
  },
  {
    "objectID": "4-MCMC.html#gibbs-sampling-algorithm-for-the-kid-iq-example",
    "href": "4-MCMC.html#gibbs-sampling-algorithm-for-the-kid-iq-example",
    "title": "Bayesian Data Analysis",
    "section": "Gibbs Sampling Algorithm for the Kid IQ example",
    "text": "Gibbs Sampling Algorithm for the Kid IQ example\nRecall data (y) are available on the cognitive test scores of three- and four-year-old children in the USA. The sample contains \\(n=434\\) observations.\n\n\n\\(y \\sim Normal(\\mu, \\sigma^2)\\)\n\\(\\mu \\sim Normal(80 ,10)\\) \\(1/\\sigma^2 = \\tau \\sim gamma(1 ,1)\\)\n\nLet’s initialise using \\(\\mu^{(1)} = 80, \\tau^{(1)} = 1\\)\nGiven starting values \\(\\mu^{(1)}\\) and \\(\\tau^{(1)}\\), draw samples s = 2, 3,….,`some large number’ as follows:\n1.1. sample \\(\\tau^{(s)}\\) from \\(p(\\tau|y,\\mu^{(s-1)})\\).\n1.2. sample \\(\\mu^{(s)}\\) from \\(p(\\mu|y,\\tau^{(s)})\\)\nRepeat and this will (eventually) generate samples from \\(p(\\mu,\\sigma|y)\\), which is what we want!\n\n\n\n## Data\ny &lt;- kidiq$kid_score\nn &lt;- length(y)\n\n## Prior parameters \nmu0 &lt;- 80; sigma.mu0 &lt;- 10 # for normal prior (for mu)\na &lt;- 1; b &lt;- 1 # for gamma prior (for tau)\n\nn_iter &lt;- 10000 ## choose # iter\n## create objects to store results\nmu_s &lt;- tau_s &lt;- sigma_s &lt;- rep(NA, n_iter)\n\n## 0. Initialise \nmu_s[1] &lt;- 80; tau_s[1] &lt;- 1; sigma_s[1] &lt;- 1\n## define parameters for gamma posterior on tau\npost.a = a + n/2; post.b = 1/2*(sum((y-mu_s[1])^2))\n\n## 1.\nfor(s in 2:n_iter){\n## 1.1 sample from complete conditional for tau\ntau_s[s] &lt;- rgamma(1,post.a,post.b)\nsigma_s[s] &lt;- sqrt(1/tau_s[s]) # transform to sigma\n## update posterior parameters for mu\nmupost.mean = (mu0/(sigma.mu0^2) + tau_s[s]*n*mean(y))/(1/(sigma.mu0^2) + tau_s[s]*n)\nmupost.sd = sqrt(1/(1/(sigma.mu0^2)+tau_s[s]*n))\n## 1.2 sample from complete conditional for mu\nmu_s[s] &lt;- rnorm(1,mupost.mean,mupost.sd)\n## update posterior parameters for tau\npost.a = a + n/2\npost.b = b + 1/2*(sum((y-mu_s[s])^2))\n} # end s loop"
  },
  {
    "objectID": "4-MCMC.html#output-from-mcmc",
    "href": "4-MCMC.html#output-from-mcmc",
    "title": "Bayesian Data Analysis",
    "section": "Output from MCMC",
    "text": "Output from MCMC\nMCMC samples are not independent draws from a target distribution:\n\nThe first draw is not a random draw from the target distribution and tuning of MCMC parameters may occur at the start of a chain.\nSubsequently, draw s + 1 depends on draw s: the samples may be autocorrelated.\n\nWe can use samples from an MCMC algorithm to do inference but ONLY IF\n\nWe exclude samples from the initial period (burn-in).\nWe “wait long enough” to get a set of samples that are representative of the target (posterior). distribution.\n\n\n\n# Create a dataset (tibble) of samples\n# chose a \"burn-in\" \nn_burnin &lt;- 100\n# Remove the first sample from mu_s and sigma_s\npost_samps &lt;- tibble(sample_index = 1:(n_iter-n_burnin),\n                     mu_s = mu_s[-(1:n_burnin)], \n                     sigma_s = sigma_s[-(1:n_burnin)])\npost_samps\n\n\n# A tibble: 9,900 × 3\n   sample_index  mu_s sigma_s\n          &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1            1  86.7    20.8\n 2            2  85.2    20.9\n 3            3  85.8    19.3\n 4            4  86.6    20.3\n 5            5  88.1    20.3\n 6            6  87.6    19.5\n 7            7  89.8    20.7\n 8            8  87.5    19.8\n 9            9  84.8    21.6\n10           10  86.8    20.9\n# ℹ 9,890 more rows"
  },
  {
    "objectID": "4-MCMC.html#output-from-mcmc-trace-plots",
    "href": "4-MCMC.html#output-from-mcmc-trace-plots",
    "title": "Bayesian Data Analysis",
    "section": "Output from MCMC: trace plots",
    "text": "Output from MCMC: trace plots\nWe can use trace plots to investigate the MCMC samples/chains for the parameters.\n\n\nlibrary(ggplot2)\n\n# Trace plot for mu\np1 &lt;- ggplot(post_samps, aes(x = sample_index, mu_s)) +\n        geom_line(colour = \"blue\") +\n        ggtitle(\"Posterior Samples of mu\")\n\n# Trace plot for sigma\np2 &lt;- ggplot(post_samps, aes(x = sample_index, sigma_s)) +\n        geom_line(colour = \"blue\") +\n        ggtitle(\"Posterior Samples of sigma\")\n\np1;p2"
  },
  {
    "objectID": "4-MCMC.html#output-from-mcmc-joint-parameter-plots",
    "href": "4-MCMC.html#output-from-mcmc-joint-parameter-plots",
    "title": "Bayesian Data Analysis",
    "section": "Output from MCMC: joint parameter plots",
    "text": "Output from MCMC: joint parameter plots\nWe can visualise the joint posterior distribution of the parameters based on the MCMC samples.\n\n\n# Scatter plot of mu_s vs. sd_s\np3 &lt;- ggplot(post_samps, aes(x = sigma_s, mu_s)) +\n        geom_point(colour = \"blue\") \n\n# Contour plot of joint density\np4 &lt;- ggplot(post_samps, aes(x = sigma_s, y = mu_s)) + \n  geom_density_2d(colour = \"blue\")    \n\np3;p4"
  },
  {
    "objectID": "4-MCMC.html#output-from-mcmc-density-plots-and-summaries",
    "href": "4-MCMC.html#output-from-mcmc-density-plots-and-summaries",
    "title": "Bayesian Data Analysis",
    "section": "Output from MCMC: density plots and summaries",
    "text": "Output from MCMC: density plots and summaries\nWe can visualise and obtain summaries for the marginal posterior distributions of the parameters based on the MCMC samples.\n\n\n# Density plot for posterior samples of mu\np5 &lt;- ggplot(post_samps, aes(x = mu_s)) +\n        geom_density(fill = \"lightblue\", \n                     color = \"blue\", \n                     alpha = 0.6) +\n        ggtitle(\"Posterior Density of mu\") \n\n# Mean and credible interval for mu\nmu_mean &lt;- mean(mu_s)\nmu_ci &lt;- quantile(mu_s, probs = c(0.025, 0.975))\n\np5;cat(\"Mean of Mu:\", mu_mean, \"\\n\");cat(\"95% Credible Interval for Mu:\", mu_ci, \"\\n\")\n\n\n\n\n\n\n\n\n\nMean of Mu: 86.73195 \n\n\n95% Credible Interval for Mu: 84.80967 88.63966 \n\n\n\n\np6 &lt;- ggplot(post_samps, aes(x = sigma_s)) +\n        geom_density(fill = \"lightblue\", \n                     color = \"blue\", \n                     alpha = 0.6) +\n        ggtitle(\"Posterior Density of sigma\") \n\n# Mean and credible interval for sigma\nsigma_mean &lt;- mean(sigma_s)\nsigma_ci &lt;- quantile(sigma_s, probs = c(0.025, 0.975))\n\np6;cat(\"Mean of Sigma:\", sigma_mean, \"\\n\");cat(\"95% Credible Interval for Sigma:\", sigma_ci, \"\\n\")\n\n\n\n\n\n\n\n\n\nMean of Sigma: 20.40145 \n\n\n95% Credible Interval for Sigma: 19.10572 21.84694"
  },
  {
    "objectID": "4-MCMC.html#stationary-distributions",
    "href": "4-MCMC.html#stationary-distributions",
    "title": "Bayesian Data Analysis",
    "section": "Stationary Distributions",
    "text": "Stationary Distributions\nAs we progress through time, the probability of being in certain states (e.g., rainy or sunny) are more likely than others. Over the long run, the distribution will reach an equilibrium with an associated probability of being in each state.\nThis is known as the Stationary Distribution.\n\n\n\n\n\n\nThe stationary distribution of a Markov chain\n\n\nThe stationary distribution of a Markov chain is a probability distribution over the states of the chain that remains unchanged as time progresses.\nMathematically, if \\(\\pi\\) represents the stationary distribution (a row vector of probabilities) and \\(P\\) is the transition matrix of the Markov chain, the stationary distribution satisfies the equation:\n\\[\\pi = \\pi P\\]\nThis means that when the transition matrix \\(P\\) is applied to \\(\\pi\\), the result is still \\(\\pi\\), indicating that the distribution has stabilized and doesn’t change over time."
  },
  {
    "objectID": "4-MCMC.html#gibbs-aampling-algorithm-for-the-kid-iq-example",
    "href": "4-MCMC.html#gibbs-aampling-algorithm-for-the-kid-iq-example",
    "title": "Bayesian Data Analysis",
    "section": "Gibbs aampling algorithm for the Kid IQ example",
    "text": "Gibbs aampling algorithm for the Kid IQ example\nRecall data (y) are available on the cognitive test scores of three- and four-year-old children in the USA. The sample contains \\(n=434\\) observations.\n\n\n\\(y \\sim Normal(\\mu, \\sigma^2)\\)\n\\(\\mu \\sim Normal(80 ,10)\\) \\(1/\\sigma^2 = \\tau \\sim gamma(1 ,1)\\)\n\nLet’s initialise using \\(\\mu^{(1)} = 80, \\tau^{(1)} = 1\\)\nGiven starting values \\(\\mu^{(1)}\\) and \\(\\tau^{(1)}\\), draw samples s = 2, 3,….,`some large number’ as follows:\n1.1. sample \\(\\tau^{(s)}\\) from \\(p(\\tau|y,\\mu^{(s-1)})\\).\n1.2. sample \\(\\mu^{(s)}\\) from \\(p(\\mu|y,\\tau^{(s)})\\)\nRepeat and this will (eventually) generate samples from \\(p(\\mu,\\sigma|y)\\), which is what we want!\n\n\n\n## Data\ny &lt;- kidiq$kid_score\nn &lt;- length(y)\n\n## Prior parameters \nmu0 &lt;- 80; sigma.mu0 &lt;- 10 # for normal prior (for mu)\na &lt;- 1; b &lt;- 1 # for gamma prior (for tau)\n\nn_iter &lt;- 10000 ## choose # iter\n## create objects to store results\nmu_s &lt;- tau_s &lt;- sigma_s &lt;- rep(NA, n_iter)\n\n## 0. Initialise \nmu_s[1] &lt;- 80; tau_s[1] &lt;- 1; sigma_s[1] &lt;- 1\n## define parameters for gamma posterior on tau\npost.a = a + n/2; post.b = 1/2*(sum((y-mu_s[1])^2))\n\n## 1.\nfor(s in 2:n_iter){\n## 1.1 sample from complete conditional for tau\ntau_s[s] &lt;- rgamma(1,post.a,post.b)\nsigma_s[s] &lt;- sqrt(1/tau_s[s]) # transform to sigma\n## update posterior parameters for mu\nmupost.mean = (mu0/(sigma.mu0^2) + tau_s[s]*n*mean(y))/(1/(sigma.mu0^2) + tau_s[s]*n)\nmupost.sd = sqrt(1/(1/(sigma.mu0^2)+tau_s[s]*n))\n## 1.2 sample from complete conditional for mu\nmu_s[s] &lt;- rnorm(1,mupost.mean,mupost.sd)\n## update posterior parameters for tau\npost.a = a + n/2\npost.b = b + 1/2*(sum((y-mu_s[s])^2))\n} # end s loop"
  }
]