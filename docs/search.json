[
  {
    "objectID": "0-Information.html#information",
    "href": "0-Information.html#information",
    "title": "Bayesian Data Analysis",
    "section": "Information",
    "text": "Information\nüè¢ Office: Room 223, Logic House\nüìß Email: niamh.cahill@mu.ie\nüìÖ Lectures:\n\nüìç Mondays @11am in ELT\n\nüìç Tuesdays @10am in Physics Hall (PH)\n\nüìö Tutorials:\n\nüìç Wednesdays @3pm, 4pm, 5pm in GFLAB, starting Week 4 (Oct 14th)"
  },
  {
    "objectID": "0-Information.html#information-1",
    "href": "0-Information.html#information-1",
    "title": "Bayesian Data Analysis",
    "section": "Information",
    "text": "Information\nüìù Assessment:\n\n4 Assignments worth 10%\n1 Midterm worth 15% (November 11th)\nFinal Exam worth 75% (Date TBC)\n\nüìñ Textbooks:\n\nDoing Bayesian Data Analysis by J. K. Kruschke\n\nBayesian Data Analysis by A. Gelman et al.\n\n‚è∞ Office Hours: By appointment (and please do make an appointment if needed)"
  },
  {
    "objectID": "0-Information.html#lecturing-format",
    "href": "0-Information.html#lecturing-format",
    "title": "Bayesian Data Analysis",
    "section": "Lecturing Format",
    "text": "Lecturing Format\n\nüìù Lecture notes will be provided, but feel free to take additional notes for your understanding.\nüåê All course materials will be available on Moodle for easy access.\nLectures will be a mix of theory and practical examples, to help you apply what you learn.\nüíª R code will be provided along with lecture notes whenever relevant.\n‚ùì Please ask questions! Interaction is encouraged to help clarify any doubts."
  },
  {
    "objectID": "0-Information.html#what-is-this-course-about",
    "href": "0-Information.html#what-is-this-course-about",
    "title": "Bayesian Data Analysis",
    "section": "What is this Course About?",
    "text": "What is this Course About?\n\nüéØ The goal of this course is to introduce you to Bayesian Data Analysis (BDA).\nüìö BDA relies on two foundational ideas:\n\nüîÑ The first idea is the updating of uncertainty across possibilities through Bayesian inference.\nüé≤ The second idea is that parameter values are given probability distributions, known as ‚Äúprior‚Äù distributions."
  },
  {
    "objectID": "0-Information.html#what-topics-will-we-cover",
    "href": "0-Information.html#what-topics-will-we-cover",
    "title": "Bayesian Data Analysis",
    "section": "What topics will we cover?",
    "text": "What topics will we cover?\nThis course will cover (but is not limited to) the following topics:\n\nBayes‚Äô Rule\nInferring a Binomial Probability\nPrior Distributions\nMarkov Chain Monte Carlo\nGibbs Sampler\nAssessing Model Convergence\nJAGS\nSimple Models with JAGS\nHierarchical Models with JAGS\nModel Checking"
  },
  {
    "objectID": "0-Information.html#diversity-inclusion",
    "href": "0-Information.html#diversity-inclusion",
    "title": "Bayesian Data Analysis",
    "section": "Diversity & inclusion",
    "text": "Diversity & inclusion\nIt is my intent to present materials and activities that are respectful of diversity: gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, and culture. I may not always get this right so please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nTo help with this:\n\nIf you have a name that differs from those that appear in your official University records, please let me know!\nPlease let me know your preferred pronouns if you wish to do so.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "1a-BayesRule.html#bayes-rule",
    "href": "1a-BayesRule.html#bayes-rule",
    "title": "Bayesian Data Analysis",
    "section": "Bayes‚Äô rule",
    "text": "Bayes‚Äô rule\nThomas Bayes‚Äô famous theorem was published in 1763.\n\nFor events A and B:\n\\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\]\n\nThe branch of statistics that you are probably most familiar with up to now is called frequentist statistics.\nBayesian statistics uses Bayes‚Äô rule for inference and decision making, frequentist statistics does not."
  },
  {
    "objectID": "1a-BayesRule.html#crimes-example",
    "href": "1a-BayesRule.html#crimes-example",
    "title": "Bayesian Data Analysis",
    "section": "Crimes Example",
    "text": "Crimes Example\nüîç A Crime Investigation (adapted from Kruschke):\n\nYou are investigating a crime and have 4 suspects: A, B, C, and D.\n\nüë§ Suspect A\n\nüë§ Suspect B\n\nüë§ Suspect C\n\nüë§ Suspect D\n\nüïµ‚Äç‚ôÇÔ∏è You are 100% sure the offender is one of these suspects, and initially, each one is considered equally likely to have committed the crime.\n‚ùó During your investigation, you discover that C did not commit the crime."
  },
  {
    "objectID": "1a-BayesRule.html#crimes-example-1",
    "href": "1a-BayesRule.html#crimes-example-1",
    "title": "Bayesian Data Analysis",
    "section": "Crimes Example",
    "text": "Crimes Example\nQuestion: Can you apply a Bayesian crime investigation to quantify the ‚Äúinformation‚Äù about who committed the crime using probability statements?\n\n\nPrior Information\n\nüë§ Suspect A\n\nüë§ Suspect B\n\nüë§ Suspect C\nüë§ Suspect D\n\n\nNew Data\n\nüë§ Suspect A\n\nüë§ Suspect B\n\n‚ùåüë§ Suspect C (Ruled out)\nüë§ Suspect D\n\n\nGiven what we know about C, what is the probability that A committed the crime?\n\n\nBefore data ‚Äúall 4 equally likely‚Äù: Prob(A) = 1/4\nAfter data ‚ÄúC did not do it‚Äù: Prob(A|C did not do it) = 1/3"
  },
  {
    "objectID": "1a-BayesRule.html#crimes-example-2",
    "href": "1a-BayesRule.html#crimes-example-2",
    "title": "Bayesian Data Analysis",
    "section": "Crimes example",
    "text": "Crimes example\nIs that Bayesian learning? Yes!\n\n\nStep 1: Set prior distribution Prob(A) = Prob(B) = Prob(C) = Prob(D) = 1/4\nStep 2: Update using data ‚ÄúC did not do it‚Äù using Bayes‚Äô rule\n\n\\[P(A|\\text{not } C) = \\frac{P(\\text{not } C|A)P(A)}{P(\\text{not } C)} = \\frac{1 \\times 1/4}{3/4}  = 1/3\\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example",
    "href": "1a-BayesRule.html#sleep-example",
    "title": "Bayesian Data Analysis",
    "section": "Sleep example",
    "text": "Sleep example\nSuppose that you are interested in the probability, that at any given time in the night between 8pm and 4am, you are asleep.\nLet‚Äôs assume we already have the following information on your sleep over this time period:\n\nThis is the probability I am asleep taking into account only the time."
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-1",
    "href": "1a-BayesRule.html#sleep-example-1",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\n\n\n\n\n\nüí° What if we know the time and have additional evidence?\n\n\n\nImagine this scenario: It‚Äôs 11pm, and we have extra evidence ‚Äî my bedroom light is on. How would this affect the probability that I am asleep? \n\nThis is where we use Bayes‚Äô Rule to update our sleep estimate. If we know information about the light, we can apply Bayes‚Äô equation to refine the probability estimate:\nFor example, at 11pm, we can use the probability of me being asleep (our prior) and then adjust it using Bayes‚Äô Rule based on the light being on: \\[\n  P(\\text{sleep}|\\text{light}) = \\frac{P(\\text{light}|\\text{sleep})P(\\text{sleep})}{P(\\text{light})}\n  \\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-2",
    "href": "1a-BayesRule.html#sleep-example-2",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\nüí° We added some information (data) about the light!\n\n\nTo update our sleep probability, we need to know the likelihood of observing this new information, given what we know about sleep habits.\nüîç Key Probabilities Based on My Habits:\n\nIf I am asleep, the probability that my bedroom light is on is very low (1%):\n\\[P(\\text{light|sleep}) = 0.01\\]\nIf I am awake, the probability that my bedroom light is on is quite high (80%):\n\\[P(\\text{light|‚àísleep}) = 0.8\\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-3",
    "href": "1a-BayesRule.html#sleep-example-3",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\nüí° We had our prior information about sleep and added some information (data) about the light!\n\nThe final piece of the equation is \\(P(\\text{light})\\), representing the total probability my light is on.\nThere are two possible conditions where my light could be on:\n\nI am asleep\n\nI am awake\n\n\nTo calculate the total probability, we use:\n\\[\nP(\\text{light}) = P(\\text{light|sleep})P(\\text{sleep}) + P(\\text{light|‚àísleep})P(\\text{‚àísleep})\n\\]"
  },
  {
    "objectID": "1a-BayesRule.html#sleep-example-4",
    "href": "1a-BayesRule.html#sleep-example-4",
    "title": "Bayesian Data Analysis",
    "section": "Sleep Example",
    "text": "Sleep Example\nNow, let‚Äôs put it all together with Bayes‚Äô Rule:\n\\(\\underset{P(A|B)}{P(\\text{sleep}|\\text{light})} =\\)\n\n\n\\[\n\\frac{\\underset{P(B|A)}{P(\\text{light}|\\text{sleep})} \\cdot \\underset{P(A)}{P(\\text{sleep})}}{\\underset{P(B)}{P(\\text{light})}}\n\\]\n\n\\[\n\\frac{\\underset{0.01}{P(\\text{light}|\\text{sleep})} \\cdot \\underset{0.27}{P(\\text{sleep})}}{\\underset{0.59}{P(\\text{light})}}\n\\]\n\nüéØ This allows us to update our belief about the probability that I‚Äôm asleep based on the new evidence (the light being on)!"
  },
  {
    "objectID": "1a-BayesRule.html#likelihood-prior-posterior",
    "href": "1a-BayesRule.html#likelihood-prior-posterior",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood, Prior & Posterior",
    "text": "Likelihood, Prior & Posterior"
  },
  {
    "objectID": "1a-BayesRule.html#bayes-rule-applied-to-parameters-and-data",
    "href": "1a-BayesRule.html#bayes-rule-applied-to-parameters-and-data",
    "title": "Bayesian Data Analysis",
    "section": "Bayes‚Äô rule applied to parameters and data",
    "text": "Bayes‚Äô rule applied to parameters and data\nGiven a set of observed data points \\(Y\\) and a set of parameters \\(\\theta\\), we write Bayes‚Äô rule as \\[\\underset{\\text{posterior}}{P(\\theta|Y)} = \\frac{\\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}}{\\underset{\\text{marginal likelihood}}{P(Y)}}\\]\nWhere the denominator is\n\n\\(P(Y) = \\sum_{\\theta^*}P(Y|\\theta^*)P(\\theta^*)\\) for discrete-valued variables, or\n\\(P(Y) = \\int P(Y|\\theta^*)P(\\theta^*) d\\theta^*\\) for continuous variables.\n\n\\(P(Y)\\) is often difficult to calculate (more on this later) and Baye‚Äôs rule is often written more simply as a proportional statement \\[\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\]"
  },
  {
    "objectID": "1a-BayesRule.html#likelihood-prior-posterior-1",
    "href": "1a-BayesRule.html#likelihood-prior-posterior-1",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood, Prior & Posterior",
    "text": "Likelihood, Prior & Posterior\n\\[\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\]\n\n\n\n\\(P(Y|\\theta)\\) - the likelihood. The likelihood represents the evidence that we have based on data.\n\\(P(\\theta)\\) - the prior. The prior represents what we know about the parameters before the data are observed.\n\\(P(\\theta|Y)\\) - the posterior. The posterior represents our updated knowledge about parameters after the data are observed."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#recall-bayes-rule",
    "href": "1b-InferringBinomialProp.html#recall-bayes-rule",
    "title": "Bayesian Data Analysis",
    "section": "Recall: Bayes‚Äô rule",
    "text": "Recall: Bayes‚Äô rule\nGiven a set of observed data points \\(y\\) and a set of parameters \\(\\theta\\), we write Bayes‚Äô rule as\n\\[\\underset{\\text{posterior}}{P(\\theta|y)} = \\frac{\\underset{\\text{likelihood}}{P(y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}}{\\underset{\\text{marginal likelihood}}{P(y)}}\\] and as a proportional statement\n\\[\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\]\nWe will now consider an example that will build some intuition for how prior distributions and data interact to produce posterior distributions."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#what-proportion-of-earths-surface-is-covered-with-water",
    "href": "1b-InferringBinomialProp.html#what-proportion-of-earths-surface-is-covered-with-water",
    "title": "Bayesian Data Analysis",
    "section": "What proportion of Earth‚Äôs surface is covered with water?",
    "text": "What proportion of Earth‚Äôs surface is covered with water?"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth",
    "href": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth",
    "title": "Bayesian Data Analysis",
    "section": "Estimating the Proportion of Water on Earth",
    "text": "Estimating the Proportion of Water on Earth\nImagine you want to estimate how much of the Earth‚Äôs surface is covered in water. üåçüíß\n\nüü¢ The Experiment: You throw a blow-up globe into the air, and wherever your index finger lands, you record an observation of either:\n\nWater üåä\nLand üåç\n\nüîÑ Repeat this process multiple times, collecting a dataset of binary outcomes (water or land).\nüìä Your Data: As you accumulate observations, you‚Äôll begin to estimate the proportion of the Earth that‚Äôs covered by water based on how often your finger lands on water.\n\nThis simple, hands-on experiment can give you an idea of the Earth‚Äôs water coverage!"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth-1",
    "href": "1b-InferringBinomialProp.html#estimating-the-proportion-of-water-on-earth-1",
    "title": "Bayesian Data Analysis",
    "section": "Estimating the Proportion of Water on Earth",
    "text": "Estimating the Proportion of Water on Earth"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#water-on-the-globe-example",
    "href": "1b-InferringBinomialProp.html#water-on-the-globe-example",
    "title": "Bayesian Data Analysis",
    "section": "Water on the Globe Example",
    "text": "Water on the Globe Example\nüéØ What is our goal? Estimate the proportion of water on the globe, denoted as Œ∏ (theta).\nüìä What data do we have?\n\nData Collected: L, W, L, L, W, W, W, L, W, W\nTotal Throws: n = 10\nWater Observations: y = 6\n\nüîç How do we perform Bayesian inference for Œ∏?\n\nModel the Data: Choose a descriptive model for the data, known as the likelihood, which includes Œ∏ (the proportion of water).\nPrior Information: Summarize existing knowledge about Œ∏ using a prior probability distribution.\nUpdate with Data: Combine the prior with the collected data using Bayes‚Äô rule to obtain the posterior distribution for Œ∏. This refines our estimate of the proportion of water on the globe based on the evidence we‚Äôve gathered!"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#brief-recap-types-of-statistical-distributions",
    "href": "1b-InferringBinomialProp.html#brief-recap-types-of-statistical-distributions",
    "title": "Bayesian Data Analysis",
    "section": "Brief Recap: Types of Statistical Distributions",
    "text": "Brief Recap: Types of Statistical Distributions\nDifferent types of distributions are used to describe data (or parameters), here‚Äôs some examples:\n\n\nNormal Distribution\n\nDescription: Bell-shaped curve, symmetric around the mean.\nExample: Heights of people, IQ scores.\n\n\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\nDescription: Discrete distribution of the # of successes in a fixed # of trials.\nExample: Presence/absence of a disease\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Distribution\n\nDescription: Discrete distribution for the # of events in a fixed interval.\nExample: # of emails received in an hour."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#likelihood-function---pytheta",
    "href": "1b-InferringBinomialProp.html#likelihood-function---pytheta",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood Function - \\(p(y|\\theta)\\)",
    "text": "Likelihood Function - \\(p(y|\\theta)\\)\nWe have data: n = 10, y = 6. We now need a data model.\nWe‚Äôll model this particular type of data using a Binomial Distribution.\n\nAssumption: \\(y\\) follows a Binomial distribution with parameters \\((\\theta, n)\\), where:\n\\[p(y|\\theta) = c\\theta^y(1-\\theta)^{n-y} \\text{ with } c = {n \\choose y}\\]\nExplanation:\n\n\\(p(y|\\theta)\\): The likelihood function shows the probability of observing y water outcomes out of n throws, given the proportion of water Œ∏.\n\\(c\\): The combinatorial factor that accounts for the number of ways to choose y successes out of n trials.\n\n\nWhy This Matters: This function tells us how our observed data (water outcomes) relate to the parameter we want to estimate (Œ∏)."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#prior-distribution---ptheta",
    "href": "1b-InferringBinomialProp.html#prior-distribution---ptheta",
    "title": "Bayesian Data Analysis",
    "section": "Prior distribution - \\(p(\\theta)\\)",
    "text": "Prior distribution - \\(p(\\theta)\\)\nNow that we‚Äôve defined the data model, the next step is to establish a prior distribution over the parameter values.\n\nLet‚Äôs start simple and assume \\(\\theta\\) can only take on values k = \\(0,0.25,0.5,0.75,1\\).\nSuppose that we believe that \\(\\theta\\) is most likely to be 0.5 and we assign lower weight to \\(\\theta\\) values far above or below 0.5.\nA prior distribution incorporating these beliefs might look like:"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#likelihood-prior",
    "href": "1b-InferringBinomialProp.html#likelihood-prior",
    "title": "Bayesian Data Analysis",
    "section": "Likelihood & Prior",
    "text": "Likelihood & Prior\nGiven that y = 6 and n = 10 with \\(\\frac{y}{n} = 0.6\\), which \\(\\theta\\) out of \\(0,0.25,0.5,0.75,1\\) do you expect to have the largest value of the likelihood function?"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#posterior-distribution---undersettextposteriorpthetay-propto-undersettextlikelihoodpythetaundersettextpriorptheta",
    "href": "1b-InferringBinomialProp.html#posterior-distribution---undersettextposteriorpthetay-propto-undersettextlikelihoodpythetaundersettextpriorptheta",
    "title": "Bayesian Data Analysis",
    "section": "Posterior distribution - \\(\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\)",
    "text": "Posterior distribution - \\(\\underset{\\text{posterior}}{P(\\theta|Y)} \\propto \\underset{\\text{likelihood}}{P(Y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}\\)"
  },
  {
    "objectID": "1b-InferringBinomialProp.html#changing-prior-assumptions-1",
    "href": "1b-InferringBinomialProp.html#changing-prior-assumptions-1",
    "title": "Bayesian Data Analysis",
    "section": "Changing prior assumptions (1)",
    "text": "Changing prior assumptions (1)\nInstead of the ‚Äútriangular‚Äù prior let‚Äôs make a different assumption where we assume 0.75 is most likely and 0.5 is somewhat likely."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#changing-prior-assumptions-2",
    "href": "1b-InferringBinomialProp.html#changing-prior-assumptions-2",
    "title": "Bayesian Data Analysis",
    "section": "Changing prior assumptions (2)",
    "text": "Changing prior assumptions (2)\nInstead of the ‚Äútriangular‚Äù prior let‚Äôs make a more uniform assumption. So for \\(k = 0,0.25,0.5,0.75,1\\), \\(Pr(\\theta = k) = 1/5\\) (i.e., all are equally likely)."
  },
  {
    "objectID": "1b-InferringBinomialProp.html#marginal-likelihood---py",
    "href": "1b-InferringBinomialProp.html#marginal-likelihood---py",
    "title": "Bayesian Data Analysis",
    "section": "Marginal likelihood - \\(p(y)\\)",
    "text": "Marginal likelihood - \\(p(y)\\)\nRecall: \\[\\underset{\\text{posterior}}{P(\\theta|y)} = \\frac{\\underset{\\text{likelihood}}{P(y|\\theta)}\\underset{\\text{prior}}{P(\\theta)}}{\\underset{\\text{marginal likelihood}}{P(y)}}\\]\nWhat is \\(P(y)\\)?\n\\[P(y) = \\sum_{\\theta^*} P(y|\\theta^*)P(\\theta^*)\\]\nSo for \\(k = 0,0.25,0.5,0.75,1\\), \\(Pr(\\theta = k) = 1/5\\) (i.e., all are equally likely)\n\\(P(y) = p(y|\\theta = 0)Pr(\\theta = 0) + P(y|\\theta = 0.25)Pr(\\theta = 0.25) + \\ldots = 0.073\\)\nTo do this in R:\n\nn_grid = 5\ntheta &lt;- seq(0,1,length = n_grid) \np_y &lt;- (1/n_grid)*(sum(dbinom(6, 10, prob = theta)))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "",
    "text": "Welcome to the course website for ST405 (code share ST645) Bayesian Data Analysis!\nModule information\nLecture material (slides, notes, videos) are licensed under CC-BY-NC 4.0.\nContact: Niamh Cahill (niamh.cahill@mu.ie)"
  },
  {
    "objectID": "index.html#lecture-slides",
    "href": "index.html#lecture-slides",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Lecture Slides",
    "text": "Lecture Slides\n\nWeek 1\n1a: Bayes Rule\n1b: Inferring a Binomial Probability\n\n\nWeek 2\n2a: Beta Binomial\n2b: Bayesian Inference\n\n\nWeek 3\n3: Single Parameter Normal\n\n\nWeek 4\n4: MCMC Sampling\n\n\nWeek 5\n5a: MCMC Diagnostics\n5b: Just Another Gibbs Sampler\n\n\nWeek 6\n6: Bayesian Linear Regression\n\n\nWeek 7\n7: Model Checking\n\n\nWeek 8\n8: Introducing Bayesian Hierarchical Modelling\n\n\nWeek 9\n9: Bayesian Hierarchical Regression Modelling\n\n\nWeek 10\n10: Bayesian GLMs\n\n\nWeek 11\n11: Bayesian Hierarchical Modelling - GLM"
  },
  {
    "objectID": "2a-BetaBinomial.html#scenario",
    "href": "2a-BetaBinomial.html#scenario",
    "title": "Bayesian Data Analysis",
    "section": "Scenario:",
    "text": "Scenario:\nSuppose females, aged 65+ in a general social survey were asked about being happy.\nIf this is a representative sample of the population of women, what is the probability that a 65+ woman is happy?"
  },
  {
    "objectID": "2a-BetaBinomial.html#what-is-our-goal-what-data-do-we-have",
    "href": "2a-BetaBinomial.html#what-is-our-goal-what-data-do-we-have",
    "title": "Bayesian Data Analysis",
    "section": "What is Our Goal? What Data do We Have?",
    "text": "What is Our Goal? What Data do We Have?\nüéØ Goal:\nTo estimate the probability that a 65+ woman is happy.\n\nThis is an unknown parameter we‚Äôll call \\(\\theta\\).\n\\(\\theta\\) represents the probability of a 65+ woman being happy.\n\nüìä Data:\n\nSample Size (n): 20 women\nReported Happy (y): 14 women reported being happy"
  },
  {
    "objectID": "2a-BetaBinomial.html#bayesian-inference-for-theta",
    "href": "2a-BetaBinomial.html#bayesian-inference-for-theta",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Inference for \\(\\theta\\)",
    "text": "Bayesian Inference for \\(\\theta\\)\nüîç Steps in Bayesian Inference:\n\nLikelihood (Model the Data):\n\nWe define a model for the data, describing how the probability \\(\\theta\\) (probability a 65+ woman is happy) fits.\n\nPrior Information:\n\nBefore looking at data, we summarize information about \\(\\theta\\) in a prior distribution.\n\nUpdating with Data:\n\nBayes‚Äô Rule helps us update our prior with the data to obtain the posterior distribution, reflecting what we know about \\(\\theta\\) after considering the data."
  },
  {
    "objectID": "2a-BetaBinomial.html#the-bayesian-formula",
    "href": "2a-BetaBinomial.html#the-bayesian-formula",
    "title": "Bayesian Data Analysis",
    "section": "The Bayesian Formula",
    "text": "The Bayesian Formula\n\\[\n\\text{Posterior}(\\theta | \\text{data}) \\propto \\text{Likelihood}(\\text{data} | \\theta) \\cdot \\text{Prior}(\\theta)\n\\]\nLater we can visualize the prior, likelihood, and posterior distributions to see how Bayesian updating works in practice, for this example.\nFirst let‚Äôs workout what the posterior is, based on assuming an appropriate likelihood and prior.\n‚ùì Questions:\n\nWhat likelihood would you choose here?\nWhat are some constraints we need to think about when choosing a prior for \\(\\theta\\)?"
  },
  {
    "objectID": "2a-BetaBinomial.html#the-happiness-example",
    "href": "2a-BetaBinomial.html#the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "The Happiness example",
    "text": "The Happiness example\nFor the Happiness example:\n\nData: n = 20 women, y = 14 women reported being happy\n\\(y \\sim Binomial(n = 20, \\theta)\\)\n\n\\[p(y|\\theta) = c\\theta^y(1-\\theta)^{n-y} \\text{ with } c = {n \\choose y}\\]\n\nWe want to find the posterior distribution for \\(\\theta\\)\n\nNow we will consider defining the prior, \\(p(\\theta)\\), with a known probability distribution, such that:\n\\[\\theta \\sim Beta(a,b)\\]"
  },
  {
    "objectID": "2a-BetaBinomial.html#the-beta-prior",
    "href": "2a-BetaBinomial.html#the-beta-prior",
    "title": "Bayesian Data Analysis",
    "section": "The Beta Prior",
    "text": "The Beta Prior\nA Beta distribution is defined on the interval [0,1] and has two parameters, \\(a\\) and \\(b\\). The density function is defined as:\n\\[p(\\theta|a,b) = \\frac{1}{B(a,b)}\\theta^{a-1}(1-\\theta)^{b-1}\\]\nwhere \\(B(a,b)\\) is a normalising constant that insures a valid probability density function.\nIf \\(\\theta \\sim Be(a,b)\\) then \\(E(\\theta) = \\frac{a}{a+b}\\) and \\(Var(\\theta) = \\frac{ab}{(a+b)^2(a+b+1)}\\)\nNote \\(B(a,b)\\) is not a function of \\(\\theta\\), so we can write\n\\[p(\\theta|a,b) \\propto \\theta^{a-1}(1-\\theta)^{b-1}\\]\nThis will become useful later."
  },
  {
    "objectID": "2a-BetaBinomial.html#the-beta-prior-with-a1-and-b1",
    "href": "2a-BetaBinomial.html#the-beta-prior-with-a1-and-b1",
    "title": "Bayesian Data Analysis",
    "section": "The Beta Prior with a=1 and b=1",
    "text": "The Beta Prior with a=1 and b=1\nLet‚Äôs use Bayes‚Äô theorem now to find the form of the posterior distribution for \\(\\theta\\) assuming \\(\\theta \\sim Beta(a=1,b=1)\\).\nThis means we‚Äôve assumed a prior mean and variance for \\(\\theta\\) of \\(\\frac{1}{2}\\) and \\(\\frac{1}{12}\\) respectively.\nSo the posterior is\n\\[\\underset{\\text{posterior}}{p(\\theta|y)} \\propto \\underset{\\text{likelihood}}{\\theta^y(1-\\theta)^{n-y} }\\underset{\\text{prior}}{\\theta^{a-1}(1-\\theta)^{b-1}}\\]\nand given a = 1 and b = 1\n\\[\\underset{\\text{posterior}}{p(\\theta|y)} \\propto {\\theta^y(1-\\theta)^{n-y}}\\]\nThis posterior actually takes the form of another Beta distribution with parameters \\(y+1\\) and \\(n-y+1\\). So, \\[\\theta|y \\sim Beta(y+1, n-y +1)\\]"
  },
  {
    "objectID": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example",
    "href": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "What does this look like for the Happiness example?",
    "text": "What does this look like for the Happiness example?\n\nVisualise the PriorVisualise the LikelihoodVisualise the Posteior\n\n\n\n\n# Beta parameters (prior)\na &lt;- 1\nb &lt;- 1\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nprior &lt;- dbeta(theta,a,b) # get the prior distribution\n\n# create a dataset\nptheta_dat &lt;- tibble::tibble(theta, prior)\n\n# plot prior\nggplot2::ggplot(ptheta_dat, aes(x = theta, y = prior)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Data\ny &lt;- 14\nn &lt;- 20\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nlikelihood &lt;- dbinom(y,n,prob = theta) # get the likelihood distribution\n\n# create a dataset\nltheta_dat &lt;- tibble::tibble(theta, likelihood)\n\n# plot likelihood\nggplot2::ggplot(ltheta_dat, aes(x = theta, y = likelihood)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Data\ny &lt;- 14\nn &lt;- 20\n\n# Beta parameters (posterior)\na_post &lt;- y+1\nb_post &lt;- n-y+1\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nposterior &lt;- dbeta(theta,a_post,b_post) # get the posterior distribution\n\n# create a dataset\nposttheta_dat &lt;- tibble::tibble(theta, posterior)\n\n# plot posterior\nggplot2::ggplot(posttheta_dat, aes(x = theta, y = posterior)) +\n  geom_line()"
  },
  {
    "objectID": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example-1",
    "href": "2a-BetaBinomial.html#what-does-this-look-like-for-the-happiness-example-1",
    "title": "Bayesian Data Analysis",
    "section": "What does this look like for the Happiness example?",
    "text": "What does this look like for the Happiness example?"
  },
  {
    "objectID": "2a-BetaBinomial.html#more-on-the-binomial-likelihood-and-the-beta-prior",
    "href": "2a-BetaBinomial.html#more-on-the-binomial-likelihood-and-the-beta-prior",
    "title": "Bayesian Data Analysis",
    "section": "More on the Binomial Likelihood and the Beta Prior",
    "text": "More on the Binomial Likelihood and the Beta Prior\nIt turns out anytime you use a Binomial likelihood and a Beta prior, such that:\n\\[\\theta \\sim Be(a,b)\\]\n\\[y \\sim Binomal(n,\\theta)\\]\nthen you get a posterior distribution which is also Beta, where\n\\[\\theta|y \\sim Beta(y+a, n-y +b)\\]\nWhen the posterior is the same form as the prior, the prior is said to be a conjugate prior. The Beta prior is a conjugate prior for the Binomial likelihood."
  },
  {
    "objectID": "2a-BetaBinomial.html#expressing-prior-knowledge",
    "href": "2a-BetaBinomial.html#expressing-prior-knowledge",
    "title": "Bayesian Data Analysis",
    "section": "Expressing prior knowledge",
    "text": "Expressing prior knowledge\nSuppose for the Happiness example, you want to express your underlying belief about \\(\\theta\\) - the probability a woman age 65+ is happy.\n\nYour beliefs may be based on previous studies or perhaps expert opinion.\nSo for example, suppose you want your prior to reflect beliefs that the proportion is 0.6 \\(\\pm\\) 0.1.\nHow do we express this belief in the Beta distribution?\n\n\n\nüõ†Ô∏è Moment Matching: Our Tool to Match Beliefs to a Distribution\n\nWe use a technique called moment matching to convert these beliefs\n(mean = 0.6, sd = 0.1) into the parameters of a Beta distribution."
  },
  {
    "objectID": "2a-BetaBinomial.html#moment-matching",
    "href": "2a-BetaBinomial.html#moment-matching",
    "title": "Bayesian Data Analysis",
    "section": "Moment Matching",
    "text": "Moment Matching\nRecall if \\(\\theta \\sim Be(a,b)\\) then \\(E(\\theta) = \\frac{a}{a+b}\\) and \\(Var(\\theta) = \\frac{ab}{(a+b)^2(a+b+1)}\\)\nBased on our prior beliefs we want:\n\\(E(\\theta) = \\frac{a}{a+b} = 0.6\\)\n\\(Var(\\theta) = \\frac{ab}{(a+b)^2(a+b+1)} = 0.1^2\\)\nWe can use these equations to solve for \\(a\\) and \\(b\\), the parameters of the Beta prior.\n\nVisualise the Prior\n\n\n\n\n# Beta parameters (prior)\na &lt;- 23\nb &lt;- 16\n\nn_grid = 1000 # grid size \ntheta &lt;- seq(0,1,length = n_grid) # grid of theta values\nprior &lt;- dbeta(theta,a,b) # get the prior distribution\n\n# create a dataset\nptheta_dat &lt;- tibble::tibble(theta, prior)\n\n# plot prior\nggplot2::ggplot(ptheta_dat, aes(x = theta, y = prior)) +\n  geom_line()"
  },
  {
    "objectID": "2a-BetaBinomial.html#how-does-this-prior-impact-the-posterior",
    "href": "2a-BetaBinomial.html#how-does-this-prior-impact-the-posterior",
    "title": "Bayesian Data Analysis",
    "section": "How does this prior impact the posterior?",
    "text": "How does this prior impact the posterior?"
  },
  {
    "objectID": "2a-BetaBinomial.html#the-posterior-is-a-compromise-of-prior-and-likelihood",
    "href": "2a-BetaBinomial.html#the-posterior-is-a-compromise-of-prior-and-likelihood",
    "title": "Bayesian Data Analysis",
    "section": "The posterior is a compromise of prior and likelihood",
    "text": "The posterior is a compromise of prior and likelihood\nThe posterior distribution is always a compromise between the prior distribution and the likelihood function.\n\nWe can illustrate this easily with the Beta-Binomial example.\nWe‚Äôve seen that for a \\(Be(a,b)\\) prior and a \\(Binomial(n,\\theta)\\) likelihood that the posterior will be of the form:\n\n\\[\\theta|y \\sim Beta(y+a, n-y +b)\\]\nand so the posterior mean is \\(E(\\theta|y) = \\frac{y+a}{n + a + b}\\).\n\nThis can be written as a weighted sum of the prior mean (\\(\\frac{a}{a+b}\\)) and the data proportion (\\(\\frac{y}{n}\\)), as follows: \\(E(\\theta|y) = \\underbrace{\\small\\frac{y}{n}}_{data}\\underbrace{\\small\\frac{n}{n+a+b}}_{weight} + \\underbrace{\\small\\frac{a}{a+b}}_{prior}\\underbrace{\\small\\frac{a+b}{n+a+b}}_{weight}\\)"
  },
  {
    "objectID": "2b-BayesianInference.html#bayesian-inference",
    "href": "2b-BayesianInference.html#bayesian-inference",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\n\nBayesian point estimates are often given by:\n\nthe posterior mean \\(E(\\theta|y)\\)\nor the posterior median \\(\\theta^*\\) with \\(P(\\theta &lt; \\theta^*|y) = 0.5\\)\n\nUncertainty is quantified with credible intervals (CIs)\n\nAn interval is a 95% credible interval if the posterior probability that \\(\\theta\\) is in the interval is 0.95.\nOften quantile based, given by posterior quantiles with \\(P(\\theta &lt; \\theta_{\\alpha/2}|y) = P(\\theta &gt; \\theta_{1-\\alpha/2}|y) = \\alpha/2\\)"
  },
  {
    "objectID": "2b-BayesianInference.html#bayesian-inference-for-the-happiness-example",
    "href": "2b-BayesianInference.html#bayesian-inference-for-the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Inference for the Happiness example",
    "text": "Bayesian Inference for the Happiness example\n\n\\(\\theta|y \\sim Beta(y+a,n-y+b)\\)\nPosterior mean \\(E(\\theta|y) = \\frac{y+a}{n+a+b}\\)\nFor quantile estimates we can use qbeta() in R\n\n\nR Code\n\n\n\n\n## data \nn = 20; y = 14\n\n## prior parameters\na = 1; b = 1\n\n## posterior parameters\na_post = y + a; b_post = n - y + b\n\n## posterior mean\np_mean &lt;- a_post/(a_post + b_post) # (y+a)/(n+a+b)\n\n## quantiles\np_quant &lt;- qbeta(c(0.025,0.5,0.975),a_post,b_post)\n\np_mean; p_quant\n\n\n[1] 0.6818182\n\n\n[1] 0.4782489 0.6874159 0.8541231"
  },
  {
    "objectID": "2b-BayesianInference.html#simulation-based-inference",
    "href": "2b-BayesianInference.html#simulation-based-inference",
    "title": "Bayesian Data Analysis",
    "section": "Simulation-based inference",
    "text": "Simulation-based inference\n\nThe general idea in simulation-based inference: We can make inference about a parameter \\(\\theta\\) , using a sample \\(\\{\\theta^{(1)}\\ldots\\theta^{(S)}\\}\\) from its probability distribution.\nAssessing the properties of a target (e.g., posterior) distribution by generating representative samples is called Monte Carlo simulation.\nBased on the law of large numbers we know that:\n\\(\\frac{1}{S}\\sum_{s=1}^{S}\\theta^{(s)} = E(\\theta)\\)\nas sample size \\(S \\to \\infty\\)\n\nThe error in the MC approximation goes to zero as \\(S \\to \\infty\\) because \\(\\frac{var(\\theta)}{S} \\to 0\\)\n\nJust about any aspect of the distribution of \\(\\theta\\) can be approximated arbitrarily exactly with a large enough Monte Carlo sample, e.g.\n\nthe \\(\\alpha\\)-percentile of the distribution of \\(\\theta\\)\n\\(Pr(\\theta \\geq x)\\) for any constant \\(x\\)"
  },
  {
    "objectID": "2b-BayesianInference.html#simulation-based-inference-for-the-happiness-example",
    "href": "2b-BayesianInference.html#simulation-based-inference-for-the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "Simulation-based inference for the Happiness example",
    "text": "Simulation-based inference for the Happiness example\nFor the Happiness example, we can approximate the mean and quantiles of \\(\\theta\\) using samples from a \\(Be(y+a,n-y+b)\\) distribution (i.e., the posterior)\n\nR Code\n\n\n\n\n## data \nn = 20; y = 14\n\n## prior parameters\na = 1; b = 1\n\n## posterior parameters\na_post = y + a; b_post = n - y + b\n\n## sample \nsamp_theta &lt;- rbeta(1000,a_post,b_post)\n\n## sample mean and quantiles\np_mean &lt;- mean(samp_theta)\np_quant &lt;- quantile(samp_theta, \n                    probs = c(0.025,0.5,0.975))\n\np_mean; p_quant\n\n\n[1] 0.6752533\n\n\n     2.5%       50%     97.5% \n0.4739717 0.6825371 0.8589662"
  },
  {
    "objectID": "2b-BayesianInference.html#monte-carlo-approximation-some-more-details",
    "href": "2b-BayesianInference.html#monte-carlo-approximation-some-more-details",
    "title": "Bayesian Data Analysis",
    "section": "Monte Carlo approximation: some more details",
    "text": "Monte Carlo approximation: some more details\n\nWith a simulation, it also becomes very easy to analyze the distributions of any function of your parameter,\n\ne.g.¬†the distribution of the odds \\(\\frac{\\theta}{1-\\theta}\\) by using samples from \\(\\frac{\\theta^{(s)}}{1-\\theta^{(s)}}\\)\n\n\n\nR Code\n\n\n\n\n## sample \nsamp_theta &lt;- rbeta(1000,a_post,b_post)\n\n## get odds based on samples\nsamp_odds &lt;- samp_theta/(1-samp_theta)\n\n## sample mean and quantiles\np_mean_odds &lt;- mean(samp_odds)\np_quant_odds &lt;- quantile(samp_odds, \n                         probs = c(0.025,0.5,0.975))\n\np_mean_odds; p_quant_odds\n\n\n[1] 2.525918\n\n\n     2.5%       50%     97.5% \n0.9277052 2.2027345 5.6566618"
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution",
    "href": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\mu\\) from a Normal Distribution",
    "text": "Estimating \\(\\mu\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\mu\\) - example in blue with \\(\\mu \\sim N(4,0.5^2)\\)\nCollect data, \\(y\\), assume y has a mean \\(\\bar{y} = 0\\) and that n = 10.\nDefine the relationship between \\(y\\) and \\(\\mu\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu, \\sigma = 1)\\).\n\nwe‚Äôre assuming \\(\\sigma\\) is known here.\n\nUse Bayes‚Äô rule to update the prior into the posterior distribution \\(p(\\mu|y) \\sim N(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-1",
    "href": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-1",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\mu\\) from a Normal Distribution",
    "text": "Estimating \\(\\mu\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\mu\\) - example in blue with \\(\\mu \\sim N(4,0.5^2)\\)\nCollect data, \\(y\\), assume y has a mean \\(\\bar{y} = 0\\) and that n = 10.\n\nDefine the relationship between \\(y\\) and \\(\\mu\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu, \\sigma = 2)\\).\n\nwe‚Äôre assuming \\(\\sigma\\) is known here.\n\nUse Bayes‚Äô rule to update the prior into the posterior distribution \\(p(\\mu|y) \\sim N(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-2",
    "href": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-2",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\mu\\) from a Normal Distribution",
    "text": "Estimating \\(\\mu\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\mu\\) - example in blue with \\(\\mu \\sim N(4,0.5^2)\\)\nCollect data, \\(y\\), assume y has a mean \\(\\bar{y} = 0\\) and that n = 500.\nDefine the relationship between \\(y\\) and \\(\\mu\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu, \\sigma = 1)\\).\n\nwe‚Äôre assuming \\(\\sigma\\) is known here.\n\nUse Bayes‚Äô rule to update the prior into the posterior distribution \\(p(\\mu|y) \\sim N(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-3",
    "href": "3a-SingleParameterNormal.html#estimating-mu-from-a-normal-distribution-3",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\mu\\) from a Normal Distribution",
    "text": "Estimating \\(\\mu\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\mu\\) - example in blue with \\(\\mu \\sim N(4,0.5^2)\\)\nCollect data, \\(y\\), assume y has a mean \\(\\bar{y} = 0\\) and that n = 500.\nDefine the relationship between \\(y\\) and \\(\\mu\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu, \\sigma = 2)\\).\n\nwe‚Äôre assuming \\(\\sigma\\) is known here.\n\nUse Bayes‚Äô rule to update the prior into the posterior distribution \\(p(\\mu|y) \\sim N(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-sigma-text-tau-1sigma2-from-a-normal-distribution",
    "href": "3a-SingleParameterNormal.html#estimating-sigma-text-tau-1sigma2-from-a-normal-distribution",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\sigma \\text{ } (\\tau = 1/\\sigma^2)\\) from a Normal Distribution",
    "text": "Estimating \\(\\sigma \\text{ } (\\tau = 1/\\sigma^2)\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\tau = 1/\\sigma^2\\) - example in blue with \\(\\tau = 1/\\sigma^2 \\sim gamma(2,1)\\)\nCollect data, \\(y\\), assume y has a standard deviation \\(s = 2, 1/s^2 = 0.5\\) and that n = 10.\n\nDefine the relationship between \\(y\\) and \\(\\tau\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu = 0, \\sigma^2 = 1/\\tau)\\).\n\nwe‚Äôre assuming \\(\\mu\\) is known here.\n\nUse Bayes‚Äô rule to update the prior into the posterior distribution \\(p(\\tau|y) \\sim gamma(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#estimating-sigma-text-tau-1sigma2-from-a-normal-distribution-1",
    "href": "3a-SingleParameterNormal.html#estimating-sigma-text-tau-1sigma2-from-a-normal-distribution-1",
    "title": "Bayesian Data Analysis",
    "section": "Estimating \\(\\sigma \\text{ } (\\tau = 1/\\sigma^2)\\) from a Normal Distribution",
    "text": "Estimating \\(\\sigma \\text{ } (\\tau = 1/\\sigma^2)\\) from a Normal Distribution\n\n\nWe do Bayesian inference:\n\nStart off with prior probability distribution to quantify information related to \\(\\tau = 1/\\sigma^2\\) - example in blue with \\(\\tau = 1/\\sigma^2 \\sim gamma(8,4)\\)\nCollect data, \\(y\\), assume y has a standard deviation \\(s = 2, 1/s^2 = 0.5\\) and that n = 10.\n\nDefine the relationship between \\(y\\) and \\(\\tau\\) through the likelihood function - example in red with \\(y_i \\sim N(\\mu = 0, \\sigma^2 = 1/\\tau)\\).\n\nwe‚Äôre assuming \\(\\mu\\) is known here.\n\nUse Bayes‚Äô rule to update the prior into the posterior distribution \\(p(\\tau|y) \\sim gamma(?,?)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#example-cognitive-test-scores",
    "href": "3a-SingleParameterNormal.html#example-cognitive-test-scores",
    "title": "Bayesian Data Analysis",
    "section": "Example: Cognitive Test Scores",
    "text": "Example: Cognitive Test Scores\nData (y) are available on the cognitive test scores of three- and four-year-old children in the USA. The sample contains \\(n=434\\) observations."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#normal-distribution-with-known-variance",
    "href": "3a-SingleParameterNormal.html#normal-distribution-with-known-variance",
    "title": "Bayesian Data Analysis",
    "section": "Normal distribution with known variance",
    "text": "Normal distribution with known variance\nWe will assume a normal model for the data where \\(y_i|\\mu,\\sigma^2 \\sim N(\\mu, \\sigma^2)\\). Assume \\(\\sigma^2\\) is known.\n\nSpecify the likelihood for \\(\\mu\\)\n\n\\(p(y|\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt {2\\pi\\sigma^2}}exp \\bigg(-\\frac{1}{2\\sigma^2}(y_i - \\mu)^2\\bigg)\\)\n\nSpecify a prior for \\(\\mu\\)\n\n\\(\\mu \\sim N(\\mu_0, \\sigma^2_{0})\\)\n\nUse Bayes‚Äô rule to obtain the posterior distribution\n\n\\(p(\\mu|y) \\propto p(y|\\mu)p(\\mu)\\)"
  },
  {
    "objectID": "3a-SingleParameterNormal.html#normal-distribution-with-known-variance-1",
    "href": "3a-SingleParameterNormal.html#normal-distribution-with-known-variance-1",
    "title": "Bayesian Data Analysis",
    "section": "Normal distribution with known variance",
    "text": "Normal distribution with known variance\n\nAs it turns out, the posterior is also a normal distribution\n\n\\[\\mu|y \\sim N \\bigg(\\frac{n\\bar{y}/\\sigma^2 + \\mu_0/\\sigma^2_{0}}{n/\\sigma^2 + 1/\\sigma^2_{0}}, {\\frac{1}{n/\\sigma^2 + 1/\\sigma^2_{0}}}\\bigg)\\]\n\nFor the Kid IQ example, assuming \\(\\mu \\sim N(\\mu_0 = 80, \\sigma_0 = 10)\\), then \\(\\mu|y \\sim N(86.7, 0.97)\\)"
  },
  {
    "objectID": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean",
    "href": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean",
    "title": "Bayesian Data Analysis",
    "section": "Normal distribution with known mean",
    "text": "Normal distribution with known mean\nWe will assume a normal model for the data where \\(y_i|\\mu,\\sigma^2 \\sim N(\\mu, \\sigma^2)\\). Assume \\(\\mu\\) is known.\n\nUsually work with precision i.e., \\(\\tau = 1/\\sigma^2\\)\nSpecify a prior for \\(\\tau\\)\n\nPopular prior for the precision of a normal distribution is a gamma prior e.g., \\(\\tau \\sim Gamma(a, b)\\) where \\(E[\\tau] = \\frac{a}{b}\\) and \\(Var[\\tau] = \\frac{a}{b^2}\\)\n\\(p(\\tau|a,b) = \\frac{b^a}{\\Gamma(a)}\\tau^{a-1}e^{-b\\tau}\\) for \\(\\tau &gt;0\\) and \\(a,b &gt; 0\\)\n\nUse Bayes‚Äô rule to obtain the posterior distribution\n\n\\(p(\\tau|y) \\propto p(y|\\tau)p(\\tau)\\)"
  },
  {
    "objectID": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean-1",
    "href": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean-1",
    "title": "Bayesian Data Analysis",
    "section": "Normal distribution with known mean",
    "text": "Normal distribution with known mean\n\nAs it turns out, the posterior is also a gamma distribution\n\n\\(\\tau|y \\sim Gamma \\bigg(a + n/2, b + 1/2\\sum_{i=1}^n (y_i - \\mu)^2\\bigg)\\)\n\nFor the Kid IQ example, assuming \\(\\tau \\sim gamma(a = 1, b = 1)\\), then \\(\\tau|y \\sim gamma(218, 90203)\\)."
  },
  {
    "objectID": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean-2",
    "href": "3a-SingleParameterNormal.html#normal-distribution-with-known-mean-2",
    "title": "Bayesian Data Analysis",
    "section": "Normal distribution with known mean",
    "text": "Normal distribution with known mean\n\nConverting back from the precision to the standard deviation, the posterior for \\(\\sigma\\) for this example will look more like:"
  },
  {
    "objectID": "Tutorial1.html",
    "href": "Tutorial1.html",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "This tutorial explores Bayesian inference using binomial and Poisson models, focusing on deriving posterior distributions, credible intervals, and the impact of prior assumptions. You will also perform simulations and compare posterior results with analytical calculations.\n\n\n\nConsider the maternal condition known as placenta previa, an uncommon condition where the placenta obstructs the fetus, leading to possible complications during delivery.\nAn early study in Germany collected data on placenta previa births, finding that out of 980 births, 437 were female.\n\n\n\nPosterior Distribution\nAssuming a Binomial likelihood for the data and a Beta(1,1) prior distribution for the probability of a female birth, derive the posterior distribution.\nPosterior Mean\nCalculate the posterior mean for the probability of a girl birth in placenta previa pregnancies.\n95% Credible Interval\nCompute the 95% credible interval for the probability of a girl birth in placenta previa pregnancies.\n\n\n\n\n\n\nIn many cases, direct calculation of the posterior density function may not be feasible. Simulation offers a practical solution for obtaining inference from the posterior distribution.\n\n\n\nSimulating 1000 Samples\nSimulate 1000 samples from the posterior distribution derived in Question 1.\nHistogram\nCreate a histogram of the simulated samples.\n95% Credible Interval from Simulated Data\nCalculate the 95% credible interval based on the simulated samples.\nComparison with Analytical Interval\nCompare the credible interval obtained from the simulation to the one calculated analytically in Question 1.\n\n\n\n\n\n\nNow, assume we change the prior distribution. Suppose the prior distribution is Beta(6,20) instead of Beta(1,1).\n\n\n\nNew Posterior Distribution\nDerive the new posterior distribution given the updated Beta(6,20) prior.\nPosterior Mean and Credible Interval\nCompute the new posterior mean and 95% credible interval for the probability of a girl birth in placenta previa pregnancies.\n\n\n\n\n\n\nAssume you are modeling data \\(y_{i},\\; i=1,\\ldots,n\\) that arise from a Poisson distribution with rate parameter \\(\\lambda\\). (Note: Poisson distributions are often used for count data.) Use Bayesian methods to estimate \\(\\lambda\\), with a Gamma(\\(a\\), \\(b\\)) distribution as the prior for \\(\\lambda\\).\n\n\n\nLikelihood Function\nWrite down the likelihood for the Poisson model (up to the constant of proportionality).\nAppropriateness of Gamma Prior\nWhy is the Gamma distribution an appropriate prior to use for the rate parameter \\(\\lambda\\) in this case?\nDetermining Parameters of the Gamma Prior\nFind the values of \\(a\\) and \\(b\\) for the Gamma prior that reflect the belief that \\(\\lambda\\) is \\(10 \\pm 1\\) (i.e., has mean = 10 and standard deviation = 1).\nPosterior Distribution\nDerive the posterior distribution for \\(\\lambda\\).\nPosterior Mean Estimate\nIf the observed data are \\(\\mathbf{y} = (17, 25, 25, 21, 13, 22, 23)\\), find the posterior mean estimate for \\(\\lambda\\).\nConjugacy of the Prior\nIs the Gamma prior a conjugate prior for the Poisson likelihood? Explain your answer.\nComparing Posterior, Prior, and Data Means\nComment on how the posterior mean estimate compares to the prior mean and the mean of the observed data. What insights do you gain from this comparison?\n\n\n\n\n\n\nThrough these exercises, you will explore how prior beliefs shape posterior distributions, utilize simulation techniques to derive credible intervals, and understand the impact of different priors on Bayesian inference. The additional question introduces a new distribution, expanding your skills in Bayesian data analysis using Poisson likelihoods and Gamma priors.\n\n\n\n\n\nYou can use the following information about the Binomial, Beta, Poisson and Gamma distributions to help you with the questions\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distribution\nPMF/PDF\nE(X)\nVar(X)\n\n\n\n\nBinomial\n\\[{n \\choose x}\\theta^x(1-\\theta)^{n-x},\\ x \\in \\{0,1,2,\\ldots, n\\}\\]\n\\[n\\theta\\]\n\\[n\\theta(1-\\theta)\\]\n\n\nBeta\n\\[\\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1},\\ x \\in (0,1)\\]\n\\[\\frac{a}{a+b}\\]\n\\[\\frac{ab}{(a+b)^2(a+b+1)}\\]\n\n\nPoisson\n\\[\\lambda^x \\frac{e^{-\\lambda}}{x!},\\ x = 0,1,\\ldots\\]\n\\[\\lambda\\]\n\\[\\lambda\\]\n\n\nGamma\n\\[\\frac{b^a}{\\Gamma(a)}x^{a-1}e^{-bx},\\ x\\geq 0\\]\n\\[\\frac{a}{b}\\]\n\\[\\frac{a}{b^2}\\]\n\n\n\n\n\n\nFor simulations, use rbeta() in R for Beta-distributed samples.\nFor plotting histograms and posterior distributions, use ggplot2 or base R (hist()) plotting functions."
  },
  {
    "objectID": "Tutorial1.html#introduction",
    "href": "Tutorial1.html#introduction",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "This tutorial explores Bayesian inference using binomial and Poisson models, focusing on deriving posterior distributions, credible intervals, and the impact of prior assumptions. You will also perform simulations and compare posterior results with analytical calculations.\n\n\n\nConsider the maternal condition known as placenta previa, an uncommon condition where the placenta obstructs the fetus, leading to possible complications during delivery.\nAn early study in Germany collected data on placenta previa births, finding that out of 980 births, 437 were female.\n\n\n\nPosterior Distribution\nAssuming a Binomial likelihood for the data and a Beta(1,1) prior distribution for the probability of a female birth, derive the posterior distribution.\nPosterior Mean\nCalculate the posterior mean for the probability of a girl birth in placenta previa pregnancies.\n95% Credible Interval\nCompute the 95% credible interval for the probability of a girl birth in placenta previa pregnancies.\n\n\n\n\n\n\nIn many cases, direct calculation of the posterior density function may not be feasible. Simulation offers a practical solution for obtaining inference from the posterior distribution.\n\n\n\nSimulating 1000 Samples\nSimulate 1000 samples from the posterior distribution derived in Question 1.\nHistogram\nCreate a histogram of the simulated samples.\n95% Credible Interval from Simulated Data\nCalculate the 95% credible interval based on the simulated samples.\nComparison with Analytical Interval\nCompare the credible interval obtained from the simulation to the one calculated analytically in Question 1.\n\n\n\n\n\n\nNow, assume we change the prior distribution. Suppose the prior distribution is Beta(6,20) instead of Beta(1,1).\n\n\n\nNew Posterior Distribution\nDerive the new posterior distribution given the updated Beta(6,20) prior.\nPosterior Mean and Credible Interval\nCompute the new posterior mean and 95% credible interval for the probability of a girl birth in placenta previa pregnancies.\n\n\n\n\n\n\nAssume you are modeling data \\(y_{i},\\; i=1,\\ldots,n\\) that arise from a Poisson distribution with rate parameter \\(\\lambda\\). (Note: Poisson distributions are often used for count data.) Use Bayesian methods to estimate \\(\\lambda\\), with a Gamma(\\(a\\), \\(b\\)) distribution as the prior for \\(\\lambda\\).\n\n\n\nLikelihood Function\nWrite down the likelihood for the Poisson model (up to the constant of proportionality).\nAppropriateness of Gamma Prior\nWhy is the Gamma distribution an appropriate prior to use for the rate parameter \\(\\lambda\\) in this case?\nDetermining Parameters of the Gamma Prior\nFind the values of \\(a\\) and \\(b\\) for the Gamma prior that reflect the belief that \\(\\lambda\\) is \\(10 \\pm 1\\) (i.e., has mean = 10 and standard deviation = 1).\nPosterior Distribution\nDerive the posterior distribution for \\(\\lambda\\).\nPosterior Mean Estimate\nIf the observed data are \\(\\mathbf{y} = (17, 25, 25, 21, 13, 22, 23)\\), find the posterior mean estimate for \\(\\lambda\\).\nConjugacy of the Prior\nIs the Gamma prior a conjugate prior for the Poisson likelihood? Explain your answer.\nComparing Posterior, Prior, and Data Means\nComment on how the posterior mean estimate compares to the prior mean and the mean of the observed data. What insights do you gain from this comparison?\n\n\n\n\n\n\nThrough these exercises, you will explore how prior beliefs shape posterior distributions, utilize simulation techniques to derive credible intervals, and understand the impact of different priors on Bayesian inference. The additional question introduces a new distribution, expanding your skills in Bayesian data analysis using Poisson likelihoods and Gamma priors.\n\n\n\n\n\nYou can use the following information about the Binomial, Beta, Poisson and Gamma distributions to help you with the questions\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distribution\nPMF/PDF\nE(X)\nVar(X)\n\n\n\n\nBinomial\n\\[{n \\choose x}\\theta^x(1-\\theta)^{n-x},\\ x \\in \\{0,1,2,\\ldots, n\\}\\]\n\\[n\\theta\\]\n\\[n\\theta(1-\\theta)\\]\n\n\nBeta\n\\[\\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1},\\ x \\in (0,1)\\]\n\\[\\frac{a}{a+b}\\]\n\\[\\frac{ab}{(a+b)^2(a+b+1)}\\]\n\n\nPoisson\n\\[\\lambda^x \\frac{e^{-\\lambda}}{x!},\\ x = 0,1,\\ldots\\]\n\\[\\lambda\\]\n\\[\\lambda\\]\n\n\nGamma\n\\[\\frac{b^a}{\\Gamma(a)}x^{a-1}e^{-bx},\\ x\\geq 0\\]\n\\[\\frac{a}{b}\\]\n\\[\\frac{a}{b^2}\\]\n\n\n\n\n\n\nFor simulations, use rbeta() in R for Beta-distributed samples.\nFor plotting histograms and posterior distributions, use ggplot2 or base R (hist()) plotting functions."
  },
  {
    "objectID": "Tutorial1_soln.html",
    "href": "Tutorial1_soln.html",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "Given:\n- The number of placenta previa births: \\(n = 980\\)\n- The number of female births: \\(x = 437\\)\n- Prior distribution: \\(\\theta \\sim \\text{Beta}(1,1)\\) (which is equivalent to a uniform prior).\n\n\n\nPosterior Distribution\nThe binomial likelihood for the number of female births \\(x\\) out of \\(n\\) total births is:\n\\[p(\\theta | y) = {n \\choose y} \\theta^x (1 - \\theta)^{n - y}\\]\nThe prior distribution is \\(\\theta \\sim \\text{Beta}(1,1)\\). The Beta-Binomial conjugacy allows us to combine the likelihood and the prior to get the posterior distribution:\n\\[\\theta | y \\sim \\text{Beta}(437 + 1, 980 - 437 + 1) = \\text{Beta}(438, 544)\\]\nPosterior Mean\nThe mean of a Beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\) is given by:\n\\[E(\\theta | x) = \\frac{\\alpha}{\\alpha + \\beta}\\]\nSubstituting \\(\\alpha = 438\\) and \\(\\beta = 544\\):\n\\[E(\\theta | x) = \\frac{438}{438 + 544} = \\frac{438}{982} \\approx 0.446\\]\n95% Credible Interval\nThe 95% credible interval for \\(\\theta\\) can be obtained using the quantiles of the Beta distribution. In R, you can use:\nqbeta(c(0.025, 0.975), 438, 544)\nThis gives the interval \\([0.415, 0.478]\\), meaning that with 95% probability, the true value of \\(\\theta\\) lies between 0.415 and 0.478.\n\n\n\n\n\n\n\n\n\nSimulating 1000 Samples\nTo simulate 1000 samples from the posterior \\(\\text{Beta}(438, 544)\\) distribution, you can use the rbeta() function in R:\nsamples &lt;- rbeta(1000, 438, 544)\nHistogram\nYou can create a histogram of the samples to visualize the posterior distribution:\nhist(samples, main = \"Posterior Distribution\", xlab = \"Theta\", col = \"lightblue\", border = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval from Simulated Data\nThe 95% credible interval based on the samples can be calculated as:\nquantile(samples, c(0.025, 0.975))\nThis should give an interval close to \\([0.415, 0.478]\\) obtained from Q1.\nComparison with Analytical Interval\nThe credible interval from the simulation should closely match the analytical interval obtained in Question 1. This shows that the simulation provides a good approximation of the posterior distribution.\n\n\n\n\n\n\nNow, assume the prior distribution is \\(\\theta \\sim \\text{Beta}(6,20)\\).\n\n\n\nNew Posterior Distribution\nThe prior is \\(\\text{Beta}(6,20)\\), and the likelihood is still binomial. Using the conjugacy property of the Beta distribution, the posterior becomes:\n\\[\\theta | x \\sim \\text{Beta}(437 + 6, 980 - 437 + 20) = \\text{Beta}(443, 563)\\]\nPosterior Mean and Credible Interval\nThe posterior mean is:\n\\[E(\\theta | x) = \\frac{443}{443 + 563} = \\frac{443}{1006} \\approx 0.440\\]\nThe 95% credible interval can be obtained similarly using R:\nqbeta(c(0.025, 0.975), 443, 563)\nThis gives the interval \\([0.408, 0.473]\\).\nComparing this to the result from the prior \\(\\text{Beta}(1,1)\\), we observe that using the Beta(6,20) prior shifts the posterior slightly, reflecting the stronger prior information.\n\n\n\n\n\n\nNote: Students have not seen an example of using these distributions in class.\n\n\n\nLikelihood Function\nThe likelihood for a Poisson model is:\n\\[p(\\lambda | \\mathbf{y}) = \\prod_{i=1}^n \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\\] Ignoring constants, the likelihood is proportional to:\n\\[p(\\lambda | \\mathbf{y}) \\propto \\lambda^{\\sum y_i} e^{-n \\lambda}\\]\nAppropriateness of Gamma Prior\nThe Gamma distribution is a conjugate prior for the Poisson likelihood because the posterior distribution remains in the Gamma family. This simplifies computation and provides an intuitive interpretation of the prior parameters.\nFinding \\(a\\) and \\(b\\) for Prior\nThe mean and standard deviation of the Gamma distribution are given by:\n\\[\\mu = \\frac{\\alpha}{\\beta}, \\quad \\sigma^2 = \\frac{\\alpha}{\\beta^2}\\] Given that \\(\\mu = 10\\) and \\(\\sigma = 1\\), solve for \\(\\alpha\\) and \\(\\beta\\):\n\\[\\alpha = \\frac{10^2}{1^2} = 100, \\quad \\beta = \\frac{10}{1^2} = 10\\] Hence, \\(\\lambda \\sim \\text{Gamma}(100,10)\\).\nPosterior Distribution\n\n\\(p(\\lambda|y) \\propto p(y|\\lambda)p(\\lambda)\\)\nThe prior: \\(p(\\lambda) \\propto \\lambda^{a-1}e^{-b\\lambda}\\)\n(note: the \\(\\frac{b^a}{\\Gamma(a)}\\) can be removed under the proportionality)\nThe posterior: \\(p(\\lambda|y) \\propto \\lambda^{a-1}e^{-b\\lambda}\\lambda^{\\sum{y_i}}e^{-n\\lambda} = \\lambda^{\\boxed{a + \\sum{y_i}}-1}e^{-\\boxed{(b+n)}\\lambda}\\)\n(note: this has the same form as a gamma distribution with \\(a_{new} = a + \\sum{y_i}\\) and \\(b_{new} = b+n\\))\nTherefore \\(\\lambda|y \\sim gamma(a + \\sum{y_i},b+n)\\)\nWith the data \\(\\mathbf{y} = (17, 25, 25, 21, 13, 22, 23)\\), we have \\(\\sum y_i = 146\\), \\(n = 7\\). So, the posterior is \\(\\lambda | \\mathbf{y} \\sim \\text{Gamma}(246, 17)\\)\n\nPosterior Mean Estimate\nThe posterior mean is:\n\\[E(\\lambda | \\mathbf{y}) = \\frac{246}{17} \\approx 14.47\\]\nConjugacy of the Prior\nYes, the Gamma distribution is a conjugate prior for the Poisson likelihood, as the posterior distribution remains a Gamma distribution.\nComparison of Posterior, Prior, and Data Means\n\nThe prior mean was 10.\nThe posterior mean is \\(14.47\\).\nThe sample mean of the observed data \\(\\mathbf{y}\\) is:\n\\[\\bar{y} = \\frac{146}{7} \\approx 20.86\\]\n\nThe posterior mean is a compromise between the prior mean (10) and the sample mean (20.86), indicating that the data has moved the posterior away from the prior, but not completely to the observed sample mean. This reflects the influence of both the prior belief and the observed data."
  },
  {
    "objectID": "Tutorial1_soln.html#solutions",
    "href": "Tutorial1_soln.html#solutions",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "Given:\n- The number of placenta previa births: \\(n = 980\\)\n- The number of female births: \\(x = 437\\)\n- Prior distribution: \\(\\theta \\sim \\text{Beta}(1,1)\\) (which is equivalent to a uniform prior).\n\n\n\nPosterior Distribution\nThe binomial likelihood for the number of female births \\(x\\) out of \\(n\\) total births is:\n\\[p(\\theta | y) = {n \\choose y} \\theta^x (1 - \\theta)^{n - y}\\]\nThe prior distribution is \\(\\theta \\sim \\text{Beta}(1,1)\\). The Beta-Binomial conjugacy allows us to combine the likelihood and the prior to get the posterior distribution:\n\\[\\theta | y \\sim \\text{Beta}(437 + 1, 980 - 437 + 1) = \\text{Beta}(438, 544)\\]\nPosterior Mean\nThe mean of a Beta distribution \\(\\text{Beta}(\\alpha, \\beta)\\) is given by:\n\\[E(\\theta | x) = \\frac{\\alpha}{\\alpha + \\beta}\\]\nSubstituting \\(\\alpha = 438\\) and \\(\\beta = 544\\):\n\\[E(\\theta | x) = \\frac{438}{438 + 544} = \\frac{438}{982} \\approx 0.446\\]\n95% Credible Interval\nThe 95% credible interval for \\(\\theta\\) can be obtained using the quantiles of the Beta distribution. In R, you can use:\nqbeta(c(0.025, 0.975), 438, 544)\nThis gives the interval \\([0.415, 0.478]\\), meaning that with 95% probability, the true value of \\(\\theta\\) lies between 0.415 and 0.478.\n\n\n\n\n\n\n\n\n\nSimulating 1000 Samples\nTo simulate 1000 samples from the posterior \\(\\text{Beta}(438, 544)\\) distribution, you can use the rbeta() function in R:\nsamples &lt;- rbeta(1000, 438, 544)\nHistogram\nYou can create a histogram of the samples to visualize the posterior distribution:\nhist(samples, main = \"Posterior Distribution\", xlab = \"Theta\", col = \"lightblue\", border = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n95% Credible Interval from Simulated Data\nThe 95% credible interval based on the samples can be calculated as:\nquantile(samples, c(0.025, 0.975))\nThis should give an interval close to \\([0.415, 0.478]\\) obtained from Q1.\nComparison with Analytical Interval\nThe credible interval from the simulation should closely match the analytical interval obtained in Question 1. This shows that the simulation provides a good approximation of the posterior distribution.\n\n\n\n\n\n\nNow, assume the prior distribution is \\(\\theta \\sim \\text{Beta}(6,20)\\).\n\n\n\nNew Posterior Distribution\nThe prior is \\(\\text{Beta}(6,20)\\), and the likelihood is still binomial. Using the conjugacy property of the Beta distribution, the posterior becomes:\n\\[\\theta | x \\sim \\text{Beta}(437 + 6, 980 - 437 + 20) = \\text{Beta}(443, 563)\\]\nPosterior Mean and Credible Interval\nThe posterior mean is:\n\\[E(\\theta | x) = \\frac{443}{443 + 563} = \\frac{443}{1006} \\approx 0.440\\]\nThe 95% credible interval can be obtained similarly using R:\nqbeta(c(0.025, 0.975), 443, 563)\nThis gives the interval \\([0.408, 0.473]\\).\nComparing this to the result from the prior \\(\\text{Beta}(1,1)\\), we observe that using the Beta(6,20) prior shifts the posterior slightly, reflecting the stronger prior information.\n\n\n\n\n\n\nNote: Students have not seen an example of using these distributions in class.\n\n\n\nLikelihood Function\nThe likelihood for a Poisson model is:\n\\[p(\\lambda | \\mathbf{y}) = \\prod_{i=1}^n \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\\] Ignoring constants, the likelihood is proportional to:\n\\[p(\\lambda | \\mathbf{y}) \\propto \\lambda^{\\sum y_i} e^{-n \\lambda}\\]\nAppropriateness of Gamma Prior\nThe Gamma distribution is a conjugate prior for the Poisson likelihood because the posterior distribution remains in the Gamma family. This simplifies computation and provides an intuitive interpretation of the prior parameters.\nFinding \\(a\\) and \\(b\\) for Prior\nThe mean and standard deviation of the Gamma distribution are given by:\n\\[\\mu = \\frac{\\alpha}{\\beta}, \\quad \\sigma^2 = \\frac{\\alpha}{\\beta^2}\\] Given that \\(\\mu = 10\\) and \\(\\sigma = 1\\), solve for \\(\\alpha\\) and \\(\\beta\\):\n\\[\\alpha = \\frac{10^2}{1^2} = 100, \\quad \\beta = \\frac{10}{1^2} = 10\\] Hence, \\(\\lambda \\sim \\text{Gamma}(100,10)\\).\nPosterior Distribution\n\n\\(p(\\lambda|y) \\propto p(y|\\lambda)p(\\lambda)\\)\nThe prior: \\(p(\\lambda) \\propto \\lambda^{a-1}e^{-b\\lambda}\\)\n(note: the \\(\\frac{b^a}{\\Gamma(a)}\\) can be removed under the proportionality)\nThe posterior: \\(p(\\lambda|y) \\propto \\lambda^{a-1}e^{-b\\lambda}\\lambda^{\\sum{y_i}}e^{-n\\lambda} = \\lambda^{\\boxed{a + \\sum{y_i}}-1}e^{-\\boxed{(b+n)}\\lambda}\\)\n(note: this has the same form as a gamma distribution with \\(a_{new} = a + \\sum{y_i}\\) and \\(b_{new} = b+n\\))\nTherefore \\(\\lambda|y \\sim gamma(a + \\sum{y_i},b+n)\\)\nWith the data \\(\\mathbf{y} = (17, 25, 25, 21, 13, 22, 23)\\), we have \\(\\sum y_i = 146\\), \\(n = 7\\). So, the posterior is \\(\\lambda | \\mathbf{y} \\sim \\text{Gamma}(246, 17)\\)\n\nPosterior Mean Estimate\nThe posterior mean is:\n\\[E(\\lambda | \\mathbf{y}) = \\frac{246}{17} \\approx 14.47\\]\nConjugacy of the Prior\nYes, the Gamma distribution is a conjugate prior for the Poisson likelihood, as the posterior distribution remains a Gamma distribution.\nComparison of Posterior, Prior, and Data Means\n\nThe prior mean was 10.\nThe posterior mean is \\(14.47\\).\nThe sample mean of the observed data \\(\\mathbf{y}\\) is:\n\\[\\bar{y} = \\frac{146}{7} \\approx 20.86\\]\n\nThe posterior mean is a compromise between the prior mean (10) and the sample mean (20.86), indicating that the data has moved the posterior away from the prior, but not completely to the observed sample mean. This reflects the influence of both the prior belief and the observed data."
  },
  {
    "objectID": "index.html#tutorials",
    "href": "index.html#tutorials",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Tutorials",
    "text": "Tutorials\nTutorial Sheet 1: Bayesian inference using binomial and Poisson models\n\nTutorial Sheet 2: Bayesian Model for Multiple Proportions - Email Campaign Click-Through Rates\nTutorial Sheet 3: Bayesian Regression Model - Fisherys Data"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "Please submit your answers to this assignment by October 23rd, 4:00 PM. Ensure that all calculations are clearly shown, and include explanations where necessary. You may refer to the provided distribution information to help with your answers."
  },
  {
    "objectID": "Assignment1.html#instructions",
    "href": "Assignment1.html#instructions",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "Please submit your answers to this assignment by October 23rd, 4:00 PM. Ensure that all calculations are clearly shown, and include explanations where necessary. You may refer to the provided distribution information to help with your answers."
  },
  {
    "objectID": "Assignment1.html#question",
    "href": "Assignment1.html#question",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "Question",
    "text": "Question\nThe energy of eight(8) particles being emitted form a radioactive source was recorded as:\n\\(y = 50, 60, 60, 80, 40, 40, 80, 70\\text{ MeV}\\).\nWe assume the particles have the following data model:\n\\(y_i \\sim N(\\mu,\\rho \\mu^2)\\),\nwhere \\(\\rho = 1\\)\nA nuclear physicist provides an expert opinion that particles will be emitted with an average energy of 80 MeV. However, she acknowledges uncertainty in this value and suggests the average energy could lie anywhere between 50 MeV and 110 MeV.\n\nShow that the likelihood for \\({\\bf y}\\) is of the form:\n\n\\[p({\\bf y}|\\mu) \\propto \\frac{1}{\\mu^8} \\exp \\left[ -\\frac{15300}{\\mu^2} + \\frac{480}{\\mu} \\right]\\]\n\nAssume a prior for \\(\\mu\\), such that \\(\\mu \\sim gamma(a,b)\\). Use moment matching to determine the values of \\(a\\) and \\(b\\) such that this prior reflects the nuclear physicist‚Äôs opinions about the average energy of the particles. Specifically, assume that the range she provided (50 MeV to 110 MeV) is \\(\\pm\\) 3 standard deviations from the mean.\nWrite down the prior probability density function \\(p(\\mu)\\), up to a constant.\nWrite down the posterior probability density function \\(p(\\mu |y)\\), up to a constant.\n\n\nDistribution Information\n\nYou can use the following information about the Normal and Gamma distributions to help you with the question:\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distribution\nPMF/PDF\nE(X)\nVar(X)\n\n\n\n\nNormal\n\\[\\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{1}{2\\sigma^2}(x_i - \\mu)^2\\right)\\]\n\\[\\mu\\]\n\\[\\sigma^2\\]\n\n\nGamma\n\\[\\frac{b^a}{\\Gamma(a)}x^{a-1}e^{-b x},\\ x \\geq 0\\]\n\\[\\frac{a}{b}\\]\n\\[\\frac{a}{b^2}\\]"
  },
  {
    "objectID": "2b-BayesianInference.html",
    "href": "2b-BayesianInference.html",
    "title": "Bayesian Data Analysis",
    "section": "",
    "text": "Bayesian point estimates are often given by:\n\nthe posterior mean \\(E(\\theta|y)\\)\nor the posterior median \\(\\theta^*\\) with \\(P(\\theta &lt; \\theta^*|y) = 0.5\\)\n\nUncertainty is quantified with credible intervals (CIs)\n\nAn interval is a 95% credible interval if the posterior probability that \\(\\theta\\) is in the interval is 0.95.\nOften quantile based, given by posterior quantiles with \\(P(\\theta &lt; \\theta_{\\alpha/2}|y) = P(\\theta &gt; \\theta_{1-\\alpha/2}|y) = \\alpha/2\\)"
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "Welcome to Bayesian Data Analysis!",
    "section": "Assignments",
    "text": "Assignments\nAssignment 1: Bayesian Inference on Particle Emission Energy\nAssignment 2: Comparing Video Game Playing Among Students in 5 Different Countries"
  },
  {
    "objectID": "4-MCMC.html#what-do-we-know-so-far",
    "href": "4-MCMC.html#what-do-we-know-so-far",
    "title": "Bayesian Data Analysis",
    "section": "",
    "text": "We can use Bayes‚Äô rule to get posterior densities\n\nExample for single parameter Normal - \\(\\mu\\)\n\n\\(p(\\mu|y) \\propto p(y|\\mu)p(\\mu)\\)\n\nExample for single parameter Normal - \\(\\tau\\)\n\n\\(p(\\tau|y) \\propto p(y|\\tau)p(\\tau)\\)\n\nWe could also do this for multiple parameters\n\n\\(p(\\mu, \\tau|y) \\propto p(y|\\mu,\\tau)p(\\mu,\\tau)\\)\n\n\nBUT‚Ä¶\n\nRealistic problems with multiple parameters, data points, and common choices of priors don‚Äôt get a closed-form expression for \\(p(\\mu, \\tau|y)\\)\nSampling to the rescue!!"
  },
  {
    "objectID": "4-MCMC.html#simulation-based-inference-recap",
    "href": "4-MCMC.html#simulation-based-inference-recap",
    "title": "Bayesian Data Analysis",
    "section": "Simulation-based inference (recap)",
    "text": "Simulation-based inference (recap)\n\nThe general idea in simulation-based inference: We can make inference about a parameter \\(\\theta\\) or \\(\\mu\\) or \\(\\tau\\) , using a sample e.g., \\(\\{\\theta^{(1)}\\ldots\\theta^{(S)}\\}\\), from its probability distribution.\nAssessing the properties of a target (e.g., posterior) distribution by generating representative samples is called Monte Carlo simulation.\nBased on the law of large numbers we know that:\n\\(\\frac{1}{S}\\sum_{s=1}^{S}\\theta^{(s)} = E(\\theta)\\)\nas sample size \\(S \\to \\infty\\)\n\nThe error in the MC approximation goes to zero as \\(S \\to \\infty\\) because \\(\\frac{var(\\theta)}{S} \\to 0\\)\n\nJust about any aspect of the distribution of \\(\\theta\\) can be approximated arbitrarily exactly with a large enough Monte Carlo sample, e.g.,\n\nthe \\(\\alpha\\)-percentile of the distribution of \\(\\theta\\)\n\\(Pr(\\theta \\geq x)\\) for any constant \\(x\\)"
  },
  {
    "objectID": "4-MCMC.html#markov-chain-monte-carlo-mcmc",
    "href": "4-MCMC.html#markov-chain-monte-carlo-mcmc",
    "title": "Bayesian Data Analysis",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\nWe‚Äôve already discussed Monte Carlo sampling. Now we‚Äôre going to consider Markov Chain Monte Carlo (MCMC) sampling. MCMC algorithms are used for sampling from a target distribution.\n\n\n\n\n\n\nMarkov Chain\n\n\n\n\nAny process in which each step has no memory of states before the current state is called a (first-order) Markov process and a sucession of such steps is a Markov chain.\nThe stationary distribution of the Markov chain is the target distribution.\n\n\n\nUsing MCMC we can construct a set of samples from an unknown target (e.g., posterior) distribution."
  },
  {
    "objectID": "4-MCMC.html#a-simple-markov-chain-example",
    "href": "4-MCMC.html#a-simple-markov-chain-example",
    "title": "Bayesian Data Analysis",
    "section": "A simple Markov chain example",
    "text": "A simple Markov chain example\nWhat is the probability of sunny \\(P(S)\\) and rainy \\(P(R)\\) weather?\n\n\n\n\nAssume we have the following ‚Äútransition‚Äù probabilities:\n\\(P(S_{t+1}|R_t) = 0.5\\)\n\\(P(R_{t+1}|R_t) = 0.5\\)\n\\(P(R_{t+1}|S_t) = 0.1\\)\n\\(P(S_{t+1}|S_t) = 0.9\\)\nNote the future state of the weather only depends on the current state."
  },
  {
    "objectID": "4-MCMC.html#creating-the-markov-chain",
    "href": "4-MCMC.html#creating-the-markov-chain",
    "title": "Bayesian Data Analysis",
    "section": "Creating the Markov Chain",
    "text": "Creating the Markov Chain\n\n\n\nThe ‚Äúalgorithm‚Äù\n\nWe have to initialize i.e., a randomly chosen start point of sunny or rainy.\nWe can sample the weather, W, (coded as 1 for sunny and 0 for rainy) from a Bernoulli distribution (Binomial with \\(n=1\\)).\n\n\nThe probability distribution will change depending on the current state of the chain, i.e., if W = 1, or W =0.\n\n\nBased on the samples of \\(W\\) we can estimate \\(P(W=1) = P(S)\\) and \\(P(W=0) = P(R)\\).\n\nWe find after enough iterations (samples) \\(P(S) = 0.83\\).\n\n\n\nR code\n\n# 0 = rainy, 1 = sunny\n\n# 1. initialize\nW_current &lt;- rbinom(1,1,0.5) \n\n# 2. run the Markov Chain\n# number of Monte Carlo sampling interations\nn_iter &lt;- 10000\n# save the samples\nW_new &lt;- p_sunny &lt;- rep(NA, n_iter)\n\nfor(i in 1:n_iter)\n{\nW_new[i] &lt;- ifelse(W_current == 0,\n       rbinom(1,1,0.5),\n       rbinom(1,1,0.9))\n\nW_current = W_new[i]\n\n# 3. get the probability of sunny\np_sunny[i] &lt;- sum(W_new, na.rm = TRUE)/i\n\n}\n\nplot(p_sunny, type = \"l\", xlab = \"Iteration\", ylab = \"P(S)\")"
  },
  {
    "objectID": "4-MCMC.html#stationary-distribution",
    "href": "4-MCMC.html#stationary-distribution",
    "title": "Bayesian Data Analysis",
    "section": "Stationary Distribution",
    "text": "Stationary Distribution\nAs we progress through time, the probability of being in certain states (e.g., rainy or sunny) are more likely than others. Over the long run, the distribution will reach an equilibrium with an associated probability of being in each state.\nThis is known as the Stationary Distribution.\n\n\n\n\n\n\nThe stationary distribution of a Markov chain\n\n\nThe stationary distribution of a Markov chain is a probability distribution over the states of the chain that remains unchanged as time progresses.\nMathematically, if \\(\\pi\\) represents the stationary distribution (a row vector of probabilities) and \\(P\\) is the transition matrix of the Markov chain, the stationary distribution satisfies the equation:\n\\[\\pi = \\pi P\\]\nThis means that when the transition matrix \\(P\\) is applied to \\(\\pi\\), the result is still \\(\\pi\\), indicating that the distribution has stabilized and doesn‚Äôt change over time."
  },
  {
    "objectID": "4-MCMC.html#the-metropolis-algorithm",
    "href": "4-MCMC.html#the-metropolis-algorithm",
    "title": "Bayesian Data Analysis",
    "section": "The Metropolis Algorithm",
    "text": "The Metropolis Algorithm\n\nSuppose we have a target distribution \\(p(\\theta|y)\\) from which we would like to generate a representative sample.\nSample values from the target distribution can be generated by taking a random walk through the parameter space.\n\n\n\n\n\n\n\nThe metropolis algorithm\n\n\n\nStart at some arbitrary parameter value (initial value).\nPropose a move to a new value in the parameter space.\nCalculate the acceptance ratio \\(r = min \\bigg(1, \\frac{ p(y|\\theta_{pro})p(\\theta_{pro})}{p(y|\\theta_{cur})p(\\theta_{cur})}\\bigg)\\).\nDraw a random number, \\(u\\) between 0 and 1. If \\(u &lt; r\\) then the move is accepted.\nRepeat until a representative sample from the target distribution has been generated (more on this later)."
  },
  {
    "objectID": "4-MCMC.html#metropolis-algorithm-for-the-happiness-example",
    "href": "4-MCMC.html#metropolis-algorithm-for-the-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "Metropolis Algorithm for the Happiness example",
    "text": "Metropolis Algorithm for the Happiness example\nRecall for the Happiness example: n = 20 women, y = 14 women reported being happy.\n\n\n\nThe Metropolis algorithm\n\\(y \\sim Binomial(n = 20, \\theta)\\)\n\\(\\theta|a,b \\sim Beta(a = 1 ,b = 1)\\)\n\nLet‚Äôs initialise using \\(\\theta_{cur}\\) = 0.5\nPropose a new move using a Normal proposal distribution such that \\(\\theta_{pro} = \\theta_{cur} + N(0,\\sigma)\\)\n\\(r = \\text{min} \\bigg(1, \\frac{ dbinom(y,n,\\theta_{pro})dbeta(\\theta_{pro},a,b)}{dbinom(y,n,\\theta_{cur})dbeta(\\theta_{cur},a,b)}\\bigg)\\)\nCompare \\(u \\sim Uniform(0,1)\\) with \\(r\\) and accept move if \\(u &lt; r\\)\nRepeat\n\nVisualise the samples and look for a ‚Äúfuzzy caterpiller‚Äù üêõ\n\n\n\nR code\n\n# data \ny &lt;- 14; N &lt;- 20\n# beta prior parameters\na &lt;- 1; b &lt;- 1\n# number of samples to generate\nn_iter &lt;- 10000\n\n# 0. \ntheta_cur &lt;- rep(NA, n_iter)\ntheta_cur[1] &lt;- 0.5\n\nfor(i in 1:(n_iter-1)){\n# 1.\ntheta_pro &lt;- theta_cur[i] + rnorm(1,0,sd = 0.2)\n# 2. \nif(theta_pro&lt;0|theta_pro&gt;1){r &lt;- 0 } # set to zero if theta outside [0,1]\nelse {\n  r &lt;- \nmin(1,dbinom(14,20,theta_pro)*dbeta(theta_pro,a,b)/\n      dbinom(14,20,theta_cur[i])*dbeta(theta_cur[i],a,b))\n  }\n# 3. \nu &lt;- runif(1,0,1)\naccept &lt;- u &lt; r\ntheta_cur[i+1]&lt;- ifelse(accept,theta_pro,theta_cur[i])\n} # end i loop\n\nplot(theta_cur, type = \"l\")"
  },
  {
    "objectID": "4-MCMC.html#trace-plot-and-chains-for-theta",
    "href": "4-MCMC.html#trace-plot-and-chains-for-theta",
    "title": "Bayesian Data Analysis",
    "section": "Trace plot and chains for \\(\\theta\\)",
    "text": "Trace plot and chains for \\(\\theta\\)\n\nTraceplots provide a visual tool for monitoring convergence of an MCMC chain towards a target distribution (i.e., the posterior).\nIn general we look for a stationary plot where the sample values display a random scatter around a mean value.\nWe typically initialise MCMC algorithms at 3 different start points to see if all chains end up in (converge to) the same place."
  },
  {
    "objectID": "4-MCMC.html#metropolis-algorithm-for-the-kid-iq-example",
    "href": "4-MCMC.html#metropolis-algorithm-for-the-kid-iq-example",
    "title": "Bayesian Data Analysis",
    "section": "Metropolis algorithm for the Kid IQ example",
    "text": "Metropolis algorithm for the Kid IQ example\nRecall that data (y) are available on the cognitive test scores of three- and four-year-old children in the USA. The sample contains 434 observations.\n\\(y_i|\\mu,\\sigma^2 \\sim N(\\mu, \\sigma^2)\\). Assume \\(\\sigma^2\\) is known where \\(\\sigma = 20.4\\)\n\\(\\mu|\\mu_0,\\sigma_0 \\sim N(\\mu_0 = 80, \\sigma^2_{_0} = 10^2)\\)\n\nLet‚Äôs initialise using \\(\\mu_{cur}\\) = 80.\nPropose a new move using a Normal proposal distribution such that \\(\\mu_{pro} = \\mu_{cur} + N(0,1)\\).\nCalculate the acceptance ratio on the log scale (avoids numerical instability).\n\n\\(log(r) = \\text{min}(0, \\sum_i log(dnorm(y_i,\\mu_{pro},20.4)) + log(dnorm(\\mu_{pro},80,10))\\) \\(- \\sum_i log(dnorm(y_i,\\mu_{cur},20.4)) - log(dnorm(\\mu_{cur},80,10)))\\)\n\nCompare \\(u \\sim Uniform(0,1)\\) with \\(r\\) and accept move if \\(log(u) &lt; r\\).\n\nTask: Code this in R and produce a trace plot for \\(\\mu\\)."
  },
  {
    "objectID": "4-MCMC.html#bayesian-inference-for-mu-and-sigma-the-kid-iq-example",
    "href": "4-MCMC.html#bayesian-inference-for-mu-and-sigma-the-kid-iq-example",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Inference for \\(\\mu\\) and \\(\\sigma\\) (the Kid IQ example)",
    "text": "Bayesian Inference for \\(\\mu\\) and \\(\\sigma\\) (the Kid IQ example)\n\nIt is more realistic to assume \\(\\mu\\) and \\(\\sigma\\) are unknown.\nIn this case we need priors for both parameters and we think about them jointly.\nThen from Bayes‚Äô rule we can get the joint posterior \\(p(\\mu, \\sigma |y) \\propto p(y|\\mu, \\sigma)p(\\mu,\\sigma)\\)\nProblem: most choices of prior will not result in a closed from expression for the posterior.\nSolution: If we can sample from the target (posterior) distribution we can still do inference.\nThe Metropolis method is very useful but can be inefficient. Another sampling method that‚Äôs often used for models with multiple parameters is Gibbs sampling."
  },
  {
    "objectID": "4-MCMC.html#the-gibbs-sampler",
    "href": "4-MCMC.html#the-gibbs-sampler",
    "title": "Bayesian Data Analysis",
    "section": "The Gibbs Sampler",
    "text": "The Gibbs Sampler\nWe can use Gibbs Sampling when we can sample directly from the conditional posterior distributions for each model parameter.\nSo instead of trying to sample directly from a joint posterior distribution, we sample parameters sequentially from their complete conditional distributions (conditioning on data as well as all other model parameters).\n\n\n\n\n\n\nThe Gibbs sampling algorithm for \\(\\mu\\) and \\(\\tau\\)\n\n\n\nAssign initial values to the parameters \\(\\mu^{(1)}\\) and \\(\\tau^{(1)}\\)\nGiven starting values \\(\\mu^{(1)}\\) and \\(\\tau^{(1)}\\), draw samples s = 2, 3,‚Ä¶.,`some large number‚Äô as follows:\n1.1. sample \\(\\tau^{(s)}\\) from \\(p(\\tau|y,\\mu^{(s-1)})\\).\n1.2. sample \\(\\mu^{(s)}\\) from \\(p(\\mu|y,\\tau^{(s)})\\)\nRepeat and this will (eventually) generate samples from \\(p(\\mu,\\sigma|y)\\), which is what we want!"
  },
  {
    "objectID": "4-MCMC.html#remember-those-complete-conditional-distributions",
    "href": "4-MCMC.html#remember-those-complete-conditional-distributions",
    "title": "Bayesian Data Analysis",
    "section": "Remember those complete conditional distributions?",
    "text": "Remember those complete conditional distributions?\nRecall for the Normal likelihood \\(y_i|\\mu,\\sigma^2 \\sim N(\\mu, \\sigma^2)\\) where \\(\\sigma\\) is known and a Normal prior \\(\\mu \\sim N(\\mu_0, \\sigma^2_{_0})\\), we get a posterior for \\(\\mu\\) that is also a normal distribution (i.e., we used a conjugate prior)\n\\[\\mu|y \\sim N \\bigg(\\frac{\\mu_0/\\sigma^2_{0}+ n\\bar{y}/\\sigma^2}{1/\\sigma^2_{0}+n/\\sigma^2}, {\\frac{1}{1/\\sigma^2_{0}+n/\\sigma^2}}\\bigg)\\]\nRecall for the Normal likelihood \\(y_i|\\mu,\\sigma^2 \\sim N(\\mu, \\sigma^2)\\) where \\(\\mu\\) is known and a Gamma prior \\(\\frac{1}{\\sigma^2} = \\tau \\sim Gamma(a, b)\\) we get a posterior for \\(\\tau\\) that will also be a gamma distribution\n\\[\\tau|y \\sim Gamma \\bigg(a + n/2, b + 1/2\\sum_{i=1}^n (y_i - \\mu)^2\\bigg)\\]\nThese are the complete conditionals. We know the posterior distribution for one parameter conditional on knowning the other parameter(s). Now we can use Gibbs sampling."
  },
  {
    "objectID": "4-MCMC.html#gibbs-sampling-algorithm-for-the-kid-iq-example",
    "href": "4-MCMC.html#gibbs-sampling-algorithm-for-the-kid-iq-example",
    "title": "Bayesian Data Analysis",
    "section": "Gibbs sampling algorithm for the Kid IQ example",
    "text": "Gibbs sampling algorithm for the Kid IQ example\nRecall data (y) are available on the cognitive test scores of three- and four-year-old children in the USA. The sample contains \\(n=434\\) observations.\n\n\n\\(y \\sim Normal(\\mu, \\sigma^2)\\)\n\\(\\mu \\sim Normal(80 ,10)\\) \\(1/\\sigma^2 = \\tau \\sim gamma(1 ,1)\\)\n\nLet‚Äôs initialise using \\(\\mu^{(1)} = 80, \\tau^{(1)} = 1\\)\nGiven starting values \\(\\mu^{(1)}\\) and \\(\\tau^{(1)}\\), draw samples s = 2, 3,‚Ä¶.,`some large number‚Äô as follows:\n1.1. sample \\(\\tau^{(s)}\\) from \\(p(\\tau|y,\\mu^{(s-1)})\\).\n1.2. sample \\(\\mu^{(s)}\\) from \\(p(\\mu|y,\\tau^{(s)})\\)\nRepeat and this will (eventually) generate samples from \\(p(\\mu,\\sigma|y)\\), which is what we want!\n\n\n\n## Data\ny &lt;- kidiq$kid_score\nn &lt;- length(y)\n\n## Prior parameters \nmu0 &lt;- 80; sigma.mu0 &lt;- 10 # for normal prior (for mu)\na &lt;- 1; b &lt;- 1 # for gamma prior (for tau)\n\nn_iter &lt;- 10000 ## choose # iter\n## create objects to store results\nmu_s &lt;- tau_s &lt;- sigma_s &lt;- rep(NA, n_iter)\n\n## 0. Initialise \nmu_s[1] &lt;- 80; tau_s[1] &lt;- 1; sigma_s[1] &lt;- 1\n## define parameters for gamma posterior on tau\npost.a = a + n/2; post.b = 1/2*(sum((y-mu_s[1])^2))\n\n## 1.\nfor(s in 2:n_iter){\n## 1.1 sample from complete conditional for tau\ntau_s[s] &lt;- rgamma(1,post.a,post.b)\nsigma_s[s] &lt;- sqrt(1/tau_s[s]) # transform to sigma\n## update posterior parameters for mu\nmupost.mean = (mu0/(sigma.mu0^2) + tau_s[s]*n*mean(y))/(1/(sigma.mu0^2) + tau_s[s]*n)\nmupost.sd = sqrt(1/(1/(sigma.mu0^2)+tau_s[s]*n))\n## 1.2 sample from complete conditional for mu\nmu_s[s] &lt;- rnorm(1,mupost.mean,mupost.sd)\n## update posterior parameters for tau\npost.a = a + n/2\npost.b = b + 1/2*(sum((y-mu_s[s])^2))\n} # end s loop"
  },
  {
    "objectID": "4-MCMC.html#output-from-mcmc",
    "href": "4-MCMC.html#output-from-mcmc",
    "title": "Bayesian Data Analysis",
    "section": "Output from MCMC",
    "text": "Output from MCMC\nMCMC samples are not independent draws from a target distribution:\n\nThe first draw is not a random draw from the target distribution and tuning of MCMC parameters may occur at the start of a chain.\nSubsequently, draw s + 1 depends on draw s: the samples may be autocorrelated.\n\nWe can use samples from an MCMC algorithm to do inference but ONLY IF\n\nWe exclude samples from the initial period (burn-in).\nWe ‚Äúwait long enough‚Äù to get a set of samples that are representative of the target (posterior). distribution.\n\n\n# Create a dataset (tibble) of samples\n# chose a \"burn-in\" \nn_burnin &lt;- 100\n# Remove the first sample from mu_s and sigma_s\npost_samps &lt;- tibble(sample_index = 1:(n_iter-n_burnin),\n                     mu_s = mu_s[-(1:n_burnin)], \n                     sigma_s = sigma_s[-(1:n_burnin)])\npost_samps\n\n# A tibble: 9,900 √ó 3\n   sample_index  mu_s sigma_s\n          &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1            1  87.9    20.3\n 2            2  86.0    20.2\n 3            3  85.0    21.8\n 4            4  86.4    18.8\n 5            5  86.1    21.3\n 6            6  88.0    21.7\n 7            7  86.8    21.4\n 8            8  86.5    20.3\n 9            9  87.6    20.3\n10           10  87.5    20.2\n# ‚Ñπ 9,890 more rows"
  },
  {
    "objectID": "4-MCMC.html#output-from-mcmc-trace-plots",
    "href": "4-MCMC.html#output-from-mcmc-trace-plots",
    "title": "Bayesian Data Analysis",
    "section": "Output from MCMC: trace plots",
    "text": "Output from MCMC: trace plots\nWe can use trace plots to investigate the MCMC samples/chains for the parameters.\n\nlibrary(ggplot2)\n\n# Trace plot for mu\np1 &lt;- ggplot(post_samps, aes(x = sample_index, mu_s)) +\n        geom_line(colour = \"blue\") +\n        ggtitle(\"Posterior Samples of mu\")\n\n# Trace plot for sigma\np2 &lt;- ggplot(post_samps, aes(x = sample_index, sigma_s)) +\n        geom_line(colour = \"blue\") +\n        ggtitle(\"Posterior Samples of sigma\")\n\np1;p2"
  },
  {
    "objectID": "4-MCMC.html#output-from-mcmc-joint-parameter-plots",
    "href": "4-MCMC.html#output-from-mcmc-joint-parameter-plots",
    "title": "Bayesian Data Analysis",
    "section": "Output from MCMC: joint parameter plots",
    "text": "Output from MCMC: joint parameter plots\nWe can visualise the joint posterior distribution of the parameters based on the MCMC samples.\n\n# Scatter plot of mu_s vs. sd_s\np3 &lt;- ggplot(post_samps, aes(x = sigma_s, mu_s)) +\n        geom_point(colour = \"blue\") \n\n# Contour plot of joint density\np4 &lt;- ggplot(post_samps, aes(x = sigma_s, y = mu_s)) + \n  geom_density_2d(colour = \"blue\")    \n\np3;p4"
  },
  {
    "objectID": "4-MCMC.html#output-from-mcmc-density-plots-and-summaries",
    "href": "4-MCMC.html#output-from-mcmc-density-plots-and-summaries",
    "title": "Bayesian Data Analysis",
    "section": "Output from MCMC: density plots and summaries",
    "text": "Output from MCMC: density plots and summaries\nWe can visualise and obtain summaries for the marginal posterior distributions of the parameters based on the MCMC samples.\n\n# Density plot for posterior samples of mu\np5 &lt;- ggplot(post_samps, aes(x = mu_s)) +\n        geom_density(fill = \"lightblue\", \n                     color = \"blue\", \n                     alpha = 0.6) +\n        ggtitle(\"Posterior Density of mu\") \n\n# Mean and credible interval for mu\nmu_mean &lt;- mean(mu_s)\nmu_ci &lt;- quantile(mu_s, probs = c(0.025, 0.975))\n\np5;cat(\"Mean of Mu:\", mu_mean, \"\\n\");cat(\"95% Credible Interval for Mu:\", mu_ci, \"\\n\")\n\n\n\n\n\n\n\n\nMean of Mu: 86.74137 \n\n\n95% Credible Interval for Mu: 84.82647 88.66879 \n\n\n\np6 &lt;- ggplot(post_samps, aes(x = sigma_s)) +\n        geom_density(fill = \"lightblue\", \n                     color = \"blue\", \n                     alpha = 0.6) +\n        ggtitle(\"Posterior Density of sigma\") \n\n# Mean and credible interval for sigma\nsigma_mean &lt;- mean(sigma_s)\nsigma_ci &lt;- quantile(sigma_s, probs = c(0.025, 0.975))\n\np6;cat(\"Mean of Sigma:\", sigma_mean, \"\\n\");cat(\"95% Credible Interval for Sigma:\", sigma_ci, \"\\n\")\n\n\n\n\n\n\n\n\nMean of Sigma: 20.39958 \n\n\n95% Credible Interval for Sigma: 19.07164 21.79159"
  },
  {
    "objectID": "4-MCMC.html#stationary-distributions",
    "href": "4-MCMC.html#stationary-distributions",
    "title": "Bayesian Data Analysis",
    "section": "Stationary Distributions",
    "text": "Stationary Distributions\nAs we progress through time, the probability of being in certain states (e.g., rainy or sunny) are more likely than others. Over the long run, the distribution will reach an equilibrium with an associated probability of being in each state.\nThis is known as the Stationary Distribution.\n\n\n\n\n\n\nThe stationary distribution of a Markov chain\n\n\n\nThe stationary distribution of a Markov chain is a probability distribution over the states of the chain that remains unchanged as time progresses.\nMathematically, if \\(\\pi\\) represents the stationary distribution (a row vector of probabilities) and \\(P\\) is the transition matrix of the Markov chain, the stationary distribution satisfies the equation:\n\\[\\pi = \\pi P\\]\nThis means that when the transition matrix \\(P\\) is applied to \\(\\pi\\), the result is still \\(\\pi\\), indicating that the distribution has stabilized and doesn‚Äôt change over time."
  },
  {
    "objectID": "4-MCMC.html#gibbs-aampling-algorithm-for-the-kid-iq-example",
    "href": "4-MCMC.html#gibbs-aampling-algorithm-for-the-kid-iq-example",
    "title": "Bayesian Data Analysis",
    "section": "Gibbs aampling algorithm for the Kid IQ example",
    "text": "Gibbs aampling algorithm for the Kid IQ example\nRecall data (y) are available on the cognitive test scores of three- and four-year-old children in the USA. The sample contains \\(n=434\\) observations.\n\n\n\\(y \\sim Normal(\\mu, \\sigma^2)\\)\n\\(\\mu \\sim Normal(80 ,10)\\) \\(1/\\sigma^2 = \\tau \\sim gamma(1 ,1)\\)\n\nLet‚Äôs initialise using \\(\\mu^{(1)} = 80, \\tau^{(1)} = 1\\)\nGiven starting values \\(\\mu^{(1)}\\) and \\(\\tau^{(1)}\\), draw samples s = 2, 3,‚Ä¶.,`some large number‚Äô as follows:\n1.1. sample \\(\\tau^{(s)}\\) from \\(p(\\tau|y,\\mu^{(s-1)})\\).\n1.2. sample \\(\\mu^{(s)}\\) from \\(p(\\mu|y,\\tau^{(s)})\\)\nRepeat and this will (eventually) generate samples from \\(p(\\mu,\\sigma|y)\\), which is what we want!\n\n\n\n## Data\ny &lt;- kidiq$kid_score\nn &lt;- length(y)\n\n## Prior parameters \nmu0 &lt;- 80; sigma.mu0 &lt;- 10 # for normal prior (for mu)\na &lt;- 1; b &lt;- 1 # for gamma prior (for tau)\n\nn_iter &lt;- 10000 ## choose # iter\n## create objects to store results\nmu_s &lt;- tau_s &lt;- sigma_s &lt;- rep(NA, n_iter)\n\n## 0. Initialise \nmu_s[1] &lt;- 80; tau_s[1] &lt;- 1; sigma_s[1] &lt;- 1\n## define parameters for gamma posterior on tau\npost.a = a + n/2; post.b = 1/2*(sum((y-mu_s[1])^2))\n\n## 1.\nfor(s in 2:n_iter){\n## 1.1 sample from complete conditional for tau\ntau_s[s] &lt;- rgamma(1,post.a,post.b)\nsigma_s[s] &lt;- sqrt(1/tau_s[s]) # transform to sigma\n## update posterior parameters for mu\nmupost.mean = (mu0/(sigma.mu0^2) + tau_s[s]*n*mean(y))/(1/(sigma.mu0^2) + tau_s[s]*n)\nmupost.sd = sqrt(1/(1/(sigma.mu0^2)+tau_s[s]*n))\n## 1.2 sample from complete conditional for mu\nmu_s[s] &lt;- rnorm(1,mupost.mean,mupost.sd)\n## update posterior parameters for tau\npost.a = a + n/2\npost.b = b + 1/2*(sum((y-mu_s[s])^2))\n} # end s loop"
  },
  {
    "objectID": "4-MCMC.html#creating-a-markov-chain-monte-carlo-sample",
    "href": "4-MCMC.html#creating-a-markov-chain-monte-carlo-sample",
    "title": "Bayesian Data Analysis",
    "section": "Creating a Markov Chain Monte Carlo Sample",
    "text": "Creating a Markov Chain Monte Carlo Sample\n\n\nThe ‚Äúalgorithm‚Äù\n\nWe have to initialize i.e., a randomly chosen start point of sunny or rainy.\nWe can sample the weather, W, (coded as 1 for sunny and 0 for rainy) from a Bernoulli distribution (Binomial with \\(n=1\\)).\n\n\nThe probability distribution will change depending on the current state of the chain, i.e., if W = 1, or W =0.\n\n\nBased on the samples of \\(W\\) we can estimate \\(P(W=1) = P(S)\\) and \\(P(W=0) = P(R)\\).\n\nWe find after enough iterations (samples) \\(P(S) = 0.83\\).\n\nR code\n\n# 0 = rainy, 1 = sunny\n\n# 1. initialize\nW_current &lt;- rbinom(1,1,0.5) \n\n# 2. run the Markov Chain\n# number of Monte Carlo sampling interations\nn_iter &lt;- 10000\n# save the samples\nW_new &lt;- p_sunny &lt;- rep(NA, n_iter)\n\nfor(i in 1:n_iter)\n{\nW_new[i] &lt;- ifelse(W_current == 0,\n       rbinom(1,1,0.5),\n       rbinom(1,1,0.9))\n\nW_current = W_new[i]\n\n# 3. get the probability of sunny\np_sunny[i] &lt;- sum(W_new, na.rm = TRUE)/i\n\n}\n\nplot(p_sunny, type = \"l\", xlab = \"Iteration\", ylab = \"P(S)\")"
  },
  {
    "objectID": "4-MCMC.html#using-mcmc-sampling-to-find-the-stationary-distribution",
    "href": "4-MCMC.html#using-mcmc-sampling-to-find-the-stationary-distribution",
    "title": "Bayesian Data Analysis",
    "section": "Using MCMC sampling to find the stationary distribution",
    "text": "Using MCMC sampling to find the stationary distribution\n\n\nThe ‚Äúalgorithm‚Äù\n\nWe have to initialize i.e., a randomly chosen start point of sunny or rainy.\nWe can sample the weather, W, (coded as 1 for sunny and 0 for rainy) from a Bernoulli distribution (Binomial with \\(n=1\\)).\n\n\nThe probability distribution will change depending on the current state of the chain, i.e., if W = 1, or W =0.\n\n\nBased on the samples of \\(W\\) we can estimate \\(P(W=1) = P(S)\\) and \\(P(W=0) = P(R)\\).\n\nWe find after enough iterations (samples) \\(P(S) = 0.83\\).\n\nR code\n\n# 0 = rainy, 1 = sunny\n\n# 1. initialize\nW_current &lt;- rbinom(1,1,0.5) \n\n# 2. run the Markov Chain\n# number of Monte Carlo sampling interations\nn_iter &lt;- 10000\n# save the samples\nW_new &lt;- p_sunny &lt;- rep(NA, n_iter)\n\nfor(i in 1:n_iter)\n{\nW_new[i] &lt;- ifelse(W_current == 0,\n       rbinom(1,1,0.5),\n       rbinom(1,1,0.9))\n\nW_current = W_new[i]\n\n# 3. get the probability of sunny\np_sunny[i] &lt;- sum(W_new, na.rm = TRUE)/i\n\n}\n\nplot(p_sunny, type = \"l\", xlab = \"Iteration\", ylab = \"P(S)\")"
  },
  {
    "objectID": "4-MCMC.html#mcmc-the-metropolis-algorithm",
    "href": "4-MCMC.html#mcmc-the-metropolis-algorithm",
    "title": "Bayesian Data Analysis",
    "section": "MCMC: The Metropolis Algorithm",
    "text": "MCMC: The Metropolis Algorithm\n\nSuppose we have a target distribution \\(p(\\theta|y)\\) from which we would like to generate a representative sample.\nSample values from the target distribution can be generated by taking a random walk through the parameter space.\n\n\n\n\n\n\n\nThe metropolis algorithm\n\n\n\n\n\n\nStart at some arbitrary parameter value (initial value).\nPropose a move to a new value in the parameter space.\nCalculate the acceptance ratio \\(r = min \\bigg(1, \\frac{ p(y|\\theta_{pro})p(\\theta_{pro})}{p(y|\\theta_{cur})p(\\theta_{cur})}\\bigg)\\).\nDraw a random number, \\(u\\) between 0 and 1. If \\(u &lt; r\\) then the move is accepted.\nRepeat until a representative sample from the target distribution has been generated (more on this later)."
  },
  {
    "objectID": "4-MCMC.html#mcmc-the-gibbs-sampler",
    "href": "4-MCMC.html#mcmc-the-gibbs-sampler",
    "title": "Bayesian Data Analysis",
    "section": "MCMC: The Gibbs Sampler",
    "text": "MCMC: The Gibbs Sampler\nWe can use Gibbs Sampling when we can sample directly from the conditional posterior distributions for each model parameter.\nSo instead of trying to sample directly from a joint posterior distribution, we sample parameters sequentially from their complete conditional distributions (conditioning on data as well as all other model parameters).\n\n\n\n\n\n\nThe Gibbs sampling algorithm for \\(\\mu\\) and \\(\\tau\\)\n\n\n\n\n\n\nAssign initial values to the parameters \\(\\mu^{(1)}\\) and \\(\\tau^{(1)}\\)\nGiven starting values \\(\\mu^{(1)}\\) and \\(\\tau^{(1)}\\), draw samples s = 2, 3,‚Ä¶.,`some large number‚Äô as follows:\n1.1. sample \\(\\tau^{(s)}\\) from \\(p(\\tau|y,\\mu^{(s-1)})\\).\n1.2. sample \\(\\mu^{(s)}\\) from \\(p(\\mu|y,\\tau^{(s)})\\)\nRepeat and this will (eventually) generate samples from \\(p(\\mu,\\sigma|y)\\), which is what we want!"
  },
  {
    "objectID": "4-MCMC.html#visualising",
    "href": "4-MCMC.html#visualising",
    "title": "Bayesian Data Analysis",
    "section": "Visualising",
    "text": "Visualising"
  },
  {
    "objectID": "4-MCMC.html#metropolis-algorithm-for-the-happiness-example-1",
    "href": "4-MCMC.html#metropolis-algorithm-for-the-happiness-example-1",
    "title": "Bayesian Data Analysis",
    "section": "Metropolis Algorithm for the Happiness example",
    "text": "Metropolis Algorithm for the Happiness example"
  },
  {
    "objectID": "5a-Diagnostics.html#recap-gibbs-sampling-algorithm-for-the-kid-iq-example",
    "href": "5a-Diagnostics.html#recap-gibbs-sampling-algorithm-for-the-kid-iq-example",
    "title": "Bayesian Data Analysis",
    "section": "Recap: Gibbs Sampling Algorithm for the Kid IQ example",
    "text": "Recap: Gibbs Sampling Algorithm for the Kid IQ example\nRecall data (y) are available on the cognitive test scores of three- and four-year-old children in the USA. The sample contains \\(n=434\\) observations.\n\n\n\\(y \\sim Normal(\\mu, \\sigma^2)\\)\n\\(\\mu \\sim Normal(80 ,10)\\) \\(1/\\sigma^2 = \\tau \\sim gamma(1 ,1)\\)\n\nLet‚Äôs initialise using \\(\\mu^{(1)} = 80, \\tau^{(1)} = 1\\)\nGiven starting values \\(\\mu^{(1)}\\) and \\(\\tau^{(1)}\\), draw samples s = 2, 3,‚Ä¶.,`some large number‚Äô as follows:\n1.1. sample \\(\\tau^{(s)}\\) from \\(p(\\tau|y,\\mu^{(s-1)})\\).\n1.2. sample \\(\\mu^{(s)}\\) from \\(p(\\mu|y,\\tau^{(s)})\\)\nRepeat and this will (eventually) generate samples from \\(p(\\mu,\\sigma|y)\\), which is what we want!\n\n\n\n## Data\ny &lt;- kidiq$kid_score\nn &lt;- length(y)\n\n## Prior parameters \nmu0 &lt;- 80; sigma.mu0 &lt;- 10 # for normal prior (for mu)\na &lt;- 1; b &lt;- 1 # for gamma prior (for tau)\n\nn_iter &lt;- 10000 ## choose # iter\n## create objects to store results\nmu_s &lt;- tau_s &lt;- sigma_s &lt;- rep(NA, n_iter)\n\n## 0. Initialise \nmu_s[1] &lt;- 80; tau_s[1] &lt;- 1; sigma_s[1] &lt;- 1\n## define parameters for gamma posterior on tau\npost.a = a + n/2; post.b = 1/2*(sum((y-mu_s[1])^2))\n\n## 1.\nfor(s in 2:n_iter){\n## 1.1 sample from complete conditional for tau\ntau_s[s] &lt;- rgamma(1,post.a,post.b)\nsigma_s[s] &lt;- sqrt(1/tau_s[s]) # transform to sigma\n## update posterior parameters for mu\nmupost.mean = (mu0/(sigma.mu0^2) + tau_s[s]*n*mean(y))/(1/(sigma.mu0^2) + tau_s[s]*n)\nmupost.sd = sqrt(1/(1/(sigma.mu0^2)+tau_s[s]*n))\n## 1.2 sample from complete conditional for mu\nmu_s[s] &lt;- rnorm(1,mupost.mean,mupost.sd)\n## update posterior parameters for tau\npost.a = a + n/2\npost.b = b + 1/2*(sum((y-mu_s[s])^2))\n} # end s loop"
  },
  {
    "objectID": "5a-Diagnostics.html#recap-output-from-mcmc",
    "href": "5a-Diagnostics.html#recap-output-from-mcmc",
    "title": "Bayesian Data Analysis",
    "section": "Recap: Output from MCMC",
    "text": "Recap: Output from MCMC\nMCMC samples are not independent draws from a target distribution:\n\nThe first draw is not a random draw from the target distribution and tuning of MCMC parameters may occur at the start of a chain.\nSubsequently, draw s + 1 depends on draw s: the samples may be autocorrelated.\n\nWe can use samples from an MCMC algorithm to do inference but ONLY IF\n\nWe exclude samples from the initial period (burn-in).\nWe ‚Äúwait long enough‚Äù to get a set of samples that are representative of the target (posterior). distribution.\n\n\n\n# Create a dataset (tibble) of samples\n# chose a \"burn-in\" \nn_burnin &lt;- 100\n# Remove the first sample from mu_s and sigma_s\npost_samps &lt;- tibble(sample_index = 1:(n_iter-n_burnin),\n                     mu_s = mu_s[-(1:n_burnin)], \n                     sigma_s = sigma_s[-(1:n_burnin)])\npost_samps\n\n\n# A tibble: 9,900 √ó 3\n   sample_index  mu_s sigma_s\n          &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1            1  87.8    21.2\n 2            2  87.7    20.1\n 3            3  86.8    20.4\n 4            4  87.0    20.0\n 5            5  89.2    21.2\n 6            6  86.4    19.6\n 7            7  87.1    19.1\n 8            8  85.7    19.8\n 9            9  85.2    20.3\n10           10  84.3    21.3\n# ‚Ñπ 9,890 more rows"
  },
  {
    "objectID": "5a-Diagnostics.html#recap-trace-plots",
    "href": "5a-Diagnostics.html#recap-trace-plots",
    "title": "Bayesian Data Analysis",
    "section": "Recap: Trace Plots",
    "text": "Recap: Trace Plots\nWe can use trace plots to investigate the MCMC samples/chains for the parameters.\n\n\nlibrary(ggplot2)\n\n# Trace plot for mu\np1 &lt;- ggplot(post_samps, aes(x = sample_index, mu_s)) +\n        geom_line(colour = \"blue\") +\n        ggtitle(\"Posterior Samples of mu\")\n\n# Trace plot for sigma\np2 &lt;- ggplot(post_samps, aes(x = sample_index, sigma_s)) +\n        geom_line(colour = \"blue\") +\n        ggtitle(\"Posterior Samples of sigma\")\n\np1;p2"
  },
  {
    "objectID": "5a-Diagnostics.html#recap-density-plots-and-summaries",
    "href": "5a-Diagnostics.html#recap-density-plots-and-summaries",
    "title": "Bayesian Data Analysis",
    "section": "Recap: Density Plots and Summaries",
    "text": "Recap: Density Plots and Summaries\nWe can visualise and obtain summaries for the marginal posterior distributions of the parameters based on the MCMC samples.\n\n\n# Density plot for posterior samples of mu\np5 &lt;- ggplot(post_samps, aes(x = mu_s)) +\n        geom_density(fill = \"lightblue\", \n                     color = \"blue\", \n                     alpha = 0.6) +\n        ggtitle(\"Posterior Density of mu\") \n\n# Mean and credible interval for mu\nmu_mean &lt;- mean(mu_s)\nmu_ci &lt;- quantile(mu_s, probs = c(0.025, 0.975))\n\np5;cat(\"Mean of Mu:\", mu_mean, \"\\n\");cat(\"95% Credible Interval for Mu:\", mu_ci, \"\\n\")\n\n\n\n\n\n\n\n\n\nMean of Mu: 86.73365 \n\n\n95% Credible Interval for Mu: 84.8307 88.6514 \n\n\n\n\np6 &lt;- ggplot(post_samps, aes(x = sigma_s)) +\n        geom_density(fill = \"lightblue\", \n                     color = \"blue\", \n                     alpha = 0.6) +\n        ggtitle(\"Posterior Density of sigma\") \n\n# Mean and credible interval for sigma\nsigma_mean &lt;- mean(sigma_s)\nsigma_ci &lt;- quantile(sigma_s, probs = c(0.025, 0.975))\n\np6;cat(\"Mean of Sigma:\", sigma_mean, \"\\n\");cat(\"95% Credible Interval for Sigma:\", sigma_ci, \"\\n\")\n\n\n\n\n\n\n\n\n\nMean of Sigma: 20.38515 \n\n\n95% Credible Interval for Sigma: 19.09044 21.81506"
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-reprentativeness-accuracy-and-efficiency",
    "href": "5a-Diagnostics.html#mcmc-reprentativeness-accuracy-and-efficiency",
    "title": "Bayesian Data Analysis",
    "section": "MCMC Reprentativeness, Accuracy and Efficiency",
    "text": "MCMC Reprentativeness, Accuracy and Efficiency\nWe have 3 main goals in generating an MCMC sample from the target (posterior) distribution.\n\nThe values must be representative of the posterior distribution. They shouldn‚Äôt be influenced by the initial values of the chain. They should explore the full range of the parameter space.\nThe chain should be a sufficient size so that estimates are accurate and stable. Estimates and uncertainty intervals should not be much different if the MCMC is run again.\nThe chain should be generated as efficiently as possible.\n\nWe cannot run chains for an infinitely long time so we must check the quality based on a set of finite samples from the chain.\nWe use a set of convergence diagnostics to check the quality."
  },
  {
    "objectID": "5a-Diagnostics.html#burn-in-and-mixing",
    "href": "5a-Diagnostics.html#burn-in-and-mixing",
    "title": "Bayesian Data Analysis",
    "section": "Burn-in and Mixing",
    "text": "Burn-in and Mixing\n\nBurn in phase = initial phase of an MCMC chain, when the chain converges away from initial values towards the target (posterior) distribution.\n\nSamples from the burn-in should be completely ignored.\n\nMixing refers to how well and how quickly a Markov chain explores and converges to the target probability distribution.\n\nWe can never be sure that a chain has converged but we can detect a lack of convergence i.e., if chains don‚Äôt overlap (don‚Äôt mix).\n\n\n\n\nHow many samples would you ‚Äúburn in‚Äù?\n\n\n\n\n\n\n\n\n\n\nWhich chains are mixing well?"
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-diagnostics-potential-scale-reduction-factor-hatr",
    "href": "5a-Diagnostics.html#mcmc-diagnostics-potential-scale-reduction-factor-hatr",
    "title": "Bayesian Data Analysis",
    "section": "MCMC diagnostics: Potential Scale Reduction Factor (\\(\\hat{R}\\))",
    "text": "MCMC diagnostics: Potential Scale Reduction Factor (\\(\\hat{R}\\))\nThe Potential Scale Reduction Factor (R-hat), also called the Gelman-Rubin diagnostic, is used to assess the convergence of multiple MCMC chains in Bayesian analysis. It compares the variance within each chain to the variance between chains, indicating whether the chains have converged to the same target distribution.\nHow R-hat Works:\n\nWithin-Chain Variance: Measures the variability of samples within a single chain.\nBetween-Chain Variance: Measures the variability between the means of multiple chains.\nIf the chains are converging well, the within-chain and between-chain variances should be similar.\n\nInterpretation:\n\nR-hat close to 1: Indicates convergence; the chains are mixing well.\nR-hat &gt; 1: Suggests that the chains have not yet converged, and more sampling may be needed."
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-diagnostics-potential-scale-reduction-factor-hatr-1",
    "href": "5a-Diagnostics.html#mcmc-diagnostics-potential-scale-reduction-factor-hatr-1",
    "title": "Bayesian Data Analysis",
    "section": "MCMC diagnostics: Potential Scale Reduction Factor (\\(\\hat{R}\\))",
    "text": "MCMC diagnostics: Potential Scale Reduction Factor (\\(\\hat{R}\\))\nThe graph above shows trace plots for three MCMC chains with an R-hat value close to 1, indicating good convergence. Each chain explores similar regions of the parameter space, reflecting similar means and variances. As a result, R-hat suggests that the chains are sampling from the same distribution effectively"
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-diagnostics-potential-scale-reduction-factor-hatr-2",
    "href": "5a-Diagnostics.html#mcmc-diagnostics-potential-scale-reduction-factor-hatr-2",
    "title": "Bayesian Data Analysis",
    "section": "MCMC diagnostics: Potential Scale Reduction Factor (\\(\\hat{R}\\))",
    "text": "MCMC diagnostics: Potential Scale Reduction Factor (\\(\\hat{R}\\))\nThe R-hat value signals that the chains haven‚Äôt converged."
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-diagnostics-potential-scale-reduction-factor-hatr-3",
    "href": "5a-Diagnostics.html#mcmc-diagnostics-potential-scale-reduction-factor-hatr-3",
    "title": "Bayesian Data Analysis",
    "section": "MCMC diagnostics: Potential Scale Reduction Factor (\\(\\hat{R}\\))",
    "text": "MCMC diagnostics: Potential Scale Reduction Factor (\\(\\hat{R}\\))\nThe R-hat value signals that the chains haven‚Äôt fully converged, even though the differences between them aren‚Äôt extreme."
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-diagnostics-autocorrelation",
    "href": "5a-Diagnostics.html#mcmc-diagnostics-autocorrelation",
    "title": "Bayesian Data Analysis",
    "section": "MCMC diagnostics: Autocorrelation",
    "text": "MCMC diagnostics: Autocorrelation\n\nMont Carlo samples are independent draws from a target distribution.\nMCMC samples are NOT independent draws from a target distribution, because:\n\nThe first draw is set by the user and thus not a random draw from the target distribution.\nSubsequently, draw \\(s+1\\) depends on draw \\(s\\) - samples are autocorrelated\n\n\nIn Bayesian analysis, autocorrelation measures how correlated samples from a MCMC chain are with their past values. High autocorrelation indicates that the samples are not exploring the parameter space effectively, which can lead to inefficient sampling and slow convergence."
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-diagnostics-autocorrelation-1",
    "href": "5a-Diagnostics.html#mcmc-diagnostics-autocorrelation-1",
    "title": "Bayesian Data Analysis",
    "section": "MCMC diagnostics: Autocorrelation",
    "text": "MCMC diagnostics: Autocorrelation\nWhy Autocorrelation Matters:\n\nEffective Sample Size: High autocorrelation reduces the effective sample size of the chain, meaning that even though you have a lot of samples, they may not provide much new information.\nMixing: It helps assess the mixing of the chains. Good mixing results in low autocorrelation, indicating that the chains are moving freely through the parameter space.\n\nHow to Compute Autocorrelation:\n\nAutocorrelation Function (ACF): You can compute the autocorrelation for different lags (how far back in the chain you want to look) and visualize it to assess the behavior of the MCMC chain.\nVisual Representation: The autocorrelation plot (ACF plot) displays the autocorrelation coefficients for various lags, helping visualize how quickly the chain becomes uncorrelated."
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-diagnostics-autocorrelation-2",
    "href": "5a-Diagnostics.html#mcmc-diagnostics-autocorrelation-2",
    "title": "Bayesian Data Analysis",
    "section": "MCMC diagnostics: Autocorrelation",
    "text": "MCMC diagnostics: Autocorrelation\nWe can use autocorrelation function (ACF) plots to help us diagnose autocorrelation issues.\nHigh autocorrelation"
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-diagnostics-autocorrelation-3",
    "href": "5a-Diagnostics.html#mcmc-diagnostics-autocorrelation-3",
    "title": "Bayesian Data Analysis",
    "section": "MCMC diagnostics: Autocorrelation",
    "text": "MCMC diagnostics: Autocorrelation\nWe can use autocorrelation function (ACF) plots to help us diagnose autocorrelation issues.\nReduced autocorrelation"
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-diagnostics-effective-sample-size-ess",
    "href": "5a-Diagnostics.html#mcmc-diagnostics-effective-sample-size-ess",
    "title": "Bayesian Data Analysis",
    "section": "MCMC diagnostics: effective sample size (ESS)",
    "text": "MCMC diagnostics: effective sample size (ESS)\nWe want to know what the sample size of a non-autocorrelated chain, that yields the same information, would be. An answer to this question can be provided with a measure called effective sample size (ESS)\nThe effective sample size (ESS) divides the actual sample size by the amount of autocorrelation.\n\\[ESS = \\frac{N}{1 + 2\\sum_{k=1}^\\infty\\rho_k}\\]\nwhere \\(\\rho_k\\) is the autocorrelation of the chain at lag \\(k\\). A good rule of thumb for the ESS is for it to be 10% of the total number of samples."
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-diagnostics-monte-carlo-standard-error-mcse",
    "href": "5a-Diagnostics.html#mcmc-diagnostics-monte-carlo-standard-error-mcse",
    "title": "Bayesian Data Analysis",
    "section": "MCMC diagnostics: Monte Carlo Standard Error (MCSE)",
    "text": "MCMC diagnostics: Monte Carlo Standard Error (MCSE)\nAnother useful measure for the effective accuracy of the chain is the Monte Carlo standard error (MCSE).\n\nThe standard deviation (SD) of the sample mean accross many replications is called the standard error and is estimated as SE = SD/\\(\\sqrt{N}\\)\nSo as the sample size \\(N\\) increases the SE decreases. In other words the bigger the sample, the more precise the estimate.\n\nExtending this to MCMC chains, we substitute the sample size N with the ESS to get\nMCSE = SD/\\(\\sqrt{\\text{ESS}}\\)\nwhere SD is the standard deviation of the chain."
  },
  {
    "objectID": "5a-Diagnostics.html#mcmc-efficiency",
    "href": "5a-Diagnostics.html#mcmc-efficiency",
    "title": "Bayesian Data Analysis",
    "section": "MCMC efficiency",
    "text": "MCMC efficiency\nThere are a number of ways to attempt to improve efficiency in the MCMC process\n\nRun chains in parallel\nAdjust the sampling algorithm e.g., use Gibbs instead of Metropolis\nChange model parameterisation (e.g., mean center the data for regression analysis)"
  },
  {
    "objectID": "5b-JAGS.html#just-another-gibbs-sampler---jags",
    "href": "5b-JAGS.html#just-another-gibbs-sampler---jags",
    "title": "Bayesian Data Analysis",
    "section": "Just Another Gibbs Sampler - JAGS",
    "text": "Just Another Gibbs Sampler - JAGS\n\nJAGS is a system that automatically builds Markov chain Monte Carlo (MCMC) samplers for Bayesian Models. It was developed by Martin Plummer.\nJAGS takes a user‚Äôs description of a Bayesian model and returns an MCMC sample of the posterior distribution for the monitored parameters.\nIt‚Äôs very easy to implement (m)any Bayesian models as long as you can write down the model specification. The user does not need to worry about the MCMC algorithm (you just have to check MCMC output for convergence/mixing).\nConveniently, there are R packages that let users run JAGS from R.\nAs a simple example we‚Äôve considered the estimation of a Binomial probability (with the happiness example). Now we‚Äôll implement this simple example using JAGS."
  },
  {
    "objectID": "5b-JAGS.html#how-to-use-jags",
    "href": "5b-JAGS.html#how-to-use-jags",
    "title": "Bayesian Data Analysis",
    "section": "How to use JAGS?",
    "text": "How to use JAGS?\n\nInstall JAGS and the R packages rjags and R2jags.\nMinimum things to do:\n\n\n\nWrite down your model specification and its corresponding JAGS model.\nSpecify data list and the names of the parameters for which you want to save the posterior samples.\nGO: Run JAGS!\nGet the posterior MCMC samples and use those for further analysis."
  },
  {
    "objectID": "5b-JAGS.html#step-1---model-specification",
    "href": "5b-JAGS.html#step-1---model-specification",
    "title": "Bayesian Data Analysis",
    "section": "Step 1 - Model Specification",
    "text": "Step 1 - Model Specification\nRecall for the Happiness example out of n = 20 women, y = 14 women reported being happy.\n\n\n\nModel specification (data model and priors): \\[\\begin{align*}\ny|\\theta &\\sim   Binomial(\\theta, N = 20) \\\\\n\\theta &\\sim Be(1,1)\\\\\n\\end{align*}\\]\n\n\n\nJAGS model specification:\n\n\nbinomialmodel = \"\nmodel{\n\n # data model (likelihood)\n    y ~ dbinom(theta, N) \n \n # prior\n theta ~ dbeta(1, 1) \n }\n \""
  },
  {
    "objectID": "5b-JAGS.html#step-2---jags-data",
    "href": "5b-JAGS.html#step-2---jags-data",
    "title": "Bayesian Data Analysis",
    "section": "Step 2 - JAGS Data",
    "text": "Step 2 - JAGS Data\n\n\n\nJAGS model specification:\n\n\nbinomialmodel = \"\nmodel{\n\n # data model (likelihood)\n    y ~ dbinom(theta, N) \n \n # prior\n theta ~ dbeta(1, 1) \n }\n \"\n\n\n\nInput data:\n\n\njags.data &lt;- list(y = 14, \n                  N = 20)\n\n\nSpecify the names of the parameters for which you want to save the posterior samples:\n\n\nparnames &lt;- c(\"theta\")"
  },
  {
    "objectID": "5b-JAGS.html#step-3---run-jags",
    "href": "5b-JAGS.html#step-3---run-jags",
    "title": "Bayesian Data Analysis",
    "section": "Step 3 - Run JAGS",
    "text": "Step 3 - Run JAGS\n\nlibrary(rjags)\nlibrary(R2jags)\nmod &lt;- jags(data = jags.data, \n            parameters.to.save = parnames, \n            model.file = textConnection(binomialmodel))\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1\n   Unobserved stochastic nodes: 1\n   Total graph size: 4\n\nInitializing model\n\n\n\nBy default, JAGS runs 3 chains with 2,000 iterations per chain of which 1,000 iterations are excluded as burnin."
  },
  {
    "objectID": "5b-JAGS.html#step-4---output",
    "href": "5b-JAGS.html#step-4---output",
    "title": "Bayesian Data Analysis",
    "section": "Step 4 - Output",
    "text": "Step 4 - Output\n\nMCMC samples can be obtained as follows:\n\n\nmcmc.array &lt;- mod$BUGSoutput$sims.array\nmcmc.array %&gt;% dim() \n\n[1] 1000    3    2\n\n\nwhere the dimension of the mcmc.array is given by niterations x nchains x (nparameters+deviance).\n\nmcmc.list &lt;- mod$BUGSoutput$sims.list\nmcmc.list$theta %&gt;% dim()\n\n[1] 3000    1\n\n\n\nmcmc.matrix &lt;- mod$BUGSoutput$sims.matrix\nmcmc.matrix %&gt;% dim()\n\n[1] 3000    2\n\n\n\nWe can also get output summaries\n\n\nmod$BUGSoutput$summary\n\n              mean         sd      2.5%       25%      50%       75%     97.5%\ndeviance 4.2373965 1.32419393 3.3052062 3.4107862 3.745141 4.5149047 8.0653926\ntheta    0.6794092 0.09703187 0.4770903 0.6123634 0.686353 0.7501163 0.8465404\n            Rhat n.eff\ndeviance 1.00742   820\ntheta    1.00202  3000"
  },
  {
    "objectID": "5b-JAGS.html#more-on-outout---basics",
    "href": "5b-JAGS.html#more-on-outout---basics",
    "title": "Bayesian Data Analysis",
    "section": "More on outout - basics",
    "text": "More on outout - basics\n\nQuick viewParameter inference\n\n\n\n\n## plot output (quick plot)\nplot(mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n## look at parameters\ntheta_s &lt;- mcmc.list$theta\n\n# plot\nplot(theta_s, type = \"l\")\n\n\n\n\n\n\n\n\n\n\n\n# plot\nplot(density(theta_s))\n\n\n\n\n\n\n\n\n\n\n\n# get quantiles\nquantile(theta_s, probs=c(0.025,0.5, 0.975))\n\n\n     2.5%       50%     97.5% \n0.4770904 0.6863530 0.8465404"
  },
  {
    "objectID": "5b-JAGS.html#trace-plots-and-density-plots-with-the-coda-package",
    "href": "5b-JAGS.html#trace-plots-and-density-plots-with-the-coda-package",
    "title": "Bayesian Data Analysis",
    "section": "Trace plots and density plots with the coda package",
    "text": "Trace plots and density plots with the coda package\nHere‚Äôs some useful functions from the coda package.\n\nlibrary(coda)\n\n# turn the model into an mcmc object\nmod_mcmc &lt;- as.mcmc(mod)\n\n# get trace plot and density\nplot(mod_mcmc)"
  },
  {
    "objectID": "5b-JAGS.html#autocorrelation-plots",
    "href": "5b-JAGS.html#autocorrelation-plots",
    "title": "Bayesian Data Analysis",
    "section": "Autocorrelation plots",
    "text": "Autocorrelation plots\n\nautocorr.plot(mod_mcmc) # get autocorrelation plot"
  },
  {
    "objectID": "5b-JAGS.html#tidybayes",
    "href": "5b-JAGS.html#tidybayes",
    "title": "Bayesian Data Analysis",
    "section": "Tidybayes",
    "text": "Tidybayes\nThe tidybayes package facilitates the use of tidy data (one observation per row) with Bayesian models in R.\n\nYou can use the output from JAGS in matrix format to work with tidybayes\nThe spread_draws function lets us extract draws into a data frame in tidy format\n\n\n\nlibrary(tidybayes)\n\n## look at samples for individual parameters (similar to sims.list but puts it in a tibble)\nmcmc.matrix &lt;- mod$BUGSoutput$sims.matrix\nmcmc.matrix %&gt;% spread_draws(theta) \n\n\n# A tibble: 3,000 √ó 4\n   .chain .iteration .draw theta\n    &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1      1          1     1 0.670\n 2      1          2     2 0.696\n 3      1          3     3 0.719\n 4      1          4     4 0.605\n 5      1          5     5 0.843\n 6      1          6     6 0.753\n 7      1          7     7 0.822\n 8      1          8     8 0.760\n 9      1          9     9 0.767\n10      1         10    10 0.573\n# ‚Ñπ 2,990 more rows"
  },
  {
    "objectID": "5b-JAGS.html#tidybayes-1",
    "href": "5b-JAGS.html#tidybayes-1",
    "title": "Bayesian Data Analysis",
    "section": "Tidybayes",
    "text": "Tidybayes\n\ntidybayes provides a family of functions for generating point summaries and intervals from draws in a tidy format. These functions follow the naming scheme [median|mean|mode]_[qi|hdi], for example, median_qi, mean_qi, mode_hdi, and so on.\n\n\n## summary for parameters\nmcmc.matrix %&gt;% \n  gather_rvars(theta) %&gt;% \n  median_qi(.value, .width = c(.95,0.9))\n\n# A tibble: 2 √ó 7\n  .variable .value .lower .upper .width .point .interval\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 theta      0.686  0.477  0.847   0.95 median qi       \n2 theta      0.686  0.510  0.826   0.9  median qi       \n\n\n\nThe ggdist::stat_halfeye() geom provides a shortcut to generating ‚Äúhalf-eye plots‚Äù (combinations of intervals and densities).\n\n\n\n## plots\nmcmc.matrix %&gt;%\n  spread_draws(theta) %&gt;% \n  ggplot(aes(x = theta)) +\n  stat_halfeye()"
  },
  {
    "objectID": "5a-Diagnostics.html#thinning",
    "href": "5a-Diagnostics.html#thinning",
    "title": "Bayesian Data Analysis",
    "section": "Thinning",
    "text": "Thinning\n\nThinning is the practice of retaining only every k-th sample from a Markov Chain in MCMC sampling.\n\nTypically used to reduce autocorrelation in the chain or manage large storage demands.\n\n\nHow Thinning Works\n\nGiven a sequence of samples: [ x_1, x_2, x_3, ‚Ä¶, x_N ]\nThinning by a factor of k: Retain [ x_k, x_{2k}, x_{3k}, ‚Ä¶ ]\nThinning factor (k): Number of iterations between successive retained samples.\n\nBest Practices\n\nAvoid thinning if possible: It can unnecessarily discard valuable samples.\nUse diagnostics like autocorrelation plots and the effective sample size (which we‚Äôll talk about next) to decide whether thinning is needed.\nIf using, choose a thinning factor based on the autocorrelation time of the chain."
  },
  {
    "objectID": "5b-JAGS.html#another-example",
    "href": "5b-JAGS.html#another-example",
    "title": "Bayesian Data Analysis",
    "section": "Another Example",
    "text": "Another Example\nAssume we have survey data from 5 countries related to the happiness of women aged 65+ and we want to compare the proportions accross countries.\n\n\n\n\n\nCountry\nN\nY\n\n\n\n\nGermany\n200\n140\n\n\nFrance\n150\n92\n\n\nItaly\n180\n122\n\n\nSpain\n170\n108\n\n\nNetherlands\n160\n116"
  },
  {
    "objectID": "5b-JAGS.html#another-example-1",
    "href": "5b-JAGS.html#another-example-1",
    "title": "Bayesian Data Analysis",
    "section": "Another Example",
    "text": "Another Example\n\nbinomialmodel = \"\nmodel{\n for(i in 1:n_country)\n{\n # data model (likelihood)\n    y[i] ~ dbinom(theta[i], N[i]) \n    \n # prior\n theta[i] ~ dbeta(1, 1) \n}\n }\n \"\n\njags.data &lt;- list(y = survey_data$Y, \n                  N = survey_data$N,\n                  n_country = length(survey_data$Country))\nparnames &lt;- c(\"theta\")\n\nlibrary(rjags)\nlibrary(R2jags)\nmod &lt;- jags(data = jags.data, \n            parameters.to.save = parnames, \n            model.file = textConnection(binomialmodel))\n\n## quick look output\nmod_mcmc &lt;- as.mcmc(mod)\nmod$BUGSoutput$summary\nplot(mod_mcmc)\n\n## tidybayes outptut\nlibrary(tidybayes)\nmcmc.matrix &lt;- mod$BUGSoutput$sims.matrix\ncountry &lt;- survey_data$Country\n\n## summary\nmcmc.matrix %&gt;%\n  spread_draws(theta[country]) %&gt;% \n  summarise_draws() \n\n## plot\nmcmc.matrix %&gt;%\n  spread_draws(theta[country]) %&gt;% \n  ggplot(aes(y = factor(country, labels = survey_data$Country), x = theta)) +\n  stat_halfeye() +\n  ylab(\"\")"
  },
  {
    "objectID": "5b-JAGS.html#another-happiness-example",
    "href": "5b-JAGS.html#another-happiness-example",
    "title": "Bayesian Data Analysis",
    "section": "Another Happiness Example",
    "text": "Another Happiness Example\nAssume we have survey data from 5 countries related to the happiness of women aged 65+ and we want to compare the proportions accross countries.\n\n\n\n\n\nCountry\nN\nY\n\n\n\n\nGermany\n200\n140\n\n\nFrance\n150\n92\n\n\nItaly\n180\n122\n\n\nSpain\n170\n108\n\n\nNetherlands\n160\n116\n\n\n\n\n\n\n\n\nModel specification (data model and priors): \\[\\begin{align*}\ny_i|\\theta_i &\\sim   Binomial(\\theta_i, N_i) \\\\\n\\theta_i &\\sim Be(1,1)\\\\\n\\end{align*}\\]\n\n\n\nJAGS model specification:\n\n\nbinomialmodel = \"\nmodel{\n for(i in 1:n_country)\n{\n # data model (likelihood)\n    y.i[i] ~ dbinom(theta.i[i], N.i[i]) \n    \n # prior\n theta.i[i] ~ dbeta(1, 1) \n}\n }\n \""
  },
  {
    "objectID": "5b-JAGS.html#another-happiness-example-rcode",
    "href": "5b-JAGS.html#another-happiness-example-rcode",
    "title": "Bayesian Data Analysis",
    "section": "Another Happiness Example: Rcode",
    "text": "Another Happiness Example: Rcode\n\njags.data &lt;- list(y.i = survey_data$Y, \n                  N.i = survey_data$N,\n                  n_country = length(survey_data$Country))\nparnames &lt;- c(\"theta.i\")\n\nlibrary(rjags)\nlibrary(R2jags)\nmod &lt;- jags(data = jags.data, \n            parameters.to.save = parnames, \n            model.file = textConnection(binomialmodel))\n\n\n## quick look output\nmod_mcmc &lt;- as.mcmc(mod)\nmod$BUGSoutput$summary\nplot(mod_mcmc)\n\n## tidybayes outptut\nlibrary(tidybayes)\nmcmc.matrix &lt;- mod$BUGSoutput$sims.matrix\ncountry &lt;- happiness_survey$Country\n\n## summary\nmcmc.matrix %&gt;%\n  spread_draws(theta.i[country]) %&gt;% \n  summarise_draws() \n\n## plot\nmcmc.matrix %&gt;%\n  spread_draws(theta.i[country]) %&gt;% \n  ggplot(aes(y = factor(country, labels = survey_data$Country), x = theta.i)) +\n  stat_halfeye() +\n  ylab(\"\")"
  },
  {
    "objectID": "Tutorial2.html",
    "href": "Tutorial2.html",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "This tutorial explores Bayesian inference using JAGS.\n\n\n\nA company is running three different email campaigns (A, B, and C) to promote a new product. The marketing team wants to estimate the click-through rates (CTR) for each campaign using a Bayesian approach and compare the effectiveness of the campaigns.\nThe data collected from these campaigns is as follows:\n\n\n\nCampaign\nEmails Sent\nEmails Clicked\n\n\n\n\nA\n100\n15\n\n\nB\n120\n20\n\n\nC\n110\n12\n\n\n\nUsing this data carry out the following tasks:\n\nModel Specification:\n\n(a) Specify an appropriate likelihood.\n(b) Specify a prior distribution for the CTR of each campaign, assuming you have no strong prior knowledge about success rates.\n\nJAGS Model:\n\n(a) Specify the JAGS model.\n(b) Implement the Bayesian model in JAGS using the data from the table. Run the MCMC sampling using 4,000 iterations and discarding the first 2,000 as burn-in.\n(c) Check for convergence.\n\nInterpretation of Results:\n\n(a) Plot the posterior distributions of the CTRs for the three campaigns.\n(b) Compare the posterior distributions and interpret what the results tell you about the relative effectiveness of the three email campaigns.\n\nAdditional Analysis:\n\n(a) Calculate the posterior probability that Campaign B has a higher CTR than Campaign A and Campaign C, and interpret this probability."
  },
  {
    "objectID": "Tutorial2.html#introduction",
    "href": "Tutorial2.html#introduction",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "This tutorial explores Bayesian inference using JAGS.\n\n\n\nA company is running three different email campaigns (A, B, and C) to promote a new product. The marketing team wants to estimate the click-through rates (CTR) for each campaign using a Bayesian approach and compare the effectiveness of the campaigns.\nThe data collected from these campaigns is as follows:\n\n\n\nCampaign\nEmails Sent\nEmails Clicked\n\n\n\n\nA\n100\n15\n\n\nB\n120\n20\n\n\nC\n110\n12\n\n\n\nUsing this data carry out the following tasks:\n\nModel Specification:\n\n(a) Specify an appropriate likelihood.\n(b) Specify a prior distribution for the CTR of each campaign, assuming you have no strong prior knowledge about success rates.\n\nJAGS Model:\n\n(a) Specify the JAGS model.\n(b) Implement the Bayesian model in JAGS using the data from the table. Run the MCMC sampling using 4,000 iterations and discarding the first 2,000 as burn-in.\n(c) Check for convergence.\n\nInterpretation of Results:\n\n(a) Plot the posterior distributions of the CTRs for the three campaigns.\n(b) Compare the posterior distributions and interpret what the results tell you about the relative effectiveness of the three email campaigns.\n\nAdditional Analysis:\n\n(a) Calculate the posterior probability that Campaign B has a higher CTR than Campaign A and Campaign C, and interpret this probability."
  },
  {
    "objectID": "6_LR.html#data-cognitive-test-scores",
    "href": "6_LR.html#data-cognitive-test-scores",
    "title": "Bayesian Data Analysis",
    "section": "Data: Cognitive Test Scores",
    "text": "Data: Cognitive Test Scores\nData are available on the cognitive test scores of three- and four-year-old children in the USA.\n\nThe sample contains 434 observations\nInformation also provided about his/her mother‚Äôs IQ and whether or not the mother graduated from highschool."
  },
  {
    "objectID": "6_LR.html#simple-linear-regression-model",
    "href": "6_LR.html#simple-linear-regression-model",
    "title": "Bayesian Data Analysis",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nWe will assume a normal model for the data such that\n\\(y_i|\\mu,\\sigma^2 \\sim N(\\mu_i, \\sigma^2)\\)\n\nLet‚Äôs start simple and consider the expected value for \\(y_i\\) as a function of one explanatory variable (mother‚Äôs IQ) such that:\n\n\\(\\mu_i = \\alpha + \\beta x_i\\)\n\nThis could also be specified as:\n\n\\(y_i = \\mu_i + \\epsilon_i\\)\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\n\nTo give the intercept more meaning we could mean center the predictor:\n\n\\(\\mu_i = \\alpha + \\beta (x_i - \\bar{x})\\)\n\n\\(\\alpha\\) is now the expected value of \\(y\\) at the average \\(x\\)."
  },
  {
    "objectID": "6_LR.html#prior-choice",
    "href": "6_LR.html#prior-choice",
    "title": "Bayesian Data Analysis",
    "section": "Prior Choice",
    "text": "Prior Choice\nWe will need to specify priors for \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\).\n\nBefore seeing the data would could assume that the value of the intercept (\\(\\alpha\\)) is unlikely to be to beyond the range of possible IQ values (1, 200). So let‚Äôs assume an expected value of 100 and a standard deviation of 30.\n\n\n\n\\(\\alpha \\sim N(100,30^2)\\)"
  },
  {
    "objectID": "6_LR.html#prior-choice-1",
    "href": "6_LR.html#prior-choice-1",
    "title": "Bayesian Data Analysis",
    "section": "Prior Choice",
    "text": "Prior Choice\nWe will need to specify priors for \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\).\n\nFor the slope (\\(\\beta\\)), before seeing the data I have no idea about whether the mother‚Äôs IQ affects the kid‚Äôs IQ, so it is reasonable to consider a value of 0 for the slope. Realistically I think we are unlikely to see the value of the slope going anywhere beyond the range (-4,4), so let‚Äôs assume a standard deviation for the slope of 2.\n\n\n\n\\(\\beta \\sim N(0,2^2)\\)"
  },
  {
    "objectID": "6_LR.html#prior-choice-2",
    "href": "6_LR.html#prior-choice-2",
    "title": "Bayesian Data Analysis",
    "section": "Prior Choice",
    "text": "Prior Choice\nFor \\(\\sigma\\) we can consider some uninformative or weakly informative priors commonly used for variance parameters:\n\nGamma prior \\(gamma(\\epsilon, \\epsilon)\\) on the precision (\\(\\tau = \\frac{1}{\\sigma^2}\\))\n\n\\(\\tau \\sim Ga(0.1,0.1)\\)\n\nUniform prior on the standard deviation\n\n\\(\\sigma \\sim U(0,50)\\)\n\nCauchy (half-t) prior on the standard deviation\n\n\\(\\sigma \\sim ht(30,10^2,1)\\)\nFor more information see this paper by Andrew Gelman."
  },
  {
    "objectID": "6_LR.html#jags-model-specification",
    "href": "6_LR.html#jags-model-specification",
    "title": "Bayesian Data Analysis",
    "section": "JAGS model specification",
    "text": "JAGS model specification\n\n\n\nModel specification (likelihood and priors):\n\nData Model (likelihood) \\[\\begin{align*}\ny_i|\\mu_i, \\sigma &\\sim   Normal(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta*(x_i - \\bar{x})\\\\\n\\end{align*}\\]\nPriors \\[\\begin{align*}\n\\alpha  &\\sim Normal(100, 30^2)\\\\\n\\beta  &\\sim Normal(0, 2^2)\\\\\n\\sigma &\\sim Uniform(0,50)\\\\\n\\end{align*}\\]\n\n\nJAGS model specification:\n\n\nlrmodel1 =\"\nmodel{\n\n# likelihood\n    for (i in 1:n){\n        y.i[i] ~ dnorm(mu.i[i], sigma^-2)\n        mu.i[i] &lt;- alpha + beta*(x.i[i] - mean(x.i))\n    }\n\n# priors\nalpha ~ dnorm(100, 30^-2) \nbeta ~ dnorm(0, 2^-2) \nsigma ~ dunif(0,50)\n}\n\""
  },
  {
    "objectID": "6_LR.html#model-fitting",
    "href": "6_LR.html#model-fitting",
    "title": "Bayesian Data Analysis",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nlibrary(rjags)\nlibrary(R2jags)\n\njags.data1 &lt;- list(y.i = kidiq$kid_score, \n                  x.i = kidiq$mom_iq, \n                  n = nrow(kidiq))\n\nparnames1 &lt;- c(\"alpha\",\"beta\",\"sigma\",\"mu.i\")\n\nmod1 &lt;- jags(data = jags.data1, \n             parameters.to.save=parnames1, \n             model.file = textConnection(lrmodel1),\n             n.iter = 4000,\n             n.burnin = 2000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 434\n   Unobserved stochastic nodes: 3\n   Total graph size: 1879\n\nInitializing model"
  },
  {
    "objectID": "6_LR.html#output---parameter-uncertainty",
    "href": "6_LR.html#output---parameter-uncertainty",
    "title": "Bayesian Data Analysis",
    "section": "Output - Parameter Uncertainty",
    "text": "Output - Parameter Uncertainty\n\nlibrary(tidybayes)\n\nmcmc.matrix1 &lt;- mod1$BUGSoutput$sims.matrix\npar_samps1 &lt;- mcmc.matrix1 %&gt;% spread_draws(alpha,beta,sigma) \npar_samps1\n\n# A tibble: 3,000 √ó 6\n   .chain .iteration .draw alpha  beta sigma\n    &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      1          1     1  88.3 0.650  18.4\n 2      1          2     2  86.6 0.663  17.8\n 3      1          3     3  86.9 0.541  18.5\n 4      1          4     4  86.6 0.565  18.6\n 5      1          5     5  85.0 0.530  18.0\n 6      1          6     6  86.6 0.610  18.2\n 7      1          7     7  87.4 0.649  18.0\n 8      1          8     8  86.1 0.578  18.1\n 9      1          9     9  85.3 0.567  18.2\n10      1         10    10  87.7 0.603  17.9\n# ‚Ñπ 2,990 more rows\n\npar_summary1 &lt;- mcmc.matrix1 %&gt;% \n                gather_rvars(alpha,beta,sigma) %&gt;% \n                median_qi(.value, .width = c(.95,0.9))\npar_summary1\n\n# A tibble: 6 √ó 7\n  .variable .value .lower .upper .width .point .interval\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 alpha     86.8   85.1   88.5     0.95 median qi       \n2 beta       0.614  0.496  0.728   0.95 median qi       \n3 sigma     18.3   17.1   19.6     0.95 median qi       \n4 alpha     86.8   85.4   88.3     0.9  median qi       \n5 beta       0.614  0.512  0.707   0.9  median qi       \n6 sigma     18.3   17.3   19.4     0.9  median qi"
  },
  {
    "objectID": "6_LR.html#output---parameter-uncertainty-1",
    "href": "6_LR.html#output---parameter-uncertainty-1",
    "title": "Bayesian Data Analysis",
    "section": "Output - parameter Uncertainty",
    "text": "Output - parameter Uncertainty"
  },
  {
    "objectID": "6_LR.html#posterior-draws-of-mu_i",
    "href": "6_LR.html#posterior-draws-of-mu_i",
    "title": "Bayesian Data Analysis",
    "section": "Posterior Draws of \\(\\mu_i\\)",
    "text": "Posterior Draws of \\(\\mu_i\\)\nFor each sample draw (simulation) \\(s= 1, \\ldots , S\\) of the parameters from the posterior distribution we can obtain \\(\\mu_i^{(s)} = \\beta_0^{(s)} + \\beta_1^{(s)}(x_{i} - \\bar{x})\\)\n\n\n\n\n# A tibble: 1,302,000 √ó 5\n# Groups:   mu_ind [434]\n   mu_ind  mu.i .chain .iteration .draw\n    &lt;int&gt; &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;\n 1      1 102.       1          1     1\n 2      1 101.       1          2     2\n 3      1  98.4      1          3     3\n 4      1  98.5      1          4     4\n 5      1  96.2      1          5     5\n 6      1  99.5      1          6     6\n 7      1 101.       1          7     7\n 8      1  98.3      1          8     8\n 9      1  97.3      1          9     9\n10      1 100.       1         10    10\n# ‚Ñπ 1,301,990 more rows\n\n\n\n5 posterior draws"
  },
  {
    "objectID": "6_LR.html#posterior-draws-of-mu_i-1",
    "href": "6_LR.html#posterior-draws-of-mu_i-1",
    "title": "Bayesian Data Analysis",
    "section": "Posterior Draws of \\(\\mu_i\\)",
    "text": "Posterior Draws of \\(\\mu_i\\)\nFor each draw (simulation) \\(s= 1, \\ldots , S\\) of the parameters from the posterior distribution we can obtain \\(\\mu_i^{(s)} = \\beta_0^{(s)} + \\beta_1^{(s)}(x_{i} - \\bar{x})\\)\n\n\n\n\n# A tibble: 1,302,000 √ó 5\n# Groups:   mu_ind [434]\n   mu_ind  mu.i .chain .iteration .draw\n    &lt;int&gt; &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt;\n 1      1 102.       1          1     1\n 2      1 101.       1          2     2\n 3      1  98.4      1          3     3\n 4      1  98.5      1          4     4\n 5      1  96.2      1          5     5\n 6      1  99.5      1          6     6\n 7      1 101.       1          7     7\n 8      1  98.3      1          8     8\n 9      1  97.3      1          9     9\n10      1 100.       1         10    10\n# ‚Ñπ 1,301,990 more rows\n\n\n\n200 posterior draws"
  },
  {
    "objectID": "6_LR.html#summary-for-mu_i---point-estimate-credible-interval",
    "href": "6_LR.html#summary-for-mu_i---point-estimate-credible-interval",
    "title": "Bayesian Data Analysis",
    "section": "Summary for \\(\\mu_i\\) - Point Estimate + Credible Interval",
    "text": "Summary for \\(\\mu_i\\) - Point Estimate + Credible Interval\n\n\nmu_ind &lt;- 1:nrow(kidiq)\nmu_samps1 &lt;- mcmc.matrix1 %&gt;% spread_draws(mu.i[mu_ind]) \nmu_summary1 &lt;- mu_samps1 %&gt;% \n                median_qi(.width = 0.95)\nmu_summary1\n\n\n# A tibble: 434 √ó 7\n   mu_ind  mu.i .lower .upper .width .point .interval\n    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1      1  99.7   96.7  103.    0.95 median qi       \n 2      2  80.3   78.2   82.4   0.95 median qi       \n 3      3  96.2   93.8   98.7   0.95 median qi       \n 4      4  86.5   84.7   88.2   0.95 median qi       \n 5      5  82.4   80.4   84.3   0.95 median qi       \n 6      6  91.6   89.7   93.6   0.95 median qi       \n 7      7 111.   106.   115.    0.95 median qi       \n 8      8 102.    98.8  106.    0.95 median qi       \n 9      9  75.5   72.8   78.3   0.95 median qi       \n10     10  83.8   81.9   85.6   0.95 median qi       \n# ‚Ñπ 424 more rows\n\n\n\n\nkidiq &lt;- kidiq %&gt;% mutate(mu.i = mu_summary1$mu.i,\n                          lower = mu_summary1$.lower,\n                          upper = mu_summary1$.upper)\n\nggplot(kidiq, aes(x = mom_iq, y = kid_score)) +\n  geom_point() +\n  geom_line(aes(x = mom_iq, y = mu.i),colour = \"blue\") +\n  geom_ribbon(aes(ymin = lower, ymax = upper, fill = \"CI\"), alpha = 0.3) +\n  theme_bw() +\n  labs(fill = \"\") +\n  ylab(\"Kid's IQ\") +\n  xlab(\"Mother's IQ\")"
  },
  {
    "objectID": "6_LR.html#predictive-distribution",
    "href": "6_LR.html#predictive-distribution",
    "title": "Bayesian Data Analysis",
    "section": "Predictive Distribution",
    "text": "Predictive Distribution\nThe posterior predictive distribution is the distribution of possible unobserved values conditional on the observed values."
  },
  {
    "objectID": "6_LR.html#bayesian-credible-intervals",
    "href": "6_LR.html#bayesian-credible-intervals",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Credible intervals",
    "text": "Bayesian Credible intervals\n\n\n\n\n\n\nBayesian Credible Interval\n\n\nA Bayesian credible interval represents the range within which a parameter value (such as a mean, a proportion, or a regression coefficient) is expected to fall with a specified probability, given the observed data.\n\n\n\n\nInterpretation: If we calculate a 95% credible interval for a parameter, we can interpret this as having a 95% probability that the parameter lies within that interval, conditional on the observed data and the chosen prior.\nUsage: Bayesian credible intervals are particularly used to estimate the uncertainty of unknown model parameters (e.g., the mean of a distribution, a regression coefficient).\n\nFor example, if we calculate a 95% credible interval for a mean \\(\\mu\\) Kid IQ given the Mothers IQ is 120, it means there‚Äôs a 95% probability that \\(mu\\) is within this interval, given the observed data and the prior belief about \\(\\mu\\)."
  },
  {
    "objectID": "6_LR.html#bayesian-prediction-intervals",
    "href": "6_LR.html#bayesian-prediction-intervals",
    "title": "Bayesian Data Analysis",
    "section": "Bayesian Prediction Intervals",
    "text": "Bayesian Prediction Intervals\n\n\n\n\n\n\nPrediction Interval\n\n\nA prediction interval represents the range within which a future, single observation is expected to fall, given the current model and data.\n\n\n\n\nInterpretation: A 95% prediction interval indicates that there is a 95% probability that a new, future observation will lie within this interval, taking into account both the uncertainty about the parameter estimates and the natural variability in future data points.\nUsage: Prediction intervals are often used to estimate the range in which future values of a variable are likely to fall, such as forecasting sales figures, temperatures, or test scores.\n\nFor example, if we have a 95% prediction interval for a future observation \\(y\\) in a Bayesian setting, it means there is a 95% probability that a new data point, drawn under the same conditions, will lie within this interval."
  },
  {
    "objectID": "6_LR.html#key-differences-between-bayesian-credible-intervals-and-prediction-intervals",
    "href": "6_LR.html#key-differences-between-bayesian-credible-intervals-and-prediction-intervals",
    "title": "Bayesian Data Analysis",
    "section": "Key Differences Between Bayesian Credible Intervals and Prediction Intervals",
    "text": "Key Differences Between Bayesian Credible Intervals and Prediction Intervals\n\n\n\n\n\n\n\n\nAspect\nBayesian Credible Interval\nPrediction Interval\n\n\n\n\nPurpose\nTo estimate the range of a parameter with specified credibility\nTo predict the range of a future observation\n\n\nFocus\nThe value of the parameter, given observed data\nA single future observation, given the model and data\n\n\nVariability Source\nOnly parameter uncertainty, conditioned on data and prior beliefs\nBoth parameter uncertainty and data variability\n\n\nInterpretation\nProbability of the parameter being within the interval\nProbability of a future observation being within the interval\n\n\nApplication\nParameter estimation in Bayesian inference\nForecasting or prediction for future observations"
  },
  {
    "objectID": "6_LR.html#add-a-predictive-distribution-to-jags",
    "href": "6_LR.html#add-a-predictive-distribution-to-jags",
    "title": "Bayesian Data Analysis",
    "section": "Add a predictive distribution to JAGS",
    "text": "Add a predictive distribution to JAGS\nFor a single \\(x_{pred} = 120\\)\n\nlrmodel1 =\"\nmodel{\n....\n## predictive distribution\n  mu_pred &lt;- alpha + beta*(120 - mean(x.i))\n    ytilde ~ dnorm(mu_pred, sigma^-2)\n}\n\"\n\nFor multiple \\(x_{pred} = 120,130,140,150\\)\n\nlrmodel1 =\"\nmodel{\n....\n## predictive distribution\n\nfor(j in 1:N_pred)\n{\n  mu_pred[j] &lt;- alpha + beta*(x_pred[j] - mean(x.i))\n    ytilde[j] ~ dnorm(mu_pred[j], sigma^-2)\n}\n}\n\""
  },
  {
    "objectID": "6_LR.html#credible-interval-vs-prediction-interval",
    "href": "6_LR.html#credible-interval-vs-prediction-interval",
    "title": "Bayesian Data Analysis",
    "section": "Credible interval vs prediction interval",
    "text": "Credible interval vs prediction interval\nLet‚Äôs consider the difference between the credible interval and the prediction interval for the Kid‚Äôs IQ when the Mother‚Äôs IQ = 120.\n\npar_samps1 &lt;- mcmc.matrix1 %&gt;% spread_draws(mu_pred,ytilde) \npar_samps1\n\n# A tibble: 3,000 √ó 5\n   .chain .iteration .draw mu_pred ytilde\n    &lt;int&gt;      &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1      1          1     1    99.5  102. \n 2      1          2     2    97.5   78.1\n 3      1          3     3    98.6   84.4\n 4      1          4     4    99.6   93.7\n 5      1          5     5    99.7   74.7\n 6      1          6     6    99.4   73.5\n 7      1          7     7    99.5   85.0\n 8      1          8     8   101.   122. \n 9      1          9     9    95.7  101. \n10      1         10    10    98.9  103. \n# ‚Ñπ 2,990 more rows\n\npar_summary1 &lt;- mcmc.matrix1 %&gt;% \n                gather_rvars(mu_pred,ytilde) %&gt;% \n                median_qi(.value)\npar_summary1\n\n# A tibble: 2 √ó 7\n  .variable .value .lower .upper .width .point .interval\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 mu_pred     99.0   96.1   102.   0.95 median qi       \n2 ytilde      99.0   62.0   135.   0.95 median qi       \n\n\nWhat do you notice about the width of the intervals?"
  },
  {
    "objectID": "6_LR.html#extending-the-regression-model",
    "href": "6_LR.html#extending-the-regression-model",
    "title": "Bayesian Data Analysis",
    "section": "Extending the regression model",
    "text": "Extending the regression model\nSuppose now we want to see if there‚Äôs a ‚Äúhighschool‚Äù effect on the relationship between Mother‚Äôs IQ and Kid‚Äôs IQ.\nSome possible choices:\n\nModel 2A: Add the highschool variable as a grouping variable, such that you have a varying intercepts model with \\(\\alpha_1\\) for highschool = no and \\(\\alpha_2\\) for highschool = yes.\nModel 2B: Add the highschool variable in as a grouping variable such that you have a varying intercepts and slopes model with \\(\\alpha_1\\) and \\(\\beta_1\\) for highschool = no and \\(\\alpha_2\\) and \\(\\beta_2\\) for highscool = yes.\n\nLet‚Äôs consider model 2B. The specification of this model could be written as\n\\(y_i|\\mu,\\sigma^2 \\sim N(\\mu_i, \\sigma^2)\\)\n\\(\\mu_i = \\underset{\\text{j[i] = HS for obs i}}{\\alpha_{j[i]} + \\beta_{j[i]}}(x_{i} - \\bar{x})\\)\n\\(\\alpha_j \\sim N(100,30^2), \\hspace{0.5em} \\text{for } j=1,2\\)\n\\(\\beta_j \\sim N(0,2^2), \\hspace{0.5em} \\text{for } j=1,2\\)"
  },
  {
    "objectID": "6_LR.html#jags-model-specification-1",
    "href": "6_LR.html#jags-model-specification-1",
    "title": "Bayesian Data Analysis",
    "section": "JAGS model specification",
    "text": "JAGS model specification\n\n\n\\(y_i|\\mu,\\sigma^2 \\sim N(\\mu_i, \\sigma^2)\\)\n\\(\\mu_i = \\underset{\\text{j[i] = HS for obs i}}{\\alpha_{j[i]} + \\beta_{j[i]}}(x_{i} - \\bar{x})\\)\n\\(\\alpha_j \\sim N(100,30^2), \\hspace{0.5em} \\text{for } j=1,2\\)\n\\(\\beta_j \\sim N(0,2^2), \\hspace{0.5em} \\text{for } j=1,2\\)\n\\(\\sigma \\sim U(0,50)\\)\n\n\nlrmodel2 =\"\nmodel{\n# likelihood\n    for (i in 1:n){\n        y.i[i] ~ dnorm(mu.i[i], sigma^-2)\n        mu.i[i] &lt;- alpha.j[hs_index[i]] + beta.j[hs_index[i]]*(x.i[i] - mean(x.i))\n    }\n\n#Priors\nfor(j in 1:m)\n{\nalpha.j[j] ~ dnorm(80, 30^-2) \nbeta.j[j] ~ dnorm(0, 2^-2) \n}\nsigma ~ dunif(0,50)\n}\n\""
  },
  {
    "objectID": "6_LR.html#model-fitting-1",
    "href": "6_LR.html#model-fitting-1",
    "title": "Bayesian Data Analysis",
    "section": "Model Fitting",
    "text": "Model Fitting\n\nlibrary(rjags)\nlibrary(R2jags)\n\njags.data2 &lt;- list(y.i = kidiq$kid_score, \n                  x.i = kidiq$mom_iq, \n                  hs_index = as.numeric(kidiq$mom_hs + 1),\n                  n = nrow(kidiq),\n                  m = 2)\n\nparnames2 &lt;- c(\"alpha.j\",\"beta.j\",\"sigma\",\"mu.i\")\nmod2 &lt;- jags(data = jags.data2, \n            parameters.to.save=parnames2, \n            model.file = textConnection(lrmodel2),\n            n.iter = 4000,\n            n.burnin = 2000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 434\n   Unobserved stochastic nodes: 5\n   Total graph size: 2376\n\nInitializing model"
  },
  {
    "objectID": "6_LR.html#output",
    "href": "6_LR.html#output",
    "title": "Bayesian Data Analysis",
    "section": "Output",
    "text": "Output\n\nmcmc.matrix2 &lt;- mod2$BUGSoutput$sims.matrix\n\npar_summary2 &lt;- mcmc.matrix2 %&gt;% \n                gather_rvars(alpha.j[1:2],beta.j[1:2]) %&gt;% \n                median_qi(.value)\npar_summary2\n\n# A tibble: 4 √ó 8\n  `1:2` .variable .value .lower .upper .width .point .interval\n  &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1     1 alpha.j   85.4   80.9   89.7     0.95 median qi       \n2     2 alpha.j   88.2   86.3   90.2     0.95 median qi       \n3     1 beta.j     0.967  0.672  1.25    0.95 median qi       \n4     2 beta.j     0.485  0.356  0.619   0.95 median qi       \n\nmu_ind &lt;- 1:nrow(kidiq)\nmu_samps2 &lt;- mcmc.matrix2 %&gt;% spread_draws(mu.i[mu_ind]) \nmu_summary2 &lt;- mu_samps2 %&gt;% \n                median_qi(.width = 0.95)\n\nmu_summary2\n\n# A tibble: 434 √ó 7\n   mu_ind  mu.i .lower .upper .width .point .interval\n    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1      1  98.5   95.3  102.    0.95 median qi       \n 2      2  83.1   80.4   85.7   0.95 median qi       \n 3      3  95.7   93.1   98.3   0.95 median qi       \n 4      4  88.0   86.0   89.9   0.95 median qi       \n 5      5  84.7   82.4   87.1   0.95 median qi       \n 6      6  93.0   86.9   99.0   0.95 median qi       \n 7      7 107.   102.   112.    0.95 median qi       \n 8      8 100.    96.9  104.    0.95 median qi       \n 9      9  79.3   75.9   82.6   0.95 median qi       \n10     10  85.8   83.7   88.0   0.95 median qi       \n# ‚Ñπ 424 more rows"
  },
  {
    "objectID": "6_LR.html#visualising-the-results",
    "href": "6_LR.html#visualising-the-results",
    "title": "Bayesian Data Analysis",
    "section": "Visualising the results",
    "text": "Visualising the results\n\nkidiq &lt;- kidiq %&gt;% mutate(mu.i = mu_summary2$mu.i,\n                          lower = mu_summary2$.lower,\n                          upper = mu_summary2$.upper,\n                          mom_hs = factor(mom_hs, \n                                          labels = c(\"no\", \"yes\")))\n\nggplot(kidiq, aes(x = mom_iq, y = kid_score, colour = mom_hs)) +\n  geom_point(alpha = 0.3) +\n  geom_line(aes(x = mom_iq, y = mu.i)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +\n  theme_bw() +\n  labs(colour = \"Highschool\") +\n  ylab(\"Kid's IQ\") +\n  xlab(\"Mother's IQ\")"
  },
  {
    "objectID": "6_LR.html#which-model-do-we-choose",
    "href": "6_LR.html#which-model-do-we-choose",
    "title": "Bayesian Data Analysis",
    "section": "Which model do we choose?",
    "text": "Which model do we choose?\nFirstly, let‚Äôs have a look at the residuals for each model"
  },
  {
    "objectID": "6_LR.html#which-model-do-we-choose-1",
    "href": "6_LR.html#which-model-do-we-choose-1",
    "title": "Bayesian Data Analysis",
    "section": "Which model do we choose?",
    "text": "Which model do we choose?\nLet‚Äôs also consider the observed values vs the model-based estimates."
  },
  {
    "objectID": "6_LR.html#model-information-criteria",
    "href": "6_LR.html#model-information-criteria",
    "title": "Bayesian Data Analysis",
    "section": "Model information criteria",
    "text": "Model information criteria\nYou might have come across these before: Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC)\n\nThe general idea is that the score on the likelihood is a good measure of model fit, except for the fact that more complex models will generally have higher likelihood scores\nIf we penalise these scores by some measure of the complexity of the model then we can compare models across complexities\nThe usual measure of complexity is some function of the number of parameters\nBecause these are relative model comparisons, the best model according to an IC might still be useless."
  },
  {
    "objectID": "6_LR.html#model-information-criteria-1",
    "href": "6_LR.html#model-information-criteria-1",
    "title": "Bayesian Data Analysis",
    "section": "Model information criteria",
    "text": "Model information criteria\nTo calculate an IC, the likelihood score gets transformed into the deviance (remember JAGS monitors the ‚Äúdeviance‚Äù parameter), which is minus twice the log-likelihood score and then add a model complexity term is added.\n\nThe two most common ICs are:\n\nAIC : -2 log \\(\\hat{L}\\) + 2p\nBIC : -2 log \\(\\hat{L}\\) + p log(n)\nwhere p is the number of parameters and n is the number of observations\n\nSmaller values indicate the preferred model."
  },
  {
    "objectID": "6_LR.html#model-information-criteria-2",
    "href": "6_LR.html#model-information-criteria-2",
    "title": "Bayesian Data Analysis",
    "section": "Model information criteria",
    "text": "Model information criteria\nFor Bayesian models it‚Äôs hard to know which value of L to use, seeing as at each iteration we get a different likelihood score. Two specific versions of IC have been developed.\n\nThe first, called the Deviance Information Criteria (DIC) is calculated via:\n\nDIC: -2 log \\(p(y|\\hat{\\theta}) + 2 p_D\\)\nwhere \\(p_D\\) = 2(log \\(p(y|\\hat{\\theta})\\) \\(- E_{post}\\)log \\(p(y|\\theta))\\) is the effective number of parameters\n\nThe second called the Widely Applicable Information Criterion (WAIC) which is calculated as:\n\nWAIC: -2 log \\(p(y|{\\hat\\theta}) + 2 p_{WAIC}\\)\nHere \\(p_{WAIC}\\) - the effective number of parameters - is a measure of the variability of the likelihood scores"
  },
  {
    "objectID": "6_LR.html#which-ic-to-use",
    "href": "6_LR.html#which-ic-to-use",
    "title": "Bayesian Data Analysis",
    "section": "Which IC to use?",
    "text": "Which IC to use?\n\nWAIC and DIC are built for Bayesian hierarchical models\nDIC is included by default in the R2jags package\nWAIC is included in the loo package which is installed alongside Stan\nWAIC is considered superior as it also provides uncertainties on the values. Most of the others just give a single value"
  },
  {
    "objectID": "6_LR.html#obtaining-dic-and-waic-from-jags",
    "href": "6_LR.html#obtaining-dic-and-waic-from-jags",
    "title": "Bayesian Data Analysis",
    "section": "Obtaining DIC and WAIC from JAGS",
    "text": "Obtaining DIC and WAIC from JAGS\n\nDIC is easy to obtain from JAGS\n\n\nDIC.m1 = mod1$BUGSoutput$DIC\nDIC.m1\n\n[1] 3757.415\n\nDIC.m2 = mod2$BUGSoutput$DIC\nDIC.m2\n\n[1] 3745.806\n\n\n\nWAIC takes a little bit more work but there‚Äôs some code here that illustrates how to do it.\n\n\n\n  waic1 p_waic1 \n 3757.2     2.9 \n\n\n  waic2 p_waic2 \n 3744.9     4.9"
  },
  {
    "objectID": "6_LR.html#cross-validation",
    "href": "6_LR.html#cross-validation",
    "title": "Bayesian Data Analysis",
    "section": "Cross Validation",
    "text": "Cross Validation\nCross Validation (CV) is a method for assessing model performance by evaluating how well it generalizes to new, unseen data.\nHow Cross Validation Works:\n\nPartition the Data: Split the dataset into two parts: training and validation.\nTrain the Model: Fit the model on the training subset.\nPredict on Validation Set: Use the trained model to predict values for the validation subset.\nEvaluate Performance: Compare predicted values with the actual values from the validation set to assess model accuracy.\n\nTypes of Cross Validation:\n\nK-Fold Cross Validation:\n\nSplit the data into \\(k\\) equal-sized ‚Äúfolds.‚Äù\nTrain the model \\(k\\) times, each time leaving out one fold as the validation set and training on the remaining \\(k - 1\\) folds.\nAverage the performance across all folds for a more stable estimate.\n\nLeave-One-Out Cross Validation (LOO-CV):\n\nFor small datasets, use LOO-CV, where each data point is left out one at a time as the validation set.\nThis maximizes data usage for training, though it can be computationally intensive for large datasets."
  },
  {
    "objectID": "Tutorial2_soln.html",
    "href": "Tutorial2_soln.html",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "The likelihood for each campaign \\(i\\) can be modeled as a binomial distribution:\n\n\\(\\text{y}_i \\sim \\text{Binomial}(\\text{N}_i, \\theta_i)\\)\nwhere:\n\n\\(\\text{N}_i\\) is the number of emails sent for campaign \\(i\\),\n\\(\\text{y}_i\\) is the number of emails clicked for campaign \\(i\\),\n\\(\\theta_i\\) is the click-through rate (CTR) for campaign \\(i\\).\n\n\nWe assume a Beta(1,1) prior for each \\(\\theta_i\\), which represents no strong prior belief about the CTR:\n\n\\(\\theta_i \\sim \\text{Beta}(1, 1)\\)\nThis is a uniform prior on the interval [0, 1].\n\n\n\n\nHere is the corresponding JAGS code for this model:\n\nCTR_model &lt;- \"\nmodel {\n  # Loop over the three campaigns\n  for (i in 1:n_campaign) {\n    # Likelihood: binomial distribution for the number of clicks\n    y.i[i] ~ dbin(theta.i[i], N.i[i])\n    \n    # Prior: Beta(1,1) distribution for the CTR (uniform prior)\n    theta.i[i] ~ dbeta(1, 1)\n  }\n}\n\"\n\nTo implement the JAGS model we need data and paramters to monitor. We can also specify initial values if we wish (JAGS will also do this by default).\n\n\nData Input for JAGS:\n\nCTR_data &lt;- list(\n  y.i = c(15, 20, 12),   # Number of clicks for campaigns A, B, C\n  N.i = c(100, 120, 110), # Number of emails sent for campaigns A, B, C\n  n_campaign = 3\n)\n\nInitial Values and Parameters to Monitor:\n\ninits &lt;- function() list(theta.i = runif(3)) # Random initial values for CTRs\nparams &lt;- c(\"theta.i\") # Monitor the CTRs\n\nPutting all of this together and running JAGS\n\n\nlibrary(rjags)\nlibrary(R2jags)\n\nCTR_model &lt;- \"\nmodel {\n  # Loop over the three campaigns\n  for (i in 1:n_campaign) {\n    # Likelihood: binomial distribution for the number of clicks\n    y.i[i] ~ dbin(theta.i[i], N.i[i])\n    \n    # Prior: Beta(1,1) distribution for the CTR (uniform prior)\n    theta.i[i] ~ dbeta(1, 1)\n  }\n}\n\"\nCTR_data &lt;- list(\n  y.i = c(15, 20, 12),   # Number of clicks for campaigns A, B, C\n  N.i = c(100, 120, 110), # Number of emails sent for campaigns A, B, C\n  n_campaign = 3\n)\n\ninits &lt;- function() list(theta.i = runif(3)) # Random initial values for CTRs\nparams &lt;- c(\"theta.i\") # Monitor the CTRs\n\nmod &lt;- jags(data = CTR_data, \n            inits = inits, \n            parameters.to.save = params, \n            n.iter = 4000,\n            n.burnin = 2000, \n            model.file = textConnection(CTR_model))\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 3\n   Unobserved stochastic nodes: 3\n   Total graph size: 11\n\nInitializing model\n\n\n\nTo check for convergence we can have a quick look at the summary output and trace plots.\n\n\nmod$BUGSoutput$summary\n\n                 mean         sd        2.5%         25%        50%        75%\ndeviance   16.3320055 2.44538944 13.50888221 14.54302864 15.6968084 17.4924310\ntheta.i[1]  0.1569582 0.03627385  0.09141125  0.13132166  0.1551643  0.1799077\ntheta.i[2]  0.1734848 0.03490499  0.11116134  0.14889168  0.1717371  0.1955938\ntheta.i[3]  0.1156323 0.03047561  0.06594569  0.09329301  0.1128922  0.1347232\n                97.5%     Rhat n.eff\ndeviance   22.8870643 1.006682   540\ntheta.i[1]  0.2329834 1.002841  1200\ntheta.i[2]  0.2478510 1.001819  1500\ntheta.i[3]  0.1822796 1.001412  2200\n\n\n\nRhat values and ESS look good.\n\n\nlibrary(coda)\n# turn the model into an mcmc object\nmod_mcmc &lt;- as.mcmc(mod)\n# get trace plot and density\nplot(mod_mcmc) \n\n\n\n\n\n\n\n\n\n\n\n\nAfter running the MCMC sampling (e.g., using 4,000 iterations and discarding the first 2,000 as burn-in), you will obtain samples from the posterior distributions of \\(\\theta_i\\).\n(a) Plot the posterior distributions of the CTRs for the three campaigns using a package like tidybayes in R (or coda or the base plotting functions).\n\n\nlibrary(tidybayes)\nlibrary(tidyverse)\n## get the output in matrix format\nmcmc.matrix &lt;- mod$BUGSoutput$sims.matrix\n## get indexes for theta\ncampaign_index &lt;- 1:3\n\n## plots\nmcmc.matrix %&gt;%\n  spread_draws(theta.i[campaign_index]) %&gt;% \n  ggplot(aes(y = factor(campaign_index, labels = c(\"A\",\"B\", \"C\")), x = theta.i)) +\n  stat_halfeye() +\n  ylab(\"Campaign\")\n\n\n\n\n\n\n\n\n\n(b) Compare the distributions to assess the relative effectiveness of the campaigns. It appears that the posterior distribution for \\(\\theta_B\\) is shifted to the right compared to \\(\\theta_A\\) and \\(\\theta_C\\), suggesting that Campaign B is likely more effective, i.e., the click through rate is estimated to be higher. Note however, there is an overlap in terms of uncertainty in the estimated click through rate.\n\n\n\n\n\n(a) Calculate the posterior probability that Campaign B has a higher CTR than Campaign A and Campaign C:\n\n\\(P(\\theta_B &gt; \\theta_A) = \\frac{1}{S} \\sum_{s=1}^{S} I(\\theta_B^{(s)} &gt; \\theta_A^{(s)})\\)\nwhere \\(S\\) is the number of posterior samples, and \\(I(\\cdot)\\) is an indicator function. You can do similar calculations for \\(P(\\theta_B &gt; \\theta_C)\\).\nWe can implement this in R as follows\n\ntheta_samps &lt;- mcmc.matrix %&gt;%\n                spread_draws(theta.i[campaign_index]) \n\ntheta_A &lt;- theta_samps %&gt;% filter(campaign_index == 1) %&gt;% pull(theta.i)\ntheta_B &lt;- theta_samps %&gt;% filter(campaign_index == 2) %&gt;% pull(theta.i)\ntheta_C &lt;- theta_samps %&gt;% filter(campaign_index == 3) %&gt;% pull(theta.i)\n\nsum(theta_B &gt; theta_A)/length(theta_B)\n\n[1] 0.635\n\nsum(theta_B &gt; theta_C)/length(theta_B)\n\n[1] 0.8913333\n\n\nInterpretation:\n\nThe posterior probability that \\(\\theta_B &gt; \\theta_A\\) is 0.61, indicating that there is a 61% chance Campaign B has a higher CTR than Campaign A.\nThe posterior probability that \\(\\theta_B &gt; \\theta_C\\) is 0.89, indicating that there is a 89% chance Campaign B has a higher CTR than Campaign C."
  },
  {
    "objectID": "Tutorial2_soln.html#solutions",
    "href": "Tutorial2_soln.html#solutions",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "The likelihood for each campaign \\(i\\) can be modeled as a binomial distribution:\n\n\\(\\text{y}_i \\sim \\text{Binomial}(\\text{N}_i, \\theta_i)\\)\nwhere:\n\n\\(\\text{N}_i\\) is the number of emails sent for campaign \\(i\\),\n\\(\\text{y}_i\\) is the number of emails clicked for campaign \\(i\\),\n\\(\\theta_i\\) is the click-through rate (CTR) for campaign \\(i\\).\n\n\nWe assume a Beta(1,1) prior for each \\(\\theta_i\\), which represents no strong prior belief about the CTR:\n\n\\(\\theta_i \\sim \\text{Beta}(1, 1)\\)\nThis is a uniform prior on the interval [0, 1].\n\n\n\n\nHere is the corresponding JAGS code for this model:\n\nCTR_model &lt;- \"\nmodel {\n  # Loop over the three campaigns\n  for (i in 1:n_campaign) {\n    # Likelihood: binomial distribution for the number of clicks\n    y.i[i] ~ dbin(theta.i[i], N.i[i])\n    \n    # Prior: Beta(1,1) distribution for the CTR (uniform prior)\n    theta.i[i] ~ dbeta(1, 1)\n  }\n}\n\"\n\nTo implement the JAGS model we need data and paramters to monitor. We can also specify initial values if we wish (JAGS will also do this by default).\n\n\nData Input for JAGS:\n\nCTR_data &lt;- list(\n  y.i = c(15, 20, 12),   # Number of clicks for campaigns A, B, C\n  N.i = c(100, 120, 110), # Number of emails sent for campaigns A, B, C\n  n_campaign = 3\n)\n\nInitial Values and Parameters to Monitor:\n\ninits &lt;- function() list(theta.i = runif(3)) # Random initial values for CTRs\nparams &lt;- c(\"theta.i\") # Monitor the CTRs\n\nPutting all of this together and running JAGS\n\n\nlibrary(rjags)\nlibrary(R2jags)\n\nCTR_model &lt;- \"\nmodel {\n  # Loop over the three campaigns\n  for (i in 1:n_campaign) {\n    # Likelihood: binomial distribution for the number of clicks\n    y.i[i] ~ dbin(theta.i[i], N.i[i])\n    \n    # Prior: Beta(1,1) distribution for the CTR (uniform prior)\n    theta.i[i] ~ dbeta(1, 1)\n  }\n}\n\"\nCTR_data &lt;- list(\n  y.i = c(15, 20, 12),   # Number of clicks for campaigns A, B, C\n  N.i = c(100, 120, 110), # Number of emails sent for campaigns A, B, C\n  n_campaign = 3\n)\n\ninits &lt;- function() list(theta.i = runif(3)) # Random initial values for CTRs\nparams &lt;- c(\"theta.i\") # Monitor the CTRs\n\nmod &lt;- jags(data = CTR_data, \n            inits = inits, \n            parameters.to.save = params, \n            n.iter = 4000,\n            n.burnin = 2000, \n            model.file = textConnection(CTR_model))\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 3\n   Unobserved stochastic nodes: 3\n   Total graph size: 11\n\nInitializing model\n\n\n\nTo check for convergence we can have a quick look at the summary output and trace plots.\n\n\nmod$BUGSoutput$summary\n\n                 mean         sd        2.5%         25%        50%        75%\ndeviance   16.3320055 2.44538944 13.50888221 14.54302864 15.6968084 17.4924310\ntheta.i[1]  0.1569582 0.03627385  0.09141125  0.13132166  0.1551643  0.1799077\ntheta.i[2]  0.1734848 0.03490499  0.11116134  0.14889168  0.1717371  0.1955938\ntheta.i[3]  0.1156323 0.03047561  0.06594569  0.09329301  0.1128922  0.1347232\n                97.5%     Rhat n.eff\ndeviance   22.8870643 1.006682   540\ntheta.i[1]  0.2329834 1.002841  1200\ntheta.i[2]  0.2478510 1.001819  1500\ntheta.i[3]  0.1822796 1.001412  2200\n\n\n\nRhat values and ESS look good.\n\n\nlibrary(coda)\n# turn the model into an mcmc object\nmod_mcmc &lt;- as.mcmc(mod)\n# get trace plot and density\nplot(mod_mcmc) \n\n\n\n\n\n\n\n\n\n\n\n\nAfter running the MCMC sampling (e.g., using 4,000 iterations and discarding the first 2,000 as burn-in), you will obtain samples from the posterior distributions of \\(\\theta_i\\).\n(a) Plot the posterior distributions of the CTRs for the three campaigns using a package like tidybayes in R (or coda or the base plotting functions).\n\n\nlibrary(tidybayes)\nlibrary(tidyverse)\n## get the output in matrix format\nmcmc.matrix &lt;- mod$BUGSoutput$sims.matrix\n## get indexes for theta\ncampaign_index &lt;- 1:3\n\n## plots\nmcmc.matrix %&gt;%\n  spread_draws(theta.i[campaign_index]) %&gt;% \n  ggplot(aes(y = factor(campaign_index, labels = c(\"A\",\"B\", \"C\")), x = theta.i)) +\n  stat_halfeye() +\n  ylab(\"Campaign\")\n\n\n\n\n\n\n\n\n\n(b) Compare the distributions to assess the relative effectiveness of the campaigns. It appears that the posterior distribution for \\(\\theta_B\\) is shifted to the right compared to \\(\\theta_A\\) and \\(\\theta_C\\), suggesting that Campaign B is likely more effective, i.e., the click through rate is estimated to be higher. Note however, there is an overlap in terms of uncertainty in the estimated click through rate.\n\n\n\n\n\n(a) Calculate the posterior probability that Campaign B has a higher CTR than Campaign A and Campaign C:\n\n\\(P(\\theta_B &gt; \\theta_A) = \\frac{1}{S} \\sum_{s=1}^{S} I(\\theta_B^{(s)} &gt; \\theta_A^{(s)})\\)\nwhere \\(S\\) is the number of posterior samples, and \\(I(\\cdot)\\) is an indicator function. You can do similar calculations for \\(P(\\theta_B &gt; \\theta_C)\\).\nWe can implement this in R as follows\n\ntheta_samps &lt;- mcmc.matrix %&gt;%\n                spread_draws(theta.i[campaign_index]) \n\ntheta_A &lt;- theta_samps %&gt;% filter(campaign_index == 1) %&gt;% pull(theta.i)\ntheta_B &lt;- theta_samps %&gt;% filter(campaign_index == 2) %&gt;% pull(theta.i)\ntheta_C &lt;- theta_samps %&gt;% filter(campaign_index == 3) %&gt;% pull(theta.i)\n\nsum(theta_B &gt; theta_A)/length(theta_B)\n\n[1] 0.635\n\nsum(theta_B &gt; theta_C)/length(theta_B)\n\n[1] 0.8913333\n\n\nInterpretation:\n\nThe posterior probability that \\(\\theta_B &gt; \\theta_A\\) is 0.61, indicating that there is a 61% chance Campaign B has a higher CTR than Campaign A.\nThe posterior probability that \\(\\theta_B &gt; \\theta_C\\) is 0.89, indicating that there is a 89% chance Campaign B has a higher CTR than Campaign C."
  },
  {
    "objectID": "Assignment2.html",
    "href": "Assignment2.html",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "Please submit your answers to this assignment by November 13th, 4:00PM.\n\n\n\nHere‚Äôs a dataset that includes information from a survey of university undergraduate students from five different countries and whether or not they play video games. This will help in examining the proportion of students who play video games across these countries.\n\nvideo_game_data &lt;- data.frame(\n  Country = c(\"USA\", \"Japan\", \"Germany\", \"Brazil\", \"India\"),\n  Plays_Video_Games_Yes = c(70, 64, 58, 81, 45),  # students who play video games\n  Total = c(120, 80, 90, 95, 70)         # total students surveyed\n)\n\n# View the data frame\nvideo_game_data\n\n  Country Plays_Video_Games_Yes Total\n1     USA                    70   120\n2   Japan                    64    80\n3 Germany                    58    90\n4  Brazil                    81    95\n5   India                    45    70\n\n\n\n\n\n\nUsing these data, carry out the following tasks and answer questions 1-6 below.\n\nTask: Model Specification\n\nSpecify an appropriate likelihood.\nSpecify a Beta prior distribution for the proportion of video game players that reflects prior information that the expected proportion \\(\\pm\\) 1 standard deviation is \\(0.6 \\pm 0.2\\). Round the Beta distribution parameters to the nearest whole numbers.\n\nTask: Correct JAGS Model Specification\n\nAssume that you‚Äôve specified the JAGS model below, but there are 5 errors in it. What are the errors?\n\n\n\nlibrary(rjags)\nlibrary(R2jags)\n\nvg_model &lt;- \"\n {\n  # Loop over the three campaigns\n  for (i in n_country) {\n    # Likelihood: binomial distribution for the number playing video games\n    y.i[i] &lt;- dbinom(theta.i, N.i[i])\n    \n    # Prior: Beta distribution for the proportion\n    theta.i[i] ~ beta(a, b)\n  }\n}\n\"\n\n\nTask: Run the JAGS Model\n\nAfter making the modifications to the JAGS model, replace the ?? in the code below to provide the appropriate data for the JAGS model. Run the code and provide the overview plot you obtain from plot(mod).\n\n\n\nvg_data &lt;- list(??)\n\ninits &lt;- function() list(theta.i = runif(5)) # Random initial values for proportions\nparams &lt;- c(\"theta.i\") # Monitor theta\n\nmod &lt;- jags(data = vg_data, \n            inits = inits, \n            parameters.to.save = params, \n            n.iter = 4000,\n            n.burnin = 2000, \n            model.file = textConnection(vg_model))\nplot(mod)\n\n\nTask: Inference\n\nFor each country, report the posterior median and 80% credible interval for the proportion of video game players.\n\nTask: Additional Analysis\n\nCalculate the posterior probability that India has a higher proportion of video game players than the USA, and interpret this probability."
  },
  {
    "objectID": "Assignment2.html#instructions",
    "href": "Assignment2.html#instructions",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "Please submit your answers to this assignment by November 13th, 4:00PM.\n\n\n\nHere‚Äôs a dataset that includes information from a survey of university undergraduate students from five different countries and whether or not they play video games. This will help in examining the proportion of students who play video games across these countries.\n\nvideo_game_data &lt;- data.frame(\n  Country = c(\"USA\", \"Japan\", \"Germany\", \"Brazil\", \"India\"),\n  Plays_Video_Games_Yes = c(70, 64, 58, 81, 45),  # students who play video games\n  Total = c(120, 80, 90, 95, 70)         # total students surveyed\n)\n\n# View the data frame\nvideo_game_data\n\n  Country Plays_Video_Games_Yes Total\n1     USA                    70   120\n2   Japan                    64    80\n3 Germany                    58    90\n4  Brazil                    81    95\n5   India                    45    70\n\n\n\n\n\n\nUsing these data, carry out the following tasks and answer questions 1-6 below.\n\nTask: Model Specification\n\nSpecify an appropriate likelihood.\nSpecify a Beta prior distribution for the proportion of video game players that reflects prior information that the expected proportion \\(\\pm\\) 1 standard deviation is \\(0.6 \\pm 0.2\\). Round the Beta distribution parameters to the nearest whole numbers.\n\nTask: Correct JAGS Model Specification\n\nAssume that you‚Äôve specified the JAGS model below, but there are 5 errors in it. What are the errors?\n\n\n\nlibrary(rjags)\nlibrary(R2jags)\n\nvg_model &lt;- \"\n {\n  # Loop over the three campaigns\n  for (i in n_country) {\n    # Likelihood: binomial distribution for the number playing video games\n    y.i[i] &lt;- dbinom(theta.i, N.i[i])\n    \n    # Prior: Beta distribution for the proportion\n    theta.i[i] ~ beta(a, b)\n  }\n}\n\"\n\n\nTask: Run the JAGS Model\n\nAfter making the modifications to the JAGS model, replace the ?? in the code below to provide the appropriate data for the JAGS model. Run the code and provide the overview plot you obtain from plot(mod).\n\n\n\nvg_data &lt;- list(??)\n\ninits &lt;- function() list(theta.i = runif(5)) # Random initial values for proportions\nparams &lt;- c(\"theta.i\") # Monitor theta\n\nmod &lt;- jags(data = vg_data, \n            inits = inits, \n            parameters.to.save = params, \n            n.iter = 4000,\n            n.burnin = 2000, \n            model.file = textConnection(vg_model))\nplot(mod)\n\n\nTask: Inference\n\nFor each country, report the posterior median and 80% credible interval for the proportion of video game players.\n\nTask: Additional Analysis\n\nCalculate the posterior probability that India has a higher proportion of video game players than the USA, and interpret this probability."
  },
  {
    "objectID": "7-checking.html#posterior-predictive-checking",
    "href": "7-checking.html#posterior-predictive-checking",
    "title": "Bayesian Data Analysis",
    "section": "Posterior Predictive Checking",
    "text": "Posterior Predictive Checking\nThe idea behind posterior predictive checking is simple: if a model is a good fit then we should be able to use it to generate data that looks a lot like the data we observed.\n\nTo generate the data used for posterior predictive checks (PPCs) we simulate from the posterior predictive distribution.\nFor each draw (simulation) \\(s= 1, \\ldots , S\\) of the parameters from the posterior distribution, we draw an entire vector of \\(N\\) outcomes \\(\\tilde{y}^{(s)}\\) from the posterior predictive distribution by simulation from the data model, conditional on parameters.\nThe result is an \\(S \\times N\\) matrix of draws \\(\\tilde{y}\\)\nWhen simulating from the posterior predictive distribution using the same values of the predictors X that we used when fitting the model we denote the simulations \\(y^{rep}\\).\nWhen predicting new or future observations we denote the simulations \\(\\tilde{y}\\).\nWe will use the bayesplot package to create various graphical displays for posterior predictive checks (PPCs)."
  },
  {
    "objectID": "7-checking.html#predictive-distribution-for-observations-yrep",
    "href": "7-checking.html#predictive-distribution-for-observations-yrep",
    "title": "Bayesian Data Analysis",
    "section": "Predictive Distribution for Observations, \\(y^{rep}\\)",
    "text": "Predictive Distribution for Observations, \\(y^{rep}\\)\nLet‚Äôs revisit the Bayesian regression Model applied to Kid IQ:\n\nModel for Observations:\n\\[y_i \\sim N(\\mu_i, \\sigma)\\]\nRegression Structure:\n\\[\\mu_i = \\alpha_{j[i]} + \\beta_{j[i]} \\left( x_i - \\text{mean}(x) \\right), \\quad \\text{where } j[i] = \\text{education level for observation } i\\]\n\n\\(\\alpha_j \\sim N(100, 30^2), \\quad j = 1, 2\\)\n\\(\\beta_j \\sim N(0, 2^2), \\quad j = 1, 2\\)\n\n\nTo simulate new observations (\\(y^{\\text{rep}}\\)) based on this model, we follow these steps:\n\nSample \\(\\mu_i^{(s)}\\) and \\(\\sigma^{(s)}\\) from the posterior distribution \\(p(\\mu, \\sigma \\mid y)\\).\nGenerate Replicates \\(y_i^{\\text{rep}(s)}\\) by drawing from \\(N(\\mu_i^{(s)}, \\sigma^{(s)})\\).\n\nThis process provides a predictive distribution of Kid IQ observations, enabling us to compare the model‚Äôs predictions to the observed data."
  },
  {
    "objectID": "7-checking.html#jags-model-to-include-yrep",
    "href": "7-checking.html#jags-model-to-include-yrep",
    "title": "Bayesian Data Analysis",
    "section": "JAGS model to include \\(y^{rep}\\)",
    "text": "JAGS model to include \\(y^{rep}\\)\n\nlrmodel2 =\"\nmodel{\n    for (i in 1:n){\n        y.i[i] ~ dnorm(mu.i[i], sigma^-2)\n        mu.i[i] &lt;- alpha.j[hs_index[i]] + beta.j[hs_index[i]]*(x.i[i] - mean(x.i))\n    }\n\n#Priors\nfor(j in 1:m)\n{\nalpha.j[j] ~ dnorm(100, 30^-2) \nbeta.j[j] ~ dnorm(0, 2^-2) \n}\nsigma ~ dunif(0,50)\n\n## predictive distribution\nfor (i in 1:n) {yrep[i] ~ dnorm(mu.i[i], sigma^-2)}\n}\n\""
  },
  {
    "objectID": "7-checking.html#posterior-predictive-checking-histograms",
    "href": "7-checking.html#posterior-predictive-checking-histograms",
    "title": "Bayesian Data Analysis",
    "section": "Posterior predictive checking: histograms",
    "text": "Posterior predictive checking: histograms\nTo use the PPC function from the bayesplot package we need the outcome values y and a matrix of replicates yrep\n\ny &lt;- kidiq$kid_score\nyrep &lt;- mod2$BUGSoutput$sims.list$yrep\n\nNow let‚Äôs look at histograms of some of the yrep datasets and see how they compare to y\n\nlibrary(bayesplot)\ncolor_scheme_set(\"brightblue\")\nppc_hist(y, yrep[1:3, ])"
  },
  {
    "objectID": "7-checking.html#posterior-predictive-checking-density-overlay",
    "href": "7-checking.html#posterior-predictive-checking-density-overlay",
    "title": "Bayesian Data Analysis",
    "section": "Posterior predictive checking: density overlay",
    "text": "Posterior predictive checking: density overlay\nWe can also look at is a comparison of the distribution of y and the distributions of some of the simulated datasets in the yrep matrix via density plots.\n\nppc_dens_overlay(y, yrep[1:50, ])"
  },
  {
    "objectID": "7-checking.html#posterior-predictive-checking-test-statistic",
    "href": "7-checking.html#posterior-predictive-checking-test-statistic",
    "title": "Bayesian Data Analysis",
    "section": "Posterior predictive checking: test statistic",
    "text": "Posterior predictive checking: test statistic\nDecide on a test quantity (min, max) \\(T(y,\\theta)\\). A good statistic would ideally be independent of the parameters of the model.\n\nobtain the summary quantity for the observed data \\(T(y)\\)\nobtain the summary quantity for the replicated data \\(T(y,\\theta)\\)\n\nLet‚Äôs consider the maximum Kid IQ score as the test quantity\n\nmax_y &lt;- max(kidiq$kid_score)\nppc_stat(y,yrep, stat = \"max\")"
  },
  {
    "objectID": "7-checking.html#posterior-predictive-checking-test-statistic-1",
    "href": "7-checking.html#posterior-predictive-checking-test-statistic-1",
    "title": "Bayesian Data Analysis",
    "section": "Posterior predictive checking: test statistic",
    "text": "Posterior predictive checking: test statistic\nMany of the available PPCs can also be carried out within levels of a grouping variable.\nFor example we can compute the test statistic within levels of the highschool grouping variable and a separate plot is made for each level.\n\nmax_y &lt;- max(kidiq$kid_score)\nppc_stat_grouped(y,yrep,group = kidiq$mom_hs,  stat = \"max\")"
  },
  {
    "objectID": "7-checking.html#sensitivity-analysis",
    "href": "7-checking.html#sensitivity-analysis",
    "title": "Bayesian Data Analysis",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\nHow do model structure and prior choices impact the results?\n\nTest Alternative Models and Priors\nEvaluate the robustness of inferences by experimenting with different model configurations and prior distributions.\nCompare Sensitivity of Key Inference Metrics\nAssess how changes in model assumptions affect critical inference measures.\nSensitivity of Extremes vs.¬†Central Tendency\nExtremes (e.g., percentiles) tend to be more affected by model changes than measures of central tendency (means, medians).\nExtrapolation vs.¬†Interpolation\nPredictions made outside the observed predictor range (extrapolation) show higher sensitivity to model assumptions than predictions within this range (interpolation)."
  },
  {
    "objectID": "7-checking.html#prior-predictive-checks",
    "href": "7-checking.html#prior-predictive-checks",
    "title": "Bayesian Data Analysis",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\nPrior predictive checks allow us to look at the situation prior to any data being observed in order to see what data distribution is being implied by the choice of priors and likelihood.\n\nIdeally this distribution would have at least some mass around all plausible data sets.\n\nJAGS has a simulation functionality that allows us to easily generate prior predictive distributions based on our specified model.\n\nThe likelihood and priors can be specified in a ‚Äúdata‚Äù block so that we can simulate what data generated from the likelihood would look like, based solely on our prior assumptions.\n\n\nlrmodel_prior =\"\ndata{\n    for (i in 1:n){\n      yrep_prior[i] ~ dnorm(mu.i[i], sigma^-2)\n        mu.i[i] &lt;- alpha.j[hs_index[i]] + beta.j[hs_index[i]]*(x.i[i] - mean(x.i))\n    }\n#Priors\nfor(j in 1:m)\n{\nalpha.j[j] ~ dnorm(100, 30^-2) \nbeta.j[j] ~ dnorm(0, 2^-2) \n}\nsigma ~ dunif(0,50)\n} # end data block\nmodel{\nfake &lt;- 0\n}\n\""
  },
  {
    "objectID": "7-checking.html#prior-predictive-checks-1",
    "href": "7-checking.html#prior-predictive-checks-1",
    "title": "Bayesian Data Analysis",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\nWe can use the run.jags() function from the runjags package to generate the simulations.\n\njags.data &lt;- list(x.i = kidiq$mom_iq, \n                  hs_index = as.numeric(kidiq$mom_hs + 1),\n                  n = nrow(kidiq),\n                  m = 2)\n\nyrep_prior &lt;- matrix(NA, nrow = 50, ncol = nrow(kidiq))\n\nfor(i in 1:50)\n  {\nout &lt;- runjags::run.jags(lrmodel_prior, \n                         data = jags.data,monitor=c(\"yrep_prior\"), \n                         sample=1, \n                         n.chains=1, \n                         summarise=FALSE)\n\n yrep_prior[i,] &lt;- coda::as.mcmc(out)\n }"
  },
  {
    "objectID": "7-checking.html#prior-predictive-checks-2",
    "href": "7-checking.html#prior-predictive-checks-2",
    "title": "Bayesian Data Analysis",
    "section": "Prior predictive checks",
    "text": "Prior predictive checks\nWe can then use the PPC functions from the bayesplot package by providing replicates yrep_prior instead of y_rep\n\nppc_hist(y, yrep_prior[1:5, ])"
  },
  {
    "objectID": "7-checking.html#prior-vs-posterior-for-parameters-alpha_j",
    "href": "7-checking.html#prior-vs-posterior-for-parameters-alpha_j",
    "title": "Bayesian Data Analysis",
    "section": "Prior vs Posterior for parameters (\\(\\alpha_j\\))",
    "text": "Prior vs Posterior for parameters (\\(\\alpha_j\\))"
  },
  {
    "objectID": "7-checking.html#prior-vs-posterior-for-parameters-beta_j",
    "href": "7-checking.html#prior-vs-posterior-for-parameters-beta_j",
    "title": "Bayesian Data Analysis",
    "section": "Prior vs Posterior for parameters (\\(\\beta_j\\))",
    "text": "Prior vs Posterior for parameters (\\(\\beta_j\\))"
  },
  {
    "objectID": "8-BHM.html#hierarchical-binomial-model",
    "href": "8-BHM.html#hierarchical-binomial-model",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Binomial Model",
    "text": "Hierarchical Binomial Model\nSuppose we are considering the success of a treatment for cardio vascular disease (CVD) in a number of different hospitals. We are interested in \\(\\theta_j\\) the survival probability associated with hospital \\(j\\).\n\nWe have observations \\(y_{ij}\\) which tells us the status of patient \\(i\\) in hospital \\(j\\) such that \\(y_{ij} = 1\\) if the patient survived and \\(y_{ij} = 0\\) otherwise.\nWe could assume that \\(\\theta_j\\) are independent."
  },
  {
    "objectID": "8-BHM.html#hierarchical-binomial-model-1",
    "href": "8-BHM.html#hierarchical-binomial-model-1",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Binomial Model",
    "text": "Hierarchical Binomial Model\nWe could assume that there‚Äôs a joint effect i.e., a common \\(\\theta\\)"
  },
  {
    "objectID": "8-BHM.html#hierarchical-binomial-model-2",
    "href": "8-BHM.html#hierarchical-binomial-model-2",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Binomial Model",
    "text": "Hierarchical Binomial Model\nBut maybe it‚Äôs more sensible to assume that we have different \\(\\theta_j\\) but that they have something in common.\n\nA natural assumption to make is that \\(\\theta_j\\) have a common population distribution\n\n\nNow we are building up levels.\n\nWe have a data level, a parameter level and a hyperparameter level"
  },
  {
    "objectID": "8-BHM.html#hierarchical-models---terms",
    "href": "8-BHM.html#hierarchical-models---terms",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Models - Terms",
    "text": "Hierarchical Models - Terms\nThe data level: observations given parameters \\(p(y_{ij}|\\theta_j)\\)\nThe parameter level: parameters given hyperparameters \\(p(\\theta_j|\\tau)\\)\nThe hyperparameter level: \\(p(\\tau)\\)\nPutting all of this together in Bayes‚Äô theorem we get:\n\\(p(\\theta, \\tau|y) \\propto p(y|\\theta)p(\\theta|\\tau)p(\\tau)\\)"
  },
  {
    "objectID": "8-BHM.html#hierarchical-models---rats-example",
    "href": "8-BHM.html#hierarchical-models---rats-example",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Models - Rats example",
    "text": "Hierarchical Models - Rats example\n\nIn the evaluation of drugs for possible clinical application, studies are routinely performed on rodents.\nSuppose we have studies to estimate \\(\\theta\\), the probability of a tumor in a population of female laboratory rats of type ‚ÄòF344‚Äô that receive a zero dose of the drug (a control group).\nThe data are whether the rat developed endometrial stromal polyps (a kind of tumor)\nThe experiment has been repeated 71 times and the data are shown below:\n\n\n\n [1] \"0/20\"  \"0/20\"  \"0/20\"  \"0/20\"  \"0/20\"  \"0/20\"  \"0/20\"  \"0/19\"  \"0/19\" \n[10] \"0/19\"  \"0/19\"  \"0/18\"  \"0/18\"  \"0/17\"  \"1/20\"  \"1/20\"  \"1/20\"  \"1/20\" \n[19] \"1/19\"  \"1/19\"  \"1/18\"  \"1/18\"  \"2/25\"  \"2/24\"  \"2/23\"  \"2/20\"  \"2/20\" \n[28] \"2/20\"  \"2/20\"  \"2/20\"  \"2/20\"  \"1/10\"  \"5/49\"  \"2/19\"  \"5/46\"  \"3/27\" \n[37] \"2/17\"  \"7/49\"  \"7/47\"  \"3/20\"  \"3/20\"  \"2/13\"  \"9/48\"  \"10/50\" \"4/20\" \n[46] \"4/20\"  \"4/20\"  \"4/20\"  \"4/20\"  \"4/20\"  \"4/20\"  \"10/48\" \"4/19\"  \"4/19\" \n[55] \"4/19\"  \"5/22\"  \"11/46\" \"12/49\" \"5/20\"  \"5/20\"  \"6/23\"  \"5/19\"  \"6/22\" \n[64] \"6/20\"  \"6/20\"  \"6/20\"  \"16/52\" \"15/46\" \"15/47\" \"9/24\"  \"4/14\""
  },
  {
    "objectID": "8-BHM.html#hierarchical-models---rats-example-1",
    "href": "8-BHM.html#hierarchical-models---rats-example-1",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Models - Rats example",
    "text": "Hierarchical Models - Rats example\nLet‚Äôs specify a Bayesian Hierarchical model for the rats example.\n\nLet \\(y_j\\) be the number of rats that had tumors in study \\(j\\) and let \\(n_j\\) be the total number of rats in study \\(j\\).\nIt is natural to assume a binomial model for the number of tumors, given \\(\\theta_j\\).\nWe can select a prior distribution for \\(\\theta_j\\) from the conjugate family and vague priors for the hyperparameters.\n\n\n\n\\(\\text{}\\)\n\\(\\text{}\\)\n\\(y_j|\\theta_j \\sim Binomial(\\theta_j,n_j)\\)\n\\(\\theta_j|\\alpha,\\beta \\sim Be(\\alpha, \\beta)\\)\n\\(\\text{}\\)\n\\(\\alpha \\sim ht(1,10^2,1)\\)\n\\(\\beta \\sim ht(1,10^2,1)\\)"
  },
  {
    "objectID": "8-BHM.html#hierarchical-models---rats-example-2",
    "href": "8-BHM.html#hierarchical-models---rats-example-2",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Models - Rats example",
    "text": "Hierarchical Models - Rats example"
  },
  {
    "objectID": "8-BHM.html#hierarchical-models---rats-example-3",
    "href": "8-BHM.html#hierarchical-models---rats-example-3",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Models - Rats example",
    "text": "Hierarchical Models - Rats example"
  },
  {
    "objectID": "8-BHM.html#hierarchical-models---rats-example-4",
    "href": "8-BHM.html#hierarchical-models---rats-example-4",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Models - Rats example",
    "text": "Hierarchical Models - Rats example\n\nSummary of hyperparameters \\(\\alpha\\) and \\(\\beta\\) from the population Beta prior distribution\n\n\n\n\n\n\n.variable\n.value\n.lower\n.upper\n.width\n.point\n.interval\n\n\n\n\nalpha\n2.302976\n1.270626\n4.538142\n0.95\nmedian\nqi\n\n\nbeta\n13.779446\n7.591737\n27.849263\n0.95\nmedian\nqi\n\n\n\n\n\n\n\\(Beta(\\alpha, \\beta)\\) given posterior draws of \\(\\alpha\\) and \\(\\beta\\)"
  },
  {
    "objectID": "8-BHM.html#hierarchical-normal-model",
    "href": "8-BHM.html#hierarchical-normal-model",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Normal Model",
    "text": "Hierarchical Normal Model\nAssume a factory has 6 machines and a quality measure that is taken on a regular basis.\n\nHow might we structure a hierarchical model such that we get an overall estimate of quality across all machines as well as a quality estimate per machine?\n\n\n\n\\(\\text{}\\)\n\\(y_{ij} \\sim N(\\mu_j,\\sigma^2)\\)\n\\(\\mu_j \\sim N(\\mu_F, \\sigma_F^2)\\)\n\\(\\text{}\\)\n\\(\\mu_F \\sim N(...)\\)\n\\(\\sigma_F \\sim ht(...)\\)\n\\(\\sigma \\sim ht(...)\\)"
  },
  {
    "objectID": "8-BHM.html#hierarchical-normal-model-1",
    "href": "8-BHM.html#hierarchical-normal-model-1",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Normal Model",
    "text": "Hierarchical Normal Model\nYou might also consider that the variation in quality measures is not constant across all machines.\n\n\n\\(\\text{}\\)\n\\(\\text{}\\)\n\\(y_{ij} \\sim N(\\mu_j,\\sigma^2)\\)\n\\(\\mu_j \\sim N(\\mu_F, \\sigma_F^2)\\)\n\\(\\sigma_j \\sim ht(...)\\)\n\\(\\text{}\\)\n\\(\\mu_F \\sim N(...)\\)\n\\(\\sigma_F \\sim ht(...)\\)"
  },
  {
    "objectID": "8-BHM.html#example-8-schools",
    "href": "8-BHM.html#example-8-schools",
    "title": "Bayesian Data Analysis",
    "section": "Example: 8 schools",
    "text": "Example: 8 schools\n\nA study was performed for the Educational Testing Service to analyze the effects of special coaching programs on test scores\nSeparate randomized experiments were performed to estimate the effects of coaching programs for the SAT (Scholastic Aptitude Test) in each of eight high schools.\nThe outcome variable in each study was the score on the SAT\nTypically the scores can vary between 200 and 800, with a mean = 500 and standard deviation = 100.\nThe SAT examinations are designed to be resistant to short-term efforts directed specifically toward improving performance on the test; instead they are designed to reflect knowledge acquired and abilities developed over many years of education.\nNevertheless, each of the eight schools in this study considered its short-term coaching program to be successful at increasing SAT scores."
  },
  {
    "objectID": "8-BHM.html#example-8-schools-1",
    "href": "8-BHM.html#example-8-schools-1",
    "title": "Bayesian Data Analysis",
    "section": "Example: 8 schools",
    "text": "Example: 8 schools\n\nThe performance gains of coached students were compared to non-coached students. Separate estimates were obtained for each school, but because the size of the schools differed, the standard errors differed as well.\nIn each school the estimated coaching effect and its standard error were obtained.\n\n\n\n\n\n\ny\nse\n\n\n\n\n28\n15\n\n\n8\n10\n\n\n-3\n16\n\n\n7\n11\n\n\n-1\n9\n\n\n1\n11\n\n\n18\n10\n\n\n12\n18"
  },
  {
    "objectID": "8-BHM.html#example-8-schools-2",
    "href": "8-BHM.html#example-8-schools-2",
    "title": "Bayesian Data Analysis",
    "section": "Example: 8 schools",
    "text": "Example: 8 schools\n\nUpon initial examination of the data it may seem that some coaching programs have moderate effects (in the range 18‚Äì28 points), most have small effects (0‚Äì12 points), and two have small negative effects.\nHowever, when we take note of the standard errors of these estimated effects, we see that it is difficult statistically to distinguish between any of the experiments:"
  },
  {
    "objectID": "8-BHM.html#hierarchical-normal-model-8-schools",
    "href": "8-BHM.html#hierarchical-normal-model-8-schools",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical normal model: 8 schools",
    "text": "Hierarchical normal model: 8 schools\nThe general overlap in the posterior intervals based on independent analyses suggests that all experiments might be estimating the same quantity.\n\nIs the effect actually the same everywhere i.e., is there one common coaching effect?\nUnder the hypothesis that all experiments have the same effect and produce independent estimates of this common effect, we could treat the data as eight normally distributed observation with an overall mean and known variances.\nBut, would it be possible to have an effect of 28 in one school just by chance if the coaching effect across all schools is the same?"
  },
  {
    "objectID": "8-BHM.html#issues-with-spearate-and-pooled-effects",
    "href": "8-BHM.html#issues-with-spearate-and-pooled-effects",
    "title": "Bayesian Data Analysis",
    "section": "Issues with spearate and pooled effects",
    "text": "Issues with spearate and pooled effects\nLet‚Äôs consider schools A, with the separate effects model we infer that for school A there is a 50% chance that the true effect is greater than ~28."
  },
  {
    "objectID": "8-BHM.html#issues-with-spearate-and-pooled-effects-1",
    "href": "8-BHM.html#issues-with-spearate-and-pooled-effects-1",
    "title": "Bayesian Data Analysis",
    "section": "Issues with spearate and pooled effects",
    "text": "Issues with spearate and pooled effects\nWith the pooled effects model, we infer that there is a 50% chance that the true effect is greater than 7.5. A value greater than 28 is highly improbable."
  },
  {
    "objectID": "8-BHM.html#issues-with-spearate-and-pooled-effects-2",
    "href": "8-BHM.html#issues-with-spearate-and-pooled-effects-2",
    "title": "Bayesian Data Analysis",
    "section": "Issues with spearate and pooled effects",
    "text": "Issues with spearate and pooled effects\n\nThe separate effects model treats School A completely in isolation, ignoring the fact that we have considerable evidence that courses similar to the one taught in School A evidently have typical effect size less than 20 points.\nThe pooled effects model would assume that the true effect in all schools is exactly equal, in spite of the courses being taught by different teachers to different students.\n\n\n\nWhat about a middle path?"
  },
  {
    "objectID": "8-BHM.html#hierarchical-normal-model---8-schools",
    "href": "8-BHM.html#hierarchical-normal-model---8-schools",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Normal Model - 8 Schools",
    "text": "Hierarchical Normal Model - 8 Schools\n\nAssume that each school‚Äôs ‚Äútrue effect‚Äù is drawn from a normal distribution with unknown mean and standard deviation\nAssume the observed effect in each school is sampled from a normal distribution with a mean equal to the true effect, and standard deviation given in the dataset.\n\n\n\n\\(\\text{}\\)\n\\(\\text{}\\)\n\\(y_{i} \\sim N(\\mu_i,\\sigma_i^2)\\)\n\\(\\mu_i \\sim N(\\mu_P, \\sigma_P^2)\\)\n\\(\\text{}\\)\n\\(\\mu_P \\sim N(0,200^2)\\)\n\\(\\sigma_P \\sim Uniform(0,200)\\)"
  },
  {
    "objectID": "8-BHM.html#hierarchical-normal-model---8-schools-1",
    "href": "8-BHM.html#hierarchical-normal-model---8-schools-1",
    "title": "Bayesian Data Analysis",
    "section": "Hierarchical Normal Model - 8 Schools",
    "text": "Hierarchical Normal Model - 8 Schools\n\n\n\\(y_{i} \\sim N(\\mu_i,\\sigma_i^2)\\)\n\\(\\mu_i \\sim N(\\mu_P, \\sigma_P^2)\\)\n\\(\\mu_P \\sim N(0,200^2)\\)\n\\(\\sigma_P \\sim Uniform(0,200)\\)\n\n\nnormalmodel = \"\nmodel{\n  for(i in 1:N)\n{\n y.i[i] ~ dnorm(mu.i[i],sigma.i[i]^-2) # data model\n mu.i[i] ~ dnorm(mu_p,sigma_p^-2)  # prior for effect\n} # end i loop\n\nmu_p ~ dnorm(0,200^-2)\nsigma_p ~ dunif(0,200)\n }\n \"\n\n\n\n\nPosterior distribution for the effects\n\n\n\n\n\n\n\n\n\n\nEffect summaries\n\n\n# A tibble: 8 √ó 8\n  school .variable .value .lower .upper .width .point .interval\n   &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1      1 mu.i       10.5   -2.02   32.5   0.95 median qi       \n2      2 mu.i        8.00  -4.68   20.6   0.95 median qi       \n3      3 mu.i        6.73 -12.0    20.1   0.95 median qi       \n4      4 mu.i        7.71  -5.96   20.6   0.95 median qi       \n5      5 mu.i        5.52  -9.19   16.9   0.95 median qi       \n6      6 mu.i        6.53  -8.64   18.8   0.95 median qi       \n7      7 mu.i       10.3   -1.21   26.4   0.95 median qi       \n8      8 mu.i        8.33  -6.37   25.5   0.95 median qi"
  },
  {
    "objectID": "8-BHM.html#what-about-sigma_p",
    "href": "8-BHM.html#what-about-sigma_p",
    "title": "Bayesian Data Analysis",
    "section": "What about \\(\\sigma_p\\)?",
    "text": "What about \\(\\sigma_p\\)?\n\n\\(\\sigma_p\\) is the shrinkage parameter, in this model it controls how much \\(\\mu_i\\) can vary away from \\(\\mu_p\\)\nIf \\(\\sigma_p\\) is close to zero we tend towards to pooled model.\nAs \\(\\sigma_p\\) increases we move back towards the independent model.\n\n\n\nPosterior distribution of \\(\\sigma_p\\)\n\n\n\n\n\n\n\n\n\n\nWhat happens to \\(\\mu_i\\) as \\(\\sigma_p\\) changes?"
  },
  {
    "objectID": "4-MCMC.html",
    "href": "4-MCMC.html",
    "title": "Bayesian Data Analysis",
    "section": "",
    "text": "We can use Bayes‚Äô rule to get posterior densities\n\nExample for single parameter Normal - \\(\\mu\\)\n\n\\(p(\\mu|y) \\propto p(y|\\mu)p(\\mu)\\)\n\nExample for single parameter Normal - \\(\\tau\\)\n\n\\(p(\\tau|y) \\propto p(y|\\tau)p(\\tau)\\)\n\nWe could also do this for multiple parameters\n\n\\(p(\\mu, \\tau|y) \\propto p(y|\\mu,\\tau)p(\\mu,\\tau)\\)\n\n\nBUT‚Ä¶\n\nRealistic problems with multiple parameters, data points, and common choices of priors don‚Äôt get a closed-form expression for \\(p(\\mu, \\tau|y)\\)\nSampling to the rescue!!"
  },
  {
    "objectID": "Tutorial3.html",
    "href": "Tutorial3.html",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "This tutorial explores posterior predictive checking.\n\n\nThe Fishery Dataset, fish_dat.csv consists of 256 snapper length measurements along with a grouping variable for different age groups. You‚Äôll find the dataset in the ST405/ST645 folder on the Rstudio server.\nYour goal is to perform a Bayesian analysis on this dataset as outlined in the steps below:\n\nVisualize the Snapper Length Data\n\nProduce a histogram of the snapper lengths (\\(y\\)) to explore the distribution of the data.\n\n\n\n\nFit a Basic Bayesian Model\n\nFit a Bayesian model that assumes a Normal likelihood for the length data, with an overall mean and variance (ignoring age groups for now).\n\n\n\n\nAssess Model Convergence\n\nUse convergence diagnostics (e.g., trace plots, R-hat values) to check if the model has successfully converged.\n\nSummarize Model Parameters\n\nProvide summaries (mean, median, credible intervals) for the key parameters of the model.\n\nConduct Posterior Predictive Checks\n\n\nPerform the following checks to assess the fit of your model:\n(i) Compare Distributions\n\nPlot the observed distribution of \\(y\\) and overlay it with the distributions of 50 simulated datasets (\\(y_{\\text{rep}}\\)) from the model.\n\n(ii) Evaluate Test Statistics\n\nCalculate the empirical values for the following test statistics based on the observed data:\n\n1st percentile\nMedian\n97th percentile\n\nCompare these empirical values with the corresponding posterior predictive distributions of test statistics.\n\n(iii) Posterior Predictive \\(p\\)-values and Effect Sizes\n\nFor each test statistic in (ii), calculate:\n\nPosterior predictive ( p )-value: Proportion of simulated datasets with test statistic values as extreme or more extreme than the observed.\nEffect size: Difference between the observed test statistic and the median of the posterior predictive distribution of test statistics, divided by the standard deviation of this predictive distribution.\n\n\n\n\n\nFit a Group-Specific Bayesian Model\n\n(a) Fit a Bayesian model with group-specific means and variances in the Normal likelihood (accounting for age groups).\n(b) Repeat parts 3‚Äì5 for this model, including convergence checks, parameter summaries, and posterior predictive checks.\n(c) Based on your posterior predictive checks, discuss which model (the overall model from part 2 or the group-specific model from part 6) you believe provides a better fit for the snapper length data. Justify your answer.\n\n\n\n\n\n\n\nThe posterior predictive ( p )-value indicates how many of the simulated datasets have test statistic values as extreme or more extreme than the value observed in the empirical data. It reflects the degree to which the model can replicate observed data characteristics.\nThe effect size quantifies the distance between the observed test statistic and the median of the posterior predictive distribution in standard deviation units. This gives an indication of how unusual the observed data is relative to the model predictions."
  },
  {
    "objectID": "Tutorial3.html#introduction",
    "href": "Tutorial3.html#introduction",
    "title": "ST405/ST645 Bayesian Data Analysis",
    "section": "",
    "text": "This tutorial explores posterior predictive checking.\n\n\nThe Fishery Dataset, fish_dat.csv consists of 256 snapper length measurements along with a grouping variable for different age groups. You‚Äôll find the dataset in the ST405/ST645 folder on the Rstudio server.\nYour goal is to perform a Bayesian analysis on this dataset as outlined in the steps below:\n\nVisualize the Snapper Length Data\n\nProduce a histogram of the snapper lengths (\\(y\\)) to explore the distribution of the data.\n\n\n\n\nFit a Basic Bayesian Model\n\nFit a Bayesian model that assumes a Normal likelihood for the length data, with an overall mean and variance (ignoring age groups for now).\n\n\n\n\nAssess Model Convergence\n\nUse convergence diagnostics (e.g., trace plots, R-hat values) to check if the model has successfully converged.\n\nSummarize Model Parameters\n\nProvide summaries (mean, median, credible intervals) for the key parameters of the model.\n\nConduct Posterior Predictive Checks\n\n\nPerform the following checks to assess the fit of your model:\n(i) Compare Distributions\n\nPlot the observed distribution of \\(y\\) and overlay it with the distributions of 50 simulated datasets (\\(y_{\\text{rep}}\\)) from the model.\n\n(ii) Evaluate Test Statistics\n\nCalculate the empirical values for the following test statistics based on the observed data:\n\n1st percentile\nMedian\n97th percentile\n\nCompare these empirical values with the corresponding posterior predictive distributions of test statistics.\n\n(iii) Posterior Predictive \\(p\\)-values and Effect Sizes\n\nFor each test statistic in (ii), calculate:\n\nPosterior predictive ( p )-value: Proportion of simulated datasets with test statistic values as extreme or more extreme than the observed.\nEffect size: Difference between the observed test statistic and the median of the posterior predictive distribution of test statistics, divided by the standard deviation of this predictive distribution.\n\n\n\n\n\nFit a Group-Specific Bayesian Model\n\n(a) Fit a Bayesian model with group-specific means and variances in the Normal likelihood (accounting for age groups).\n(b) Repeat parts 3‚Äì5 for this model, including convergence checks, parameter summaries, and posterior predictive checks.\n(c) Based on your posterior predictive checks, discuss which model (the overall model from part 2 or the group-specific model from part 6) you believe provides a better fit for the snapper length data. Justify your answer.\n\n\n\n\n\n\n\nThe posterior predictive ( p )-value indicates how many of the simulated datasets have test statistic values as extreme or more extreme than the value observed in the empirical data. It reflects the degree to which the model can replicate observed data characteristics.\nThe effect size quantifies the distance between the observed test statistic and the median of the posterior predictive distribution in standard deviation units. This gives an indication of how unusual the observed data is relative to the model predictions."
  }
]