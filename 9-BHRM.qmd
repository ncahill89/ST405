---
title: "Bayesian Data Analysis"
subtitle: "Bayesian Hierarchical Regression Models"
author: "Prof. Niamh Cahill (she/her)"
format: revealjs
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(DiagrammeR)
library(tidyverse)
library(rjags)
library(R2jags)
library(tidybayes)
library(bayesplot)

school_performance_dat <- read_csv("school_performance_dat.csv")

```

## Hierarchical Linear Models {style="font-size: 65%;"}

**Why Use Hierarchical Regression Models?**

Hierarchical regression models are useful when predictors vary across different levels of a dataset. They help account for nested or clustered data structures, providing more accurate insights into relationships at each level.

### Examples

-   **Educational Research**\
    Studying scholastic achievement involves information at multiple levels:
    -   **Individual-level**: Student characteristics, such as family background\
    -   **Class-level**: Teacher characteristics and teaching style\
    -   **School-level**: Policies, neighborhood context
-   **Clustered or Stratified Sampling**\
    When data are collected through stratified or cluster sampling, hierarchical models allow for regression analyses that respect these data groupings, providing insights within and across clusters.

## Hierarchical Linear Models {style="font-size: 65%;"}

**Adding Predictors at Multiple Levels**

Predictors can be introduced for each level in hierarchical data. For example:

-   **Educational Context**: Predictors for individual students, classrooms, and schools

-   **Sampling Context**: Predictors for strata or clusters

**Challenges in Multi-Level (Hierarchical) Modeling**

-   **Increasing Parameters**: Introducing predictors at multiple levels increases the number of parameters, often dramatically.

-   **Estimation Approach**: Estimating these parameters accurately requires additional modeling, typically through a population distribution.

**Handling Multiple Levels and Bayesian Estimation**

-   Hierarchical models can handle multiple levels of variation without a set limit.

-   Bayesian methods offer robust approaches for estimating unknown parameters across levels.

## Example: Math Scores Data {style="font-size: 65%;"}

Suppose that we have some data on the math scores for students in 12 different schools.

-   Each school contributes a number of data points.

-   We also have predictors on number of completed homeworks and type of school (public, private).

-   School-specific regression lines can describe the relationship between math scores and homework completed at the school level.

-   If there is a lack of data for some schools then parameter estimates can be informed by all other schools.

-   We can consider the school type as a predictor for the school level parameters.

-   We can get an overall estimate of the relationship between math scores and no. of homeworks completed.

## Visualising the Data {style="font-size: 65%;"}

```{r}

# Load necessary library
library(dplyr)

# Set seed for reproducibility
set.seed(67)

# Define parameters
n_schools <- 12
n_students <- sample(3:30, n_schools, replace = TRUE)  # Different number of students per school
school_ids <- 1:n_schools

# Randomly assign three schools to be private
private_schools <- sample(school_ids, 3)

# Initialize empty data frame to store the results
simulated_data <- data.frame()

# Loop through each school to generate data
for (school in school_ids) {
  
  # Determine if the school is private or public
  is_private <- ifelse(school %in% private_schools, 1, 0)
  
  # Set baseline math score depending on school type
  base_score <- ifelse(is_private == 1, 65, 45) + rnorm(1, sd = 5)
  
  # Generate number of homework assignments completed per student
  homework_completed <- sample(0:7, n_students[school], replace = TRUE)
  
  # Set relationship strength (slope) between homework and math score
  slope <- ifelse(is_private == 1, runif(1, 1.5, 2.5), runif(1, 0.5, 1.5))
  
  # Generate math scores with a positive relationship to homework and some random noise
  math_scores <- base_score + slope * homework_completed + rnorm(n_students[school], sd = 5)
  
  # Create a data frame for this school's data
  school_data <- data.frame(
    school_id = school,
    private = is_private,
    homework_completed = homework_completed,
    math_score = math_scores
  )
  
  # Bind this school's data to the main dataset
  simulated_data <- bind_rows(simulated_data, school_data)
}

school_performance_dat <- simulated_data
#  school_performance <- read_csv("school_performance.csv")
# 
# library(purrr)
# library(tidyr)
# set.seed(4561)
# 
# (nested_school <- school_performance %>%
#     select(schid, math, homework,public) %>%
#     group_by(schid) %>%   # prep for work by Species
#     nest() %>%              # --> one row per Species
#     ungroup() %>%
#     mutate(n = c(23,20,2,20,20,20,50,1,20,20))) # add sample sizes
# 
# (sampled_schools <- nested_school %>%
#   mutate(samp = map2(data, n, sample_n)))
# 
# school_performance_dat <- sampled_schools %>%
#   select(-data) %>%
#   unnest(samp)
# 
# 
# school_performance_dat <- school_performance_dat %>% mutate(schid = factor(schid, labels = 1:10))
# write_csv(school_performance_dat,"school_performance_dat.csv")
# 
```

```{r}
ggplot(school_performance_dat, aes(x = homework_completed, y = math_score, colour = factor(private))) +
  geom_point() +
  facet_wrap(~school_id)
```

## Hierarcical Regression on Individuals within Groups {style="font-size: 65%;"}

**Goal:** Model math scores with a linear regression and share information across schools to provide information where data are sparse. Include relevant predictors at the different levels in the hierarchy.

**Modelling Option**

$y_{i} \sim N(\mu_i,\sigma^2)$

$\mu_i = \alpha_{j[i]} + \beta x_i$ for $i = 1 \ldots n$

$\alpha_j \sim N(\mu_{\alpha},\sigma^2_{\alpha})$ for $j = 1 \ldots m$

where $x_i$ is the number of homeworks completed.

$\beta \sim Normal(0,10^2)$

$\sigma \sim Uniform(0,30)$

## Including Predictors at the Group Level {style="font-size: 65%;"}

Additionally, type of school is potentially informative for across-school variation in average math scores and in this case we can further extend the model such that

$\alpha_j \sim N(\mu_{\alpha_j},\sigma^2_{\alpha})$ for $j = 1 \ldots m$

$\mu_{\alpha_j} = \gamma_0 + \gamma_1u_j$

where $u_j$ is the school type (public or private).

$\gamma_0 \sim Uniform(0,100)$

$\gamma_1 \sim Normal(0,10^2)$

$\sigma_{\alpha} \sim dt(0,2^2,1)$

## The model DAG

![](images/Screenshot%202024-11-15%20at%2015.25.28.png)

```{r, eval = FALSE}
grViz("
digraph boxes_and_circles {

  # a 'graph' statement
  graph [compound = true, fontsize = 10]

  subgraph cluster0 {
  
  # several 'node' statements
  node [shape = box,
        width = 0.5]
  y[label = <y<sub>ij</sub>>]
  
  node [shape = box,
        fixedsize = true,
        width = 0.5] // sets as circles
  x[label = <x<sub>ij</sub>>] 


  node [shape = circle,
        fixedsize = true,
        width = 0.5] // sets as circles
  mu[label = <&mu;<sub>ij</sub>>] 



  node [shape = plaintext,
  peripheries = 0,
        fontname = Helvetica]
  i[label = i]


  x->mu mu->y  i
  

}

  subgraph cluster1 {
  
    node [shape = circle,
        fixedsize = true,
        width = 0.5] // sets as circles
  alpha[label = <&alpha;<sub>j</sub>>] 


    node [shape = circle,
        fixedsize = true,
        width = 0.6] // sets as circles
  mualpha[label = <&mu;<sub>&alpha;</sub><sub>j</sub>>] 


    node [shape = plaintext,
  peripheries = 0,
        fontname = Helvetica]
  j[label = j]


alpha->mu j mualpha -> alpha
  
  }

  node [shape = circle,
        fixedsize = true,
        width = 0.5]
  sigma[label = <&sigma;>]
  
  node [shape = circle,
        fixedsize = true,
        width = 0.5] // sets as circles
  beta[label = <&beta;>] 


  node [shape = circle,
        fixedsize = true,
        color = red,
        fontcolor = red,
        peripheries = 1,
        width = 0.5] // sets as circles
  gamma0[label = <&gamma;<sub>0</sub>>]
  
  node [shape = circle,
        fixedsize = true,
        color = red,
        fontcolor = red,
        peripheries = 1,
        width = 0.5] // sets as circles
  gamma1[label = <&gamma;<sub>1</sub>>]

  
  node [shape = circle,
        fixedsize = true,
        color = red,
        fontcolor = red,
        peripheries = 1,
        width = 0.5] // sets as circles
  nualpha[label = <&sigma;<sub>&alpha;</sub>>]

  
  # node [shape = circle,
  #       fixedsize = true,
  #       color = red,
  #       fontcolor = red,
  #       peripheries = 1,
  #       width = 0.5] // sets as circles
  # betap[label = <&beta;<sub>P</sub>>]
  # 
  # node [shape = circle,
  #       fixedsize = true,
  #       color = red,
  #       fontcolor = red,
  #       peripheries = 1,
  #       width = 0.5] // sets as circles
  # nubeta[label = <&nu;<sub>&beta;</sub>>]




 nualpha -> alpha  sigma->y beta -> mu gamma0 -> mualpha gamma1 -> mualpha
}
",
width = 200, height = 200)
```

## The JAGS model {style="font-size: 65%;"}

```{r, echo = TRUE}
bhregmodel = "
model{
  for(i in 1:N)
{
 y.i[i] ~ dnorm(mu.i[i],sigma^-2) # data model
 mu.i[i] <- alpha.j[school[i]] + beta*x.i[i]
} # end i loop

for(j in 1:n_school)
{
alpha.j[j] ~ dnorm(mu_alpha.j[j],sigma_alpha^-2)
mu_alpha.j[j] <- gamma0 + gamma1*u.j[j]
}

beta ~ dnorm(0,10^-2)
gamma0 ~ dunif(0,100)
gamma1 ~ dnorm(0,10^-2)

sigma ~ dunif(0,30)
sigma_alpha ~ dt(0,2^-2,1)T(0,)
 "
```

```{r, include = FALSE, echo = FALSE}
bhregmodel = "
model{
  for(i in 1:N)
{
 y.i[i] ~ dnorm(mu.i[i],sigma^-2) # data model
 mu.i[i] <- alpha.j[school[i]] + beta*x.i[i]
} # end i loop

for(j in 1:n_school)
{
alpha.j[j] ~ dnorm(mu_alpha.j[j],sigma_alpha^-2)
mu_alpha.j[j] <- gamma0 + gamma1*u.j[j]
}

beta ~ dnorm(0,10^-2)
gamma0 ~ dunif(0,100)
gamma1 ~ dnorm(0,10^-2)

sigma ~ dunif(0,30)
sigma_alpha ~ dt(0,2^-2,1)T(0,)


  for(k in 1:npred)
{
 ytilde.k[k] ~ dnorm(mu_pred.k[k],sigma^-2) # data model
 mu_pred.k[k] <- alpha.j[school_pred[k]] + beta*xpred.k[k]
 mu_private_pred.k[k] <- mu_alpha.j[school_pred[k]] + beta*xpred.k[k]
} # end k loop


  for(i in 1:N)
{
 yrep[i] ~ dnorm(mu.i[i],sigma^-2) # data model
} # end k loop

 }
 "

jags.data <- list(y.i = school_performance_dat$math_score,
                  x.i = school_performance_dat$homework_completed,
                  u.j = school_performance_dat %>% group_by(school_id) %>% summarise(private = unique(private)) %>% pull(private),
                  n_school = school_performance_dat$school_id %>% unique() %>% length(),
                  school = school_performance_dat$school_id, 
                  N = nrow(school_performance_dat),
                  xpred.k = rep(0:7, 12),
                  school_pred = rep(1:12, each = 8),
                  npred = 8*12)
  
parnames <- c("ytilde.k","mu_pred.k","mu_private_pred.k","yrep","alpha.j","beta","mu_alpha.j","gamma0","gamma1","sigma","sigma_alpha")
  
mod <- jags(data = jags.data, 
              parameters.to.save = parnames, 
              model.file = textConnection(bhregmodel),
              n.iter = 10000,
              n.burnin = 2000,
              n.thin = 4)

m <- mod$BUGSoutput$sims.matrix
```

## Results: parameter estimates {style="font-size: 65%;"}

```{r}
par_summary <- m %>% 
                gather_rvars(alpha.j[1:10],beta,mu_alpha.j[1:10],sigma,sigma_alpha) %>% 
                median_qi(.value)
par_summary %>% print(n = 23)

```

## Results: school specific intercepts {style="font-size: 65%;"}

Recall: $\alpha_j \sim N(\gamma_0 + \gamma_1u_j, \sigma_{\alpha}^2)$

```{r, warning=FALSE}
alpha_ind <- 1:jags.data$n_school
alpha_samps <- m %>% spread_draws(alpha.j[alpha_ind]) %>% 
    dplyr::select(alpha_ind, alpha.j) %>% dplyr::ungroup() %>% 
    dplyr::mutate(alpha_ind = as.factor(alpha_ind))

ggplot(alpha_samps, aes(x = alpha_ind, y = alpha.j)) +
  geom_boxplot(colour = jags.data$u.j+1) +
  xlab("school") +
  geom_hline(yintercept = mod$BUGSoutput$mean$gamma0) +
  geom_hline(yintercept = mod$BUGSoutput$mean$gamma0 + mod$BUGSoutput$mean$gamma1) 
```

<!-- ## Results: Posterior samples of $\mu_{ij}$ {style="font-size: 65%;"} -->

<!-- ```{r} -->
<!-- pred_ind <- 1:jags.data$npred -->
<!-- pred_samps <- m %>% spread_draws(mu_pred.k[pred_ind])  -->

<!-- pred_samps$traj <- rep(1:dim(m)[1],jags.data$npred) -->
<!-- pred_samps$x <- rep(jags.data$xpred.k, each = dim(m)[1]) -->
<!-- pred_samps$school_id <- rep(jags.data$school_pred, each = dim(m)[1]) -->

<!-- pred_samps <- pred_samps %>% arrange(school_id,x) %>% mutate(traj = as.factor(traj)) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- ggplot(school_performance_dat, aes(x = homework_completed, y = math_score)) + -->
<!--   geom_point(alpha = 0.3) + -->
<!--   geom_line(data = pred_samps %>% filter(traj == c(1:20)),aes(x = x, y = mu_pred.k,group = traj),alpha = 0.3) + -->
<!--   theme_bw() + -->
<!--   facet_wrap(~school_id) -->
<!-- ``` -->

## Results: Regression lines + 95% Credible Intervals {style="font-size: 65%;"}

```{r}
pred_summary <- m %>% 
                gather_rvars(mu_pred.k[pred_ind]) %>% 
                median_qi(.value) %>% 
                mutate(x = jags.data$xpred.k,
                       school_id = jags.data$school_pred)

ggplot(pred_summary,aes(x = x, y = .value),alpha = 0.3) +
  geom_line() +
  geom_ribbon(data = pred_summary,aes(ymin = .lower, ymax = .upper), alpha = 0.2) +
  geom_point(data = school_performance_dat, aes(x = homework_completed,y=math_score)) +
  theme_bw() +
  facet_wrap(~school_id)

```

## Results: Overall regression lines for each school type {style="font-size: 65%;"}

```{r}
pred_private_summary <- m %>% 
                gather_rvars(mu_private_pred.k[pred_ind]) %>% 
                median_qi(.value) %>% 
                mutate(x = jags.data$xpred.k,
                       school_id = jags.data$school_pred,
                       private = ifelse(school_id %in% private_schools, 1,0))

ggplot(pred_private_summary,aes(x = x, y = .value),alpha = 0.3) +
  geom_line() +
  geom_ribbon(data = pred_private_summary,aes(ymin = .lower, ymax = .upper), alpha = 0.2) +
  geom_point(data = school_performance_dat, aes(x = homework_completed,y=math_score)) +
  theme_bw() +
  ylab("math score") +
  xlab("homework") +
  facet_wrap(~private)

```

## Posterior predicitive checks (density overlay) {style="font-size: 65%;"}

```{r}
y <- school_performance_dat$math
yrep <- mod$BUGSoutput$sims.list$yrep

ppc_dens_overlay (y, yrep[1:50, ])
```

## Posterior predicitive checks (test statistic) {style="font-size: 65%;"}

Test statistic = max(y) - min(y)

```{r, warning=FALSE}
r <- function(y) max(y) - min(y)
ppc_stat_grouped(y,yrep,group = school_performance_dat$private,  stat = "r",binwidth = 2)
```

## Further extend the model to include varying slopes {style="font-size: 65%;"}

$y_{i} \sim N(\mu_i,\sigma^2)$

$\mu_i = \alpha_{j[i]} + \beta_{j[i]} x_i$ for $i = 1 \ldots n$

$\beta \sim Normal(\mu_{\beta}, \sigma_{\beta})$

![](images/Screenshot 2024-11-22 at 09.50.02.png)

```{r, eval = FALSE}
grViz("
digraph boxes_and_circles {

  # a 'graph' statement
  graph [compound = true, fontsize = 10]

  subgraph cluster0 {
  
  # several 'node' statements
  node [shape = box,
        width = 0.5]
  y[label = <y<sub>ij</sub>>]
  
  node [shape = box,
        fixedsize = true,
        width = 0.5] // sets as circles
  x[label = <x<sub>ij</sub>>] 


  node [shape = circle,
        fixedsize = true,
        width = 0.5] // sets as circles
  mu[label = <&mu;<sub>ij</sub>>] 



  node [shape = plaintext,
  peripheries = 0,
        fontname = Helvetica]
  i[label = i]


  x->mu mu->y  i
  

}

  subgraph cluster1 {
  
    node [shape = circle,
        fixedsize = true,
        width = 0.5] // sets as circles
  alpha[label = <&alpha;<sub>j</sub>>] 

  node [shape = circle,
        fixedsize = true,
        width = 0.5] // sets as circles
  beta[label = <&beta;<sub>j</sub>>] 


    node [shape = circle,
        fixedsize = true,
        width = 0.6] // sets as circles
  mualpha[label = <&mu;<sub>&alpha;</sub><sub>j</sub>>] 


    node [shape = plaintext,
  peripheries = 0,
        fontname = Helvetica]
  j[label = j]


alpha->mu j mualpha -> alpha beta -> mu
  
  }

  node [shape = circle,
        fixedsize = true,
        width = 0.5]
  sigma[label = <&sigma;>]
  

  node [shape = circle,
        fixedsize = true,
        color = red,
        fontcolor = red,
        peripheries = 1,
        width = 0.5] // sets as circles
  gamma0[label = <&gamma;<sub>0</sub>>]
  
  node [shape = circle,
        fixedsize = true,
        color = red,
        fontcolor = red,
        peripheries = 1,
        width = 0.5] // sets as circles
  gamma1[label = <&gamma;<sub>1</sub>>]

  
  node [shape = circle,
        fixedsize = true,
        color = red,
        fontcolor = red,
        peripheries = 1,
        width = 0.5] // sets as circles
  nualpha[label = <&sigma;<sub>&alpha;</sub>>]
  
    node [shape = circle,
        fixedsize = true,
        color = red,
        fontcolor = red,
        peripheries = 1,
        width = 0.5] // sets as circles
  mubeta[label = <&mu;<sub>&beta;</sub>>]

     node [shape = circle,
        fixedsize = true,
        color = red,
        fontcolor = red,
        peripheries = 1,
        width = 0.5] // sets as circles
  sigbeta[label = <&sigma;<sub>&beta;</sub>>]

  
  # node [shape = circle,
  #       fixedsize = true,
  #       color = red,
  #       fontcolor = red,
  #       peripheries = 1,
  #       width = 0.5] // sets as circles
  # betap[label = <&beta;<sub>P</sub>>]
  # 
  # node [shape = circle,
  #       fixedsize = true,
  #       color = red,
  #       fontcolor = red,
  #       peripheries = 1,
  #       width = 0.5] // sets as circles
  # nubeta[label = <&nu;<sub>&beta;</sub>>]




 nualpha -> alpha  sigma->y gamma0 -> mualpha gamma1 -> mualpha mubeta -> beta sigbeta -> beta
}
",
width = 200, height = 200)
```

```{r}
bhregmodel = "
model{
  for(i in 1:N)
{
 y.i[i] ~ dnorm(mu.i[i],sigma^-2) # data model
 mu.i[i] <- alpha.j[school[i]] + beta.j[school[i]]*x.i[i]
} # end i loop

for(j in 1:n_school)
{
alpha.j[j] ~ dnorm(mu_alpha.j[j],sigma_alpha^-2)
mu_alpha.j[j] <- gamma0 + gamma1*u.j[j]

beta.j[j] ~ dnorm(mu_beta,sigma_beta)
}

mu_beta ~ dnorm(0,10^-2)
sigma_beta ~ dt(0,2^-2,1)T(0,)

gamma0 ~ dunif(0,100)
gamma1 ~ dnorm(0,10^-2)

sigma ~ dunif(0,30)
sigma_alpha ~ dt(0,2^-2,1)T(0,)

}"
```

```{r, include = FALSE, echo = FALSE}
bhregmodel = "
model{
  for(i in 1:N)
{
 y.i[i] ~ dnorm(mu.i[i],sigma^-2) # data model
 mu.i[i] <- alpha.j[school[i]] + beta.j[school[i]]*x.i[i]
} # end i loop

for(j in 1:n_school)
{
alpha.j[j] ~ dnorm(mu_alpha.j[j],sigma_alpha^-2)
mu_alpha.j[j] <- gamma0 + gamma1*u.j[j]

beta.j[j] ~ dnorm(mu_beta,sigma_beta)
}

mu_beta ~ dnorm(0,10^-2)
sigma_beta ~ dt(0,2^-2,1)T(0,)

gamma0 ~ dunif(0,100)
gamma1 ~ dnorm(0,10^-2)

sigma ~ dunif(0,30)
sigma_alpha ~ dt(0,2^-2,1)T(0,)


  for(k in 1:npred)
{
 ytilde.k[k] ~ dnorm(mu_pred.k[k],sigma^-2) # data model
 mu_pred.k[k] <- alpha.j[school_pred[k]] + beta.j[school_pred[k]]*xpred.k[k]
 mu_private_pred.k[k] <- mu_alpha.j[school_pred[k]] + mu_beta*xpred.k[k]
} # end k loop


  for(i in 1:N)
{
 yrep[i] ~ dnorm(mu.i[i],sigma^-2) # data model
} # end k loop

 }
 "

jags.data2 <- list(y.i = school_performance_dat$math,
                  x.i = school_performance_dat$homework,
                  u.j = school_performance_dat %>% group_by(school_id) %>% summarise(private = unique(private)) %>% pull(private),
                  n_school = school_performance_dat$school_id %>% unique() %>% length(),
                  school = school_performance_dat$school_id, 
                  N = nrow(school_performance_dat),
                  xpred.k = rep(0:7, 12),
                  school_pred = rep(1:12, each = 8),
                  npred = 8*12)
  
parnames2 <- c("ytilde.k","mu_pred.k","mu_private_pred.k","yrep","alpha.j","beta.j","mu_alpha.j","gamma0","gamma1","sigma","sigma_alpha")
  
mod2 <- jags(data = jags.data2, 
              parameters.to.save = parnames2, 
              model.file = textConnection(bhregmodel))

m2 <- mod2$BUGSoutput$sims.matrix
```

## Results: Compare Regression lines {style="font-size: 65%;"}

```{r, include = FALSE, echo = FALSE}
regmodel = "
model{
  for(i in 1:N)
{
 y.i[i] ~ dnorm(mu.i[i],sigma^-2) # data model
 mu.i[i] <- alpha.j[school[i]] + beta.j[school[i]]*x.i[i]
} # end i loop

for(j in 1:n_school)
{
beta.j[j] ~ dnorm(0,10^-2)
alpha.j[j] ~ dunif(0,100)
}

sigma ~ dunif(0,30)

  for(k in 1:npred)
{
 ytilde.k[k] ~ dnorm(mu_pred.k[k],sigma^-2) # data model
 mu_pred.k[k] <- alpha.j[school_pred[k]] + beta.j[school_pred[k]]*xpred.k[k]
} # end k loop


  for(i in 1:N)
{
 yrep[i] ~ dnorm(mu.i[i],sigma^-2) # data model
} # end k loop

 }
 "

jags.data3 <- list(y.i = school_performance_dat$math_score,
                  x.i = school_performance_dat$homework_completed,
                  N = nrow(school_performance_dat),
                  xpred.k = rep(0:7, 12),
                  n_school = school_performance_dat$school_id %>% unique() %>% length(),
                  school = school_performance_dat$school_id, 
                  school_pred = rep(1:12, each = 8),
                  npred = 8*12)
  
parnames3 <- c("ytilde.k","mu_pred.k","yrep","alpha.j","beta.j","sigma")
  
mod3 <- jags(data = jags.data3, 
              parameters.to.save = parnames3, 
              model.file = textConnection(regmodel))

m3 <- mod3$BUGSoutput$sims.matrix
```

```{r}
pred_summary2 <- m2 %>% 
                gather_rvars(mu_pred.k[pred_ind]) %>% 
                median_qi(.value) %>% 
                mutate(x = jags.data2$xpred.k,
                       school_id = jags.data2$school_pred)

pred_summary3 <- m3 %>% 
                gather_rvars(mu_pred.k[pred_ind]) %>% 
                median_qi(.value) %>% 
                mutate(x = jags.data$xpred.k,
                       school_id = jags.data2$school_pred)


ggplot(data = pred_summary,aes(x = x, y = .value),alpha = 0.3) +
  geom_line(aes(colour = "BHM - common slope")) +
  geom_line(data = pred_summary2,aes(x = x, y = .value, colour = "BHM - varying slope"),alpha = 0.3) +
  geom_line(data = pred_summary3,aes(x = x, y = .value, colour = "SLR"),alpha = 0.3) +
  #geom_ribbon(data = pred_summary,aes(ymin = .lower, ymax = .upper), alpha = 0.2) +
  geom_point(data = school_performance_dat, aes(x = homework_completed,y=math_score)) +
  theme_bw() +
  facet_wrap(~school_id) +
  labs(colour = "")

```


## Zoom in {style="font-size: 65%;"}

```{r}
ggplot(data = pred_summary,aes(x = x, y = .value),alpha = 0.3) +
  geom_line(aes(colour = "BHM - common slope")) +
  geom_line(data = pred_summary2,aes(x = x, y = .value, colour = "BHM - varying slope"),alpha = 0.3) +
  geom_line(data = pred_summary3,aes(x = x, y = .value, colour = "SLR"),alpha = 0.3) +
  #geom_ribbon(data = pred_summary,aes(ymin = .lower, ymax = .upper), alpha = 0.2) +
  geom_point(data = school_performance_dat, aes(x = homework_completed,y=math_score)) +
  theme_bw() +
  facet_wrap(~school_id, scales = "free_y") +
  labs(colour = "")
```

## Posterior predicitive checks (density overlay) {style="font-size: 65%;"}

```{r, warning=FALSE}
y <- school_performance_dat$math
yrep3 <- mod3$BUGSoutput$sims.list$yrep
ppc_dens_overlay (y, yrep3[1:50, ])
```

## Posterior predicitive checks (test statistic) {style="font-size: 65%;"}

Test statistic = max(y) - min(y)

```{r, warning = FALSE}
r <- function(y) max(y) - min(y)
ppc_stat_grouped(y,yrep3,group = school_performance_dat$private,  stat = "r",binwidth = 2)
# ppc_stat_grouped(y,yrep,group = school_performance_dat$private,  stat = "median")
```
