---
title: "Bayesian Data Analysis"
subtitle: "MCMC Diagnostics"
author: "Dr Niamh Cahill (she/her)"
format: revealjs
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(rstanarm)
library(coda)
data(kidiq)

mcmc_theta <- function(sigma_pro = 0.01,
                       n_iter = 1000,
                       a = 1,
                       b = 1,
                       y = 14, 
                       n = 20)
{

# 0. 
theta_cur <- array(NA, c(n_iter,3))
theta_cur[1,1] <- 0.5
theta_cur[1,2] <- 0.01
theta_cur[1,3] <- 0.9

for(j in 1:3)
{
for(i in 1:(n_iter-1))
{

# 1.
theta_pro <- theta_cur[i,j] + rnorm(1,0,sigma_pro)

# 2. 
if(theta_pro<0|theta_pro>1)
{
  r <- 0 # set to zero if theta outside [0,1]
}
else
{
r <- 
min(1,dbinom(y,n,theta_pro)*dbeta(theta_pro,a,b)/
      dbinom(y,n,theta_cur[i,j])*dbeta(theta_cur[i,j],a,b))
}

# 3. 
u <- runif(1,0,1)
accept <- u < r

theta_cur[i+1,j]<- ifelse(accept,theta_pro,theta_cur[i,j])

}
}

return(theta_cur)
}
```

## Recap: Gibbs Sampling Algorithm for the Kid IQ example {style="font-size: 65%;"}

Recall data (y) are available on the cognitive test scores of three- and four-year-old children in the USA. The sample contains $n=434$ observations.

::: columns
::: {.column width="50%"}
$y \sim Normal(\mu, \sigma^2)$

$\mu \sim Normal(80 ,10)$ $1/\sigma^2 = \tau \sim gamma(1 ,1)$

0.  Let's initialise using $\mu^{(1)} = 80, \tau^{(1)} = 1$

1.  Given starting values $\mu^{(1)}$ and $\tau^{(1)}$, draw samples s = 2, 3,....,\`some large number' as follows:

    1.1. sample $\tau^{(s)}$ from $p(\tau|y,\mu^{(s-1)})$.

    1.2. sample $\mu^{(s)}$ from $p(\mu|y,\tau^{(s)})$

2.  Repeat and this will (eventually) generate samples from $p(\mu,\sigma|y)$, which is what we want!
:::

::: {.column width="50%"}
```{r, echo = TRUE, include=TRUE}
#| code-line-numbers: "1-11|13-16|20-30|19,31"

## Data
y <- kidiq$kid_score
n <- length(y)

## Prior parameters 
mu0 <- 80; sigma.mu0 <- 10 # for normal prior (for mu)
a <- 1; b <- 1 # for gamma prior (for tau)

n_iter <- 10000 ## choose # iter
## create objects to store results
mu_s <- tau_s <- sigma_s <- rep(NA, n_iter)

## 0. Initialise 
mu_s[1] <- 80; tau_s[1] <- 1; sigma_s[1] <- 1
## define parameters for gamma posterior on tau
post.a = a + n/2; post.b = 1/2*(sum((y-mu_s[1])^2))

## 1.
for(s in 2:n_iter){
## 1.1 sample from complete conditional for tau
tau_s[s] <- rgamma(1,post.a,post.b)
sigma_s[s] <- sqrt(1/tau_s[s]) # transform to sigma
## update posterior parameters for mu
mupost.mean = (mu0/(sigma.mu0^2) + tau_s[s]*n*mean(y))/(1/(sigma.mu0^2) + tau_s[s]*n)
mupost.sd = sqrt(1/(1/(sigma.mu0^2)+tau_s[s]*n))
## 1.2 sample from complete conditional for mu
mu_s[s] <- rnorm(1,mupost.mean,mupost.sd)
## update posterior parameters for tau
post.a = a + n/2
post.b = b + 1/2*(sum((y-mu_s[s])^2))
} # end s loop

```
:::
:::

## Recap: Output from MCMC {style="font-size: 65%;"}

MCMC samples are not *independent* draws from a target distribution:

-   The first draw is not a random draw from the target distribution and tuning of MCMC parameters may occur at the start of a chain.

-   Subsequently, draw s + 1 depends on draw s: the samples may be autocorrelated.

We can use samples from an MCMC algorithm to do inference but ONLY IF

-   We exclude samples from the initial period (burn-in).

-   We "wait long enough" to get a set of samples that are representative of the target (posterior). distribution.

```{r, echo = TRUE, include=TRUE}
#| output-location: column-fragment

# Create a dataset (tibble) of samples
# chose a "burn-in" 
n_burnin <- 100
# Remove the first sample from mu_s and sigma_s
post_samps <- tibble(sample_index = 1:(n_iter-n_burnin),
                     mu_s = mu_s[-(1:n_burnin)], 
                     sigma_s = sigma_s[-(1:n_burnin)])
post_samps
```

## Recap: Trace Plots {style="font-size: 65%;"}

We can use trace plots to investigate the MCMC samples/chains for the parameters.

```{r, echo = TRUE, include=TRUE}
#| output-location: column-fragment

library(ggplot2)

# Trace plot for mu
p1 <- ggplot(post_samps, aes(x = sample_index, mu_s)) +
        geom_line(colour = "blue") +
        ggtitle("Posterior Samples of mu")

# Trace plot for sigma
p2 <- ggplot(post_samps, aes(x = sample_index, sigma_s)) +
        geom_line(colour = "blue") +
        ggtitle("Posterior Samples of sigma")

p1;p2
```

## Recap: Density Plots and Summaries {style="font-size: 60%;"}

We can visualise and obtain summaries for the marginal posterior distributions of the parameters based on the MCMC samples.

```{r, echo = TRUE, include=TRUE}
#| output-location: column-fragment

# Density plot for posterior samples of mu
p5 <- ggplot(post_samps, aes(x = mu_s)) +
        geom_density(fill = "lightblue", 
                     color = "blue", 
                     alpha = 0.6) +
        ggtitle("Posterior Density of mu") 

# Mean and credible interval for mu
mu_mean <- mean(mu_s)
mu_ci <- quantile(mu_s, probs = c(0.025, 0.975))

p5;cat("Mean of Mu:", mu_mean, "\n");cat("95% Credible Interval for Mu:", mu_ci, "\n")
```

```{r, echo = TRUE, include=TRUE}
#| output-location: column-fragment

p6 <- ggplot(post_samps, aes(x = sigma_s)) +
        geom_density(fill = "lightblue", 
                     color = "blue", 
                     alpha = 0.6) +
        ggtitle("Posterior Density of sigma") 

# Mean and credible interval for sigma
sigma_mean <- mean(sigma_s)
sigma_ci <- quantile(sigma_s, probs = c(0.025, 0.975))

p6;cat("Mean of Sigma:", sigma_mean, "\n");cat("95% Credible Interval for Sigma:", sigma_ci, "\n")
```

## MCMC Reprentativeness, Accuracy and Efficiency {style="font-size: 70%;"}

We have 3 main goals in generating an MCMC sample from the target (posterior) distribution.

1.  The values must be representative of the posterior distribution. They shouldn't be influenced by the initial values of the chain. They should explore the full range of the parameter space.

2.  The chain should be a sufficient size so that estimates are accurate and stable. Estimates and uncertainty intervals should not be much different if the MCMC is run again.

3.  The chain should be generated as efficiently as possible.

We cannot run chains for an infinitely long time so we must check the quality based on a set of finite samples from the chain.

We use a set of convergence diagnostics to check the quality.

## Burn-in and Mixing {style="font-size: 65%;"}

-   Burn in phase = initial phase of an MCMC chain, when the chain converges away from initial values towards the target (posterior) distribution.

    -   Samples from the burn-in should be completely ignored.

-   Mixing refers to how well and how quickly a Markov chain explores and converges to the target probability distribution.

    -   We can never be sure that a chain has converged but we can detect a lack of convergence i.e., if chains don't overlap (don't mix).

::: columns
::: {.column width="50%"}
How many samples would you "burn in"?

```{r}
# Load necessary libraries
library(ggplot2)
library(coda)

# Set seed for reproducibility
set.seed(42)

# Define burn-in length
burn_in_length <- 100

# Generate synthetic MCMC chains with a burn-in period
chain_1 <- c(rnorm(burn_in_length, mean = -2, sd = 1), rnorm(400, mean = 0, sd = 1))
chain_2 <- c(rnorm(burn_in_length, mean = 2, sd = 1), rnorm(400, mean = 0, sd = 1))
chain_3 <- c(rnorm(burn_in_length, mean = 0, sd = 1), rnorm(400, mean = 0, sd = 1))

# Combine chains into a matrix after the burn-in period
chains <- as.mcmc.list(list(as.mcmc(chain_1[(burn_in_length + 1):length(chain_1)]),
                            as.mcmc(chain_2[(burn_in_length + 1):length(chain_2)]),
                            as.mcmc(chain_3[(burn_in_length + 1):length(chain_3)])))

# Compute R-hat using coda package (post burn-in)
r_hat_value <- gelman.diag(chains)$psrf[1]

# Prepare data for plotting
iterations <- 1:(burn_in_length + 400)
data <- data.frame(
  Iteration = rep(iterations, 3),
  Chain = factor(rep(1:3, each = length(iterations))),
  Value = c(chain_1, chain_2, chain_3)
)

# Plot the MCMC chains with a burn-in period
ggplot(data, aes(x = Iteration, y = Value, color = Chain)) +
  geom_line(alpha = 0.7) +
  geom_vline(xintercept = burn_in_length, linetype = "dashed", color = "red", 
             label = "Burn-in Period") +
  labs(title = paste("Trace Plots of MCMC Chains with Burn-in"),
       x = "Iteration", y = "Sample Value") +
  theme_minimal() +
  theme(legend.position = "bottom")
```
:::

::: {.column width="50%"}
Which chains are mixing well?

```{r, fig.height=6}

theta_cur1 <- as_tibble(mcmc_theta(sigma_pro = 0.01))
names(theta_cur1) <- paste0("chain",1:3)
theta_cur1$ex <- "example1"
theta_cur1 <- theta_cur1 %>% pivot_longer(-ex,
                          names_to = "chain",
                          values_to = "theta")


theta_cur2 <- as_tibble(mcmc_theta(sigma_pro = 0.1))
names(theta_cur2) <- paste0("chain",1:3)
theta_cur2$ex <- "example2"
theta_cur2 <- theta_cur2 %>% pivot_longer(-ex,
                          names_to = "chain",
                          values_to = "theta")

theta_cur <- rbind(theta_cur1,theta_cur2)

ggplot(theta_cur, aes(x = c(rep(1:1000,each = 3),rep(1:1000,each = 3)),y = theta)) +
  geom_line(aes(colour = chain)) +
  facet_wrap(~ex, nrow = 2) +
  xlab("Sample Value") +
  ylab(expression(theta)) +
  labs(colour = "") +
  theme_minimal()
```
:::
:::

<!-- ## MCMC diagnostics: did we obtain a representative sample? {style="font-size: 70%;"} -->

<!-- After excluding warmup, you need to check if you’ve generated “enough representative samples” -->

<!-- How many MCMC samples are needed depends on: -->

<!--   - Required precision of your estimates (MC error) -->

<!--     - higher precision means we need more samples. -->

<!--   - How fast the chain moves around the sample space, the autocorrelation in the sampled values. -->

<!--     - relates to how well the chains mix. -->

## MCMC diagnostics: Potential Scale Reduction Factor ($\hat{R}$) {style="font-size: 60%;"}

The **Potential Scale Reduction Factor (R-hat)**, also called the **Gelman-Rubin diagnostic**, is used to assess the convergence of multiple MCMC chains in Bayesian analysis. It compares the variance within each chain to the variance between chains, indicating whether the chains have converged to the same target distribution.

### How R-hat Works:

-   **Within-Chain Variance**: Measures the variability of samples within a single chain.
-   **Between-Chain Variance**: Measures the variability between the means of multiple chains.
-   If the chains are converging well, the within-chain and between-chain variances should be similar.

### Interpretation:

-   **R-hat close to 1**: Indicates convergence; the chains are mixing well.
-   **R-hat \> 1**: Suggests that the chains have not yet converged, and more sampling may be needed.

## MCMC diagnostics: Potential Scale Reduction Factor ($\hat{R}$) {style="font-size: 60%;"}

The graph above shows trace plots for three MCMC chains with an R-hat value close to 1, indicating good convergence. Each chain explores similar regions of the parameter space, reflecting similar means and variances. As a result, R-hat suggests that the chains are sampling from the same distribution effectively

```{r}
# Load necessary libraries
library(ggplot2)
library(coda)

# Set seed for reproducibility
set.seed(42)

# Generate synthetic MCMC chains
chain_1 <- rnorm(1000, mean = 0, sd = 1)
chain_2 <- rnorm(1000, mean = 0, sd = 1)
chain_3 <- rnorm(1000, mean = 0, sd = 1)

# Combine chains into a matrix
chains <- as.mcmc.list(list(as.mcmc(chain_1), as.mcmc(chain_2), as.mcmc(chain_3)))

# Compute R-hat using coda package
r_hat_value <- gelman.diag(chains)$psrf[1]

# Prepare data for plotting
iterations <- 1:1000
data <- data.frame(
  Iteration = rep(iterations, 3),
  Chain = factor(rep(1:3, each = 1000)),
  Value = c(chain_1, chain_2, chain_3)
)

# Plot the MCMC chains
ggplot(data, aes(x = Iteration, y = Value, color = Chain)) +
  geom_line(alpha = 0.7) +
  labs(title = paste("Trace Plots of MCMC Chains with R-hat =", round(r_hat_value, 3)),
       x = "Iteration", y = "Sample Value") +
  theme_minimal() +
  theme(legend.position = "bottom")


```

## MCMC diagnostics: Potential Scale Reduction Factor ($\hat{R}$) {style="font-size: 60%;"}

The R-hat value signals that the chains haven't converged.

```{r}
# Load necessary libraries
library(ggplot2)
library(coda)

# Set seed for reproducibility
set.seed(42)

# Generate synthetic MCMC chains with slightly more separation
chain_1 <- rnorm(500, mean = 0, sd = 1)
chain_2 <- rnorm(500, mean = 2, sd = 1)
chain_3 <- rnorm(500, mean = -2, sd = 1)

# Combine chains into a matrix
chains <- as.mcmc.list(list(as.mcmc(chain_1), as.mcmc(chain_2), as.mcmc(chain_3)))

# Compute R-hat using coda package
r_hat_value <- gelman.diag(chains)$psrf[1]

# Prepare data for plotting
iterations <- 1:500
data <- data.frame(
  Iteration = rep(iterations, 3),
  Chain = factor(rep(1:3, each = 500)),
  Value = c(chain_1, chain_2, chain_3)
)

# Plot the MCMC chains
ggplot(data, aes(x = Iteration, y = Value, color = Chain)) +
  geom_line(alpha = 0.7) +
  labs(title = paste("Trace Plots of Slightly Separated MCMC Chains with R-hat =", round(r_hat_value, 3)),
       x = "Iteration", y = "Sample Value") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

## MCMC diagnostics: Potential Scale Reduction Factor ($\hat{R}$) {style="font-size: 60%;"}

The R-hat value signals that the chains haven't fully converged, even though the differences between them aren't extreme.

```{r}
# Load necessary libraries
library(ggplot2)
library(coda)

# Set seed for reproducibility
set.seed(42)

# Generate synthetic MCMC chains with moderate separation (looks converged but subtle differences)
chain_1 <- rnorm(500, mean = 0, sd = 1)
chain_2 <- rnorm(500, mean = 1, sd = 1)
chain_3 <- rnorm(500, mean = -1, sd = 1)

# Combine chains into a matrix
chains <- as.mcmc.list(list(as.mcmc(chain_1), as.mcmc(chain_2), as.mcmc(chain_3)))

# Compute R-hat using coda package
r_hat_value <- gelman.diag(chains)$psrf[1]

# Prepare data for plotting
iterations <- 1:500
data <- data.frame(
  Iteration = rep(iterations, 3),
  Chain = factor(rep(1:3, each = 500)),
  Value = c(chain_1, chain_2, chain_3)
)

# Plot the MCMC chains
ggplot(data, aes(x = Iteration, y = Value, color = Chain)) +
  geom_line(alpha = 0.7) +
  labs(title = paste("Trace Plots of Moderately Poor Mixing MCMC Chains with R-hat =", round(r_hat_value, 3)),
       x = "Iteration", y = "Sample Value") +
  theme_minimal() +
  theme(legend.position = "bottom")

```

## MCMC diagnostics: Autocorrelation {style="font-size: 70%;"}

-   Mont Carlo samples *are* independent draws from a target distribution.

-   MCMC samples *are NOT* independent draws from a target distribution, because:

    1.  The first draw is set by the user and thus not a random draw from the target distribution.
    2.  Subsequently, draw $s+1$ depends on draw $s$ - samples are autocorrelated

In Bayesian analysis, **autocorrelation** measures how correlated samples from a MCMC chain are with their past values. High autocorrelation indicates that the samples are not exploring the parameter space effectively, which can lead to inefficient sampling and slow convergence.

## MCMC diagnostics: Autocorrelation {style="font-size: 70%;"}

### Why Autocorrelation Matters:

-   **Effective Sample Size**: High autocorrelation reduces the effective sample size of the chain, meaning that even though you have a lot of samples, they may not provide much new information.
-   **Mixing**: It helps assess the mixing of the chains. Good mixing results in low autocorrelation, indicating that the chains are moving freely through the parameter space.

### How to Compute Autocorrelation:

1.  **Autocorrelation Function (ACF)**: You can compute the autocorrelation for different lags (how far back in the chain you want to look) and visualize it to assess the behavior of the MCMC chain.

2.  **Visual Representation**: The autocorrelation plot (ACF plot) displays the autocorrelation coefficients for various lags, helping visualize how quickly the chain becomes uncorrelated.

## MCMC diagnostics: Autocorrelation {style="font-size: 70%;"}

We can use autocorrelation function (ACF) plots to help us diagnose autocorrelation issues.

**High autocorrelation**

```{r}
theta_samps = mcmc_theta(sigma_pro = 0.015,
                    n_iter = 10000)


theta.mcmc <- as.mcmc(theta_samps)
autocorr.plot(theta.mcmc[,1])
```

## MCMC diagnostics: Autocorrelation {style="font-size: 70%;"}

We can use autocorrelation function (ACF) plots to help us diagnose autocorrelation issues.

**Reduced autocorrelation**

```{r}
theta_samps = mcmc_theta(sigma_pro = 0.2,
                    n_iter = 10000)


theta.mcmc <- as.mcmc(theta_samps)
autocorr.plot(theta.mcmc[,1])
```

## MCMC diagnostics: effective sample size (ESS) {style="font-size: 70%;"}

We want to know what the sample size of a non-autocorrelated chain, that yields the same information, would be. An answer to this question can be provided with a measure called *effective sample size (ESS)*

The effective sample size (ESS) divides the actual sample size by the amount of autocorrelation.

$$ESS = \frac{N}{1 + 2\sum_{k=1}^\infty\rho_k}$$

where $\rho_k$ is the autocorrelation of the chain at lag $k$. A good rule of thumb for the ESS is for it to be 10% of the total number of samples.

## Thinning {style="font-size: 70%;"}

- **Thinning** is the practice of **retaining only every _k-th_ sample** from a Markov Chain in MCMC sampling.

  - Typically used to **reduce autocorrelation** in the chain or manage **large storage** demands.


 **How Thinning Works**
 
- Given a sequence of samples: \[ x_1, x_2, x_3, ..., x_N \]

- Thinning by a factor of _k_: Retain \[ x_k, x_{2k}, x_{3k}, ... \]

- Thinning factor (k): Number of iterations between successive retained samples.


**Best Practices**

1. **Avoid thinning if possible**: It can unnecessarily discard valuable samples.

2. Use **diagnostics** like autocorrelation plots and the **effective sample size** (which we'll talk about next) to decide whether thinning is needed.

3. If using, choose a thinning factor based on the **autocorrelation time** of the chain.

---


## MCMC diagnostics: Monte Carlo Standard Error (MCSE) {style="font-size: 70%;"}

Another useful measure for the effective accuracy of the chain is the Monte Carlo standard error (MCSE).

-   The standard deviation (SD) of the sample mean accross many replications is called the standard error and is estimated as SE = SD/$\sqrt{N}$

-   So as the sample size $N$ increases the SE decreases. In other words the bigger the sample, the more precise the estimate.

Extending this to MCMC chains, we substitute the sample size N with the ESS to get

MCSE = SD/$\sqrt{\text{ESS}}$

where SD is the standard deviation of the chain.

## MCMC efficiency {style="font-size: 70%;"}

There are a number of ways to attempt to improve efficiency in the MCMC process

1.  Run chains in parallel

2.  Adjust the sampling algorithm e.g., use Gibbs instead of Metropolis

3.  Change model parameterisation (e.g., mean center the data for regression analysis)
