---
title: "ST405/ST645 Bayesian Data Analysis"
subtitle: "Tutorial Questions (4)"
author: "Prof. Niamh Cahill"
format: 
  html:
    embed-resources: true
    self-contained-math: true
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
head(Carseats)
library(tidyverse)
library(rjags)
library(R2jags)
library(tidybayes)
library(bayesplot)
```

## **Introduction**

This tutorial explores Bayesian Hierarchical Regression Modelling.

Let's create a dataset to explore the idea of independent modelling versus pooling versus partial pooling (via hierarchical modelling). We will create a simple one-dimensional regression problem, i.e. there is a single predictor and a single outcome. There are eight different groups, with each group having its own slope, and a fixed intercept of zero. The code for simulating the data is given below.

```{r, fig.width=5, fig.height=4}
set.seed(28) # set the seed
mean_slope = 2 # the 8 different slopes have a mean of 2
sigma_slope = 0.2 # across group variation
slopes = rnorm(8,mean_slope,sigma_slope) # groups slopes vary around the mean
groups = c(rep(1:6,each = 14),rep(7,5), rep(8,5)) # some groups have smaller sample sizes
sigma <- c(0.5,0.5,2,2,0.5,2,0.5,0.5) # have some groups with large variation
x = rnorm(length(groups)) # simulate a predictor
y =  slopes[groups]*x + rnorm(length(groups),0,sigma[groups]) # simulate y
y[length(groups)] <- 8 # add an outlier
sim_dat <- tibble(x = x, y = y, group = groups) # create the simulated dataset

## Plot the simulated data
ggplot(sim_dat, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~group)
```

**Some things to note:**

-   The groups variable contains the group each observation belongs to. Groups 1 to 6 have fourteen data points and groups 7,8 have five data points.

-   Groups 3,4 and 6 have more variation than the other groups.

-   An outlier has been added to group 8.

### **Question: Bayesian Hierarcical Regression Modelling - Simulated Data**

**1. Fit a regression model with varying slopes**

-   Fit regression model with a common intercept and varying slopes such that

$$y_i \sim N(\alpha + \beta_{j[i]} x_i, \sigma)$$

-   Assume independent slopes, such that $\beta_{j} \sim N(0, 10^2)$

------------------------------------------------------------------------

**2. Change the model structure**

-   Extend 1 to model $\beta_{j}$ hierarchically instead of independently

------------------------------------------------------------------------

**3. Compare posterior distributions**

-   For each model (1 & 2), plot the posterior distributions for the slope parameters and compare the distributions to the true slopes used to simulate the data.

-   Note the differences and similarities between the parameter estimates for both models.

------------------------------------------------------------------------

**4. Look at hyperparameters**

-   For the model in 2, compare the distributions for the model hyperparmaters to the true values from the simulation above.

------------------------------------------------------------------------

**5. Modify the simulation**

-   Change some of the simulation settings and see how it impacts 1-4.

------------------------------------------------------------------------

```{r, echo = FALSE, include = FALSE}
# independent slopes model
reg_model = "
model{
  for(i in 1:n)
{
 y.i[i] ~ dnorm(mu.i[i],sigma^-2) # data model
 mu.i[i] <- alpha + beta.j[group[i]]*x.i[i]
} # end i loop

for(j in 1:m)
{
 beta.j[j] ~ dnorm(0,10^-2)
}

alpha ~ dnorm(0,2^-2)
sigma ~ dt(0,2^-2,1)T(0,)

 
for(i in 1:n) {yrep[i] ~ dnorm(mu.i[i],sigma^-2)}
}
"

# data
jags.data <- list(y.i = sim_dat$y,
                  x.i = sim_dat$x,
                  group = sim_dat$group,
                  n = nrow(sim_dat),
                  m = sim_dat$group %>% unique() %>% length()) 
# parameters 
parnames <- c("yrep","alpha","beta.j","sigma")
  
# run model
mod <- jags(data = jags.data, 
            parameters.to.save = parnames, 
            model.file = textConnection(reg_model),
            n.iter = 10000,
            n.burnin = 2000,
            n.thin = 4)
m <- mod$BUGSoutput$sims.matrix

true_slopes <- tibble(slopes, group = factor(1:8))
group_ind <- 1:8

m %>% spread_draws(beta.j[group_ind]) %>% 
  ggplot(aes(x = beta.j, y = factor(group_ind))) +
  stat_halfeye() +
  geom_point(data = true_slopes,aes(x = slopes, y = group, colour = "true slope"))
mod$BUGSoutput$summary[1:11,]

yrep <- mod$BUGSoutput$sims.list$yrep
ppc_dens_overlay(y, yrep[1:50, ])
```

```{r, echo = FALSE, include=FALSE}
#partial pooled regression model
bhreg_model = "
model{
  for(i in 1:n)
{
 y.i[i] ~ dnorm(mu.i[i],sigma^-2) # data model
 mu.i[i] <- alpha + beta.j[group[i]]*x.i[i]
} # end i loop

for(j in 1:m)
{
  beta.j[j] ~ dnorm(beta_mu,sigma_beta^-2)
}

beta_mu ~ dnorm(0,2^-2)
alpha ~ dnorm(0,2^-2)
sigma ~ dt(0,2^-2,1)T(0,)
sigma_beta ~ dt(0,1^-2,1)T(0,)

 
for(i in 1:n) {yrep[i] ~ dnorm(mu.i[i],sigma^-2)}
}
"

# data
jags.data <- list(y.i = sim_dat$y,
                  x.i = sim_dat$x,
                  group = sim_dat$group,
                  n = nrow(sim_dat),
                  m = sim_dat$group %>% unique() %>% length()) 
# parameters 
parnames <- c("yrep","alpha","beta.j","sigma","beta_mu","sigma_beta")
  
# run model
mod <- jags(data = jags.data, 
            parameters.to.save = parnames, 
            model.file = textConnection(bhreg_model),
            n.iter = 10000,
            n.burnin = 2000,
            n.thin = 4)
m <- mod$BUGSoutput$sims.matrix

true_slopes <- tibble(slopes, group = factor(1:8))
group_ind <- 1:8

m %>% spread_draws(beta.j[group_ind]) %>% 
  ggplot(aes(x = beta.j, y = factor(group_ind))) +
  stat_halfeye() +
  geom_point(data = true_slopes,aes(x = slopes, y = group, colour = "true slope"))
mod$BUGSoutput$summary[1:13,]

yrep <- mod$BUGSoutput$sims.list$yrep
ppc_dens_overlay(y, yrep[1:50, ])
```
