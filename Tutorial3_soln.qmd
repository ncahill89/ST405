---
title: "ST405/ST645 Bayesian Data Analysis"
subtitle: "Tutorial Questions (3)"
author: "Prof. Niamh Cahill"
format: 
  html:
    embed-resources: true
    self-contained-math: true
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidybayes)
library(rjags)
library(R2jags)
library(bayesplot)


fish_dat <- read_csv("fish_dat.csv")
```

## **Solutions**

### **Question: Bayesian Regression Model - Fisherys Data**

1.  **Visualize the Snapper Length Data**

```{r}
ggplot(fish_dat, aes(x = snapper_length)) +
  geom_histogram(bins = 40,colour = "blue")
```

------------------------------------------------------------------------

2.  **Fit a Basic Bayesian Model**

    -   A Bayesian model that assumes a **Normal likelihood** for the length data, with an **overall mean** and **variance** (ignoring age groups for now).

```{r}
## model spec
normmodel ="
model{
	for (i in 1:n){
 		y.i[i] ~ dnorm(mu, sigma^-2)
	}

## priors
mu ~ dnorm(0,100^-2)  # vague prior
sigma ~ dunif(0,30) # using variation not going beyond 30

## data reps
for(i in 1:n){yrep[i] ~ dnorm(mu,sigma^-2)}
}
"

## data and parameters
jags.data <- list(y.i = fish_dat$snapper_length, 
                  n = nrow(fish_dat))

parnames <- c("mu","sigma","yrep")

## run JAGS
mod <- jags(data = jags.data, 
            parameters.to.save=parnames, 
            model.file = textConnection(normmodel),
            n.iter = 5000,
            n.burnin = 1000,
            n.thin = 2)

## output
m <- mod$BUGSoutput$sims.matrix
```

------------------------------------------------------------------------

3.  **Assess Model Convergence**

```{r}
plot(mod) # quick check for convergence
```

4.  **Summarize Model Parameters**

```{r}
par_summary <- m %>% 
                gather_rvars(mu,sigma) %>% 
                median_qi(.value)
par_summary
```

5.  **Conduct Posterior Predictive Checks**

    **(i)** **Compare Distributions**

    -   Plot the observed distribution of $y$ and overlay it with the distributions of 50 simulated datasets ($y_{\text{rep}}$) from the model.

```{r}
y <- fish_dat$snapper_length
yrep <- mod$BUGSoutput$sims.list$yrep 
ppc_dens_overlay(y, yrep[1:50, ]) 
```

This is ok, but not great. The mean in the yreps is shifted to the right and overdoing it on the lower end with higher density for the lower extremes than the observed data would suggest.

**(ii)** **Evaluate Test Statistics**

-   Calculate the empirical values for the following test statistics based on the observed data:
    -   1st percentile
    -   Median
    -   97th percentile
-   Compare these empirical values with the corresponding posterior predictive distributions of test statistics.

```{r}
## test statistics
ppc_stat(y,yrep, stat = "median") 
```

The median is being overestimated in yreps rel. to the observed data

```{r}
q1 <- function(y) quantile(y, 0.01) # create a function for the quantile (pp_stat help files give an example of this)
ppc_stat(y,yrep, stat = "q1")
```

The lower extremes of yreps are mostly lower than observed lower extreme (based on 1st percentile).

```{r}
q97 <- function(y) quantile(y, 0.97) # same but for 97th percentile
ppc_stat(y,yrep, stat = "q97")
```

The upper extremes of yreps mostly lower than observed upper extreme.

**(iii)** **Posterior Predictive** $p$-values and Effect Sizes

```{r}
# do median first
med_rep <- ppc_stat(y,yrep, stat = "median") # the data in this object contains medians for each yrep

## p-value (median)
sum(med_rep$data$value >= median(fish_dat$snapper_length))/length(med_rep$data$value)
```

There's a 99.4% chance of seeing a median as or more extreme than what was observed based on this model.

```{r}
## effect size (median)
(median(fish_dat$snapper_length - median(med_rep$data$value)))/sd(med_rep$data$value)
```

The effect size is large reflecting what we saw in the posterior predictive probability.

```{r}
# 1st percentile 
perc_1_rep <- ppc_stat(y,yrep, stat = "q1") # the data in this object contains 1st percentiles for each yrep

## p-value (1st percentile)
1- sum(perc_1_rep$data$value >= quantile(fish_dat$snapper_length,probs = 0.01))/length(perc_1_rep$data$value)
```

There's a 99.5% chance of seeing a 1st percentile as or more extreme than what was observed, based on this model. So the model has lower extremes beyond what is suggested by the observed data.

```{r}
## effect size (median)
(quantile(fish_dat$snapper_length,probs = 0.01) - median(perc_1_rep$data$value))/sd(perc_1_rep$data$value)
```

Large effect size again.

```{r}
# 97th percentile 
perc_97_rep <- ppc_stat(y,yrep, stat = "q97") # the data in this object contains 1st percentiles for each yrep

## p-value (97th percentile)
sum(perc_1_rep$data$value >= quantile(fish_dat$snapper_length,probs = 0.01))/length(perc_1_rep$data$value)
```

There's a 0.5% chance of seeing a 97th percentile as or more extreme than what was observed, based on this model. The model might underestimate the upper extremes.

```{r}
## effect size (median)
(quantile(fish_dat$snapper_length,probs = 0.01) - median(perc_1_rep$data$value))/sd(perc_1_rep$data$value)
```

------------------------------------------------------------------------

6.  **Fit a Group-Specific Bayesian Model**

```{r}
## Model spec
mixmodel ="
model{
	for (i in 1:n){
 		y.i[i] ~ dnorm(mu.g[group[i]], sigma.g[group[i]]^-2)
	}

## priors
for(g in 1:ng)
{
mu.g[g] ~ dnorm(0,100^-2)
sigma.g[g] ~ dunif(0,30) 
}

for(i in 1:n){yrep[i] ~ dnorm(mu.g[group[i]],sigma.g[group[i]]^-2)}
}
"

## data and parameters
jags.data <- list(y.i = fish_dat$snapper_length, 
                  n = nrow(fish_dat),
                  group = fish_dat$group,
                  ng = 2)

parnames <- c("mu.g","sigma.g","yrep")

## run JAGS
mod <- jags(data = jags.data, 
             parameters.to.save=parnames, 
             model.file = textConnection(mixmodel),
             n.iter = 5000,
             n.burnin = 1000,
             n.thin = 2)

## output
m <- mod$BUGSoutput$sims.matrix
plot(mod)

```

```{r}
# (i)
y <- fish_dat$snapper_length
yrep <- mod$BUGSoutput$sims.list$yrep
ppc_dens_overlay(y, yrep[1:50, ])
```

This is alot better than the 1st model

```{r}
# (ii)
## test statistics
ppc_stat(y,yrep, stat = "median") 

q1 <- function(y) quantile(y, 0.01) 
ppc_stat(y,yrep, stat = "q1") 

q97 <- function(y) quantile(y, 0.97) 
ppc_stat(y,yrep, stat = "q97") 

```

All of these have improved.

```{r}
# (iii)
med_rep <- ppc_stat(y,yrep, stat = "median") # the data in this object contains medians for each yrep

## p-value
sum(med_rep$data$value >= median(fish_dat$snapper_length))/length(med_rep$data$value)
```

There's a 55% chance of seeing a median as or more extreme than what was observed based on this model. This is an improvement on the 1st model.

```{r}
## effect size

(median(fish_dat$snapper_length - median(med_rep$data$value)))/sd(med_rep$data$value)
```

There's a much smaller effect size

------------------------------------------------------------------------

### **Notes**

-   The **posterior predictive ( p )-value** indicates how many of the simulated datasets have test statistic values as extreme or more extreme than the value observed in the empirical data. It reflects the degree to which the model can replicate observed data characteristics.

-   The **effect size** quantifies the distance between the observed test statistic and the median of the posterior predictive distribution in standard deviation units. This gives an indication of how unusual the observed data is relative to the model predictions.

------------------------------------------------------------------------
