---
title: "Bayesian Data Analysis"
subtitle: "Bayes' Rule"
author: "Dr Niamh Cahill (she/her)"
format: revealjs
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
```

## Bayes' rule {style="font-size: 80%;"}

Thomas Bayes' famous theorem was published in 1763.

![](images/images-2-01.jpeg)

For events A and B:

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$

-   The branch of statistics that you are probably most familiar with up to now is called *frequentist* statistics.

-   *Bayesian* statistics uses Bayes' rule for inference and decision making, *frequentist* statistics does not.

## Crimes Example {style="font-size: 80%;"}

ğŸ” **A Crime Investigation** (adapted from Kruschke):

-   You are investigating a crime and have **4 suspects**: **A, B, C, and D**.

    -   ğŸ‘¤ **Suspect A**\
    -   ğŸ‘¤ **Suspect B**\
    -   ğŸ‘¤ **Suspect C**\
    -   ğŸ‘¤ **Suspect D**

-   ğŸ•µâ€â™‚ï¸ You are **100% sure** the offender is **one of these suspects**, and initially, each one is considered **equally likely** to have committed the crime.

-   â— During your investigation, you discover that **C did not commit the crime**.

## Crimes Example {style="font-size: 80%;"}

**Question:** Can you apply a **Bayesian crime investigation** to quantify the **"information"** about who committed the crime using probability statements?

::: columns
::: {.column width="50%"}
**Prior Information**

-   ğŸ‘¤ **Suspect A**\
-   ğŸ‘¤ **Suspect B**\
-   ğŸ‘¤ **Suspect C**
-   ğŸ‘¤ **Suspect D**
:::

::: {.column width="50%"}
**New Data**

-   ğŸ‘¤ **Suspect A**\
-   ğŸ‘¤ **Suspect B**\
-   âŒğŸ‘¤ **Suspect C** (Ruled out)
-   ğŸ‘¤ **Suspect D**
:::
:::

Given what we know about C, what is the probability that A committed the crime?

::: incremental
-   Before data â€œall 4 equally likelyâ€: Prob(A) = 1/4

-   After data â€œC did not do itâ€: Prob(A\|C did not do it) = 1/3
:::

## Crimes example {style="font-size: 80%;"}

Is that Bayesian learning? Yes!

![](images/bayesian-machine-learning-appplications-examples.png)

-   Step 1: Set prior distribution Prob(A) = Prob(B) = Prob(C) = Prob(D) = 1/4

-   Step 2: Update using data â€œC did not do itâ€ using Bayesâ€™ rule

$$P(A|\text{not } C) = \frac{P(\text{not } C|A)P(A)}{P(\text{not } C)} = \frac{1 \times 1/4}{3/4}  = 1/3$$

## Sleep example {style="font-size: 80%;"}

Suppose that you are interested in the probability, that at any given time in the night between 8pm and 4am, you are asleep.

Let's assume we already have the following information on your sleep over this time period:

```{r}
set.seed(666)
x1 = seq(0,8,0.05)           # some continuous variables 

z =  -4 + 1*x1        # linear combination with a bias
pr = 1/(1+exp(-z))       # pass through an inv-logit function
#pr = pr - (0.25*min(pr))
y = rbinom(length(x1),1,pr)      # bernoulli response variable

df = data.frame(y=y,x1=x1)

mod <- glm(y ~ x1, data = df)

#predict(mod, newdata = data.frame(x1 = 3)) # 0.3786
ggplot(df, aes(x = x1, y = y)) +
  #geom_point() +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
  xlab("hours since 8pm") +
  ylab("probability")
```

This is the probability I am asleep taking into account only the time.

## Sleep Example {style="font-size: 80%;"}

<!-- <style> -->

<!-- .nobullet li { -->

<!--   list-style-type: none; -->

<!-- } -->

<!-- </style> -->

ğŸ’¡ **What if we know the time and have additional evidence?**

::: incremental
<!-- <div class="nobullet"> -->

-   Imagine this scenario: It's **11pm**, and we have extra evidence â€” my **bedroom light is on**. How would this affect the probability that I am **asleep**? <!-- <div> -->

    -   This is where we use **Bayes' Rule** to update our sleep estimate. If we know information about the light, we can apply **Bayes' equation** to refine the probability estimate:

    -   For example, at **11pm**, we can use the probability of me being asleep (our **prior**) and then adjust it using Bayes' Rule based on the light being on: $$
          P(\text{sleep}|\text{light}) = \frac{P(\text{light}|\text{sleep})P(\text{sleep})}{P(\text{light})}
          $$
:::

## Sleep Example {style="font-size: 80%;"}

ğŸ’¡ **We added some information (data) about the light!**

::: incremental
-   To update our sleep probability, we need to know the likelihood of observing this new information, given what we know about sleep habits.

-   ğŸ” **Key Probabilities Based on My Habits:**

    -   If I am **asleep**, the probability that my bedroom light is **on** is very low (1%):\
        $$P(\text{light|sleep}) = 0.01$$

    -   If I am **awake**, the probability that my bedroom light is **on** is quite high (80%):\
        $$P(\text{light|âˆ’sleep}) = 0.8$$
:::

## Sleep Example {style="font-size: 80%;"}

ğŸ’¡ **We had our prior information about sleep and added some information (data) about the light!**

-   The final piece of the equation is $P(\text{light})$, representing the **total probability** my light is on.

-   There are two possible conditions where my light could be on:

    1.  **I am asleep**\
    2.  **I am awake**

**To calculate the total probability, we use:**

$$
P(\text{light}) = P(\text{light|sleep})P(\text{sleep}) + P(\text{light|âˆ’sleep})P(\text{âˆ’sleep})
$$

## Sleep Example {style="font-size: 80%;"}

**Now, let's put it all together with Bayes' Rule:**

$\underset{P(A|B)}{P(\text{sleep}|\text{light})} =$

::: columns
::: {.column width="50%"}
$$
 \frac{\underset{P(B|A)}{P(\text{light}|\text{sleep})} \cdot \underset{P(A)}{P(\text{sleep})}}{\underset{P(B)}{P(\text{light})}}
$$
:::

::: {.column width="50%"}
$$
 \frac{\underset{0.01}{P(\text{light}|\text{sleep})} \cdot \underset{0.25}{P(\text{sleep})}}{\underset{0.5}{P(\text{light})}}
$$
:::
:::

ğŸ¯ This allows us to **update our belief** about the probability that Iâ€™m asleep based on the new evidence (the light being on)!

## Likelihood, Prior & Posterior

![](images/bayesian-machine-learning-appplications-examples.png)

## Bayes' rule applied to parameters and data {style="font-size: 75%;"}

Given a set of observed data points $Y$ and a set of parameters $\theta$, we write Bayes' rule as $$\underset{\text{posterior}}{P(\theta|Y)} = \frac{\underset{\text{likelihood}}{P(Y|\theta)}\underset{\text{prior}}{P(\theta)}}{\underset{\text{marginal likelihood}}{P(Y)}}$$

Where the denominator is

-   $P(Y) = \sum_{\theta^*}P(Y|\theta^*)P(\theta^*)$ for discrete-valued variables, or

-   $P(Y) = \int P(Y|\theta^*)P(\theta^*) d\theta^*$ for continuous variables.

$P(Y)$ is often difficult to calculate (more on this later) and Baye's rule is often written more simply as a proportional statement $$\underset{\text{posterior}}{P(\theta|Y)} \propto \underset{\text{likelihood}}{P(Y|\theta)}\underset{\text{prior}}{P(\theta)}$$

## Likelihood, Prior & Posterior {style="font-size: 75%;"}

$$\underset{\text{posterior}}{P(\theta|Y)} \propto \underset{\text{likelihood}}{P(Y|\theta)}\underset{\text{prior}}{P(\theta)}$$

::: columns
::: {.column width="43%"}
-   $P(Y|\theta)$ - the *likelihood*. The likelihood represents the evidence that we have based on data.

-   $P(\theta)$ - the *prior*. The prior represents what we know about the parameters before the data are observed.

-   $P(\theta|Y)$ - the *posterior*. The posterior represents our updated knowledge about parameters after the data are observed.
:::

::: {.column width="57%"}
![](images/PictureBayes.png){fig-align="center"}
:::
:::

<!-- ## Disease screening example {style="font-size: 80%;"} -->

<!-- -   Suppose there is a test for a disease that has a sensitivity of 80% (i.e., if 100 people that have the disease take the test then 80 of them will get a positive result). -->

<!-- P(+iv\|disease) = 0.8 -->

<!-- -   Suppose the chance of having this disease is 2%. -->

<!-- P(disease) = 0.02 -->

<!-- -   Suppose we also know that the test gives false positive results 5% of the time (this relates to specificity of the test). -->

<!-- P(+iv\|no disease) = 0.05 -->

<!-- What is the probability you have the disease given that you receive a positive test result? -->

<!-- $$P(\text{disease}|+iv) = \frac{P(+ive|\text{disease})P(\text{disease})}{P(+ive)}$$ -->

<!-- ## Disease screening example -->

<!-- Find the probability that a person has the disease given they get a positive test result. -->

<!-- (1) $P(+ive|\text{disease})P(\text{disease}) = 0.8 \times 0.02$ -->

<!-- (2) $P(+ive) = P(+ive|\text{disease})P(\text{disease}) + P(+ive|\text{no disease})P(\text{no disease}) = 0.065$ -->

<!-- $$P(\text{disease}|+iv) = \frac{0.8 \times 0.02}{0.065} = 0.25$$ -->

<!-- -   So even with a positive test result for a test with an 80% "hit rate", the probability of having the disease is only 25% -->

<!-- -   The lower probability is a consequence of the low *prior* probability of the disease and the non-negligible false positive rate. -->
